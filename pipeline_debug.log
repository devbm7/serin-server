2025-07-30 17:32:13,583 - __main__ - INFO - Loading all models
2025-07-30 17:32:19,481 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-30 17:32:29,754 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-30 17:32:29,754 - __main__ - INFO - ASR model loaded in 16.171s
2025-07-30 17:33:35,364 - __main__ - INFO - LLM model selected in 65.610s
2025-07-30 17:33:35,364 - __main__ - INFO - Initializing TTS Processor
2025-07-30 17:34:04,435 - __main__ - INFO - Loading all models
2025-07-30 17:34:19,291 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-tiny
2025-07-30 17:34:24,772 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-30 17:34:24,772 - __main__ - INFO - ASR model loaded in 20.337s
2025-07-30 17:34:27,312 - __main__ - INFO - LLM model selected in 2.540s
2025-07-30 17:34:27,312 - __main__ - INFO - Initializing TTS Processor
2025-07-30 17:34:31,470 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-30 17:34:31,470 - __main__ - INFO - TTS model loaded in 4.158s
2025-07-30 17:34:31,470 - __main__ - INFO - Initializing VAD Processor
2025-07-30 17:34:32,109 - __main__ - INFO - VAD model loaded successfully
2025-07-30 17:34:32,109 - __main__ - INFO - VAD model loaded in 0.639s
2025-07-30 17:34:32,109 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-30 17:34:32,109 - __main__ - INFO - ASR: 20.337s
2025-07-30 17:34:32,109 - __main__ - INFO - LLM Selection: 2.540s
2025-07-30 17:34:32,109 - __main__ - INFO - TTS: 4.158s
2025-07-30 17:34:32,110 - __main__ - INFO - VAD: 0.639s
2025-07-30 17:34:32,110 - __main__ - INFO - TOTAL: 27.674s
2025-07-30 17:34:32,110 - __main__ - INFO - =============================
2025-07-30 17:34:32,110 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-30 17:34:32,231 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-30 17:35:49,564 - __main__ - INFO - Starting continuous recording
2025-07-30 17:35:49,808 - __main__ - INFO - Recording started successfully
2025-07-30 17:35:49,810 - __main__ - INFO - Starting continuous audio processing thread
2025-07-30 17:35:49,810 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-30 17:35:49,810 - __main__ - INFO - Initializing Ollama client with base URL: http://localhost:11434
2025-07-30 17:35:53,292 - __main__ - INFO - Stopping recording
2025-07-30 17:35:53,383 - __main__ - INFO - Audio stream closed successfully
2025-07-30 17:35:53,383 - __main__ - INFO - Sentinel value added to queue
2025-07-30 17:35:53,384 - __main__ - INFO - Received sentinel value, stopping processing
2025-07-30 17:35:53,384 - __main__ - INFO - Recording stopped. Total chunks recorded: 78
2025-07-30 17:35:53,384 - __main__ - INFO - === CONTINUOUS PROCESSING SUMMARY ===
2025-07-30 17:35:53,384 - __main__ - INFO - Total processing time: 3.574s
2025-07-30 17:35:53,384 - __main__ - INFO - Chunks processed: 78
2025-07-30 17:35:53,384 - __main__ - INFO - Utterances processed: 0
2025-07-30 17:35:53,384 - __main__ - INFO - =====================================
2025-07-30 17:36:10,709 - __main__ - INFO - Starting continuous recording
2025-07-30 17:36:10,849 - __main__ - INFO - Recording started successfully
2025-07-30 17:36:10,855 - __main__ - INFO - Starting continuous audio processing thread
2025-07-30 17:36:10,855 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-30 17:36:10,855 - __main__ - INFO - Initializing Ollama client with base URL: http://localhost:11434
2025-07-30 17:36:11,402 - __main__ - INFO - Stopping recording
2025-07-30 17:36:11,425 - __main__ - INFO - === CONTINUOUS PROCESSING SUMMARY ===
2025-07-30 17:36:11,425 - __main__ - INFO - Total processing time: 0.570s
2025-07-30 17:36:11,425 - __main__ - INFO - Chunks processed: 0
2025-07-30 17:36:11,425 - __main__ - INFO - Utterances processed: 0
2025-07-30 17:36:11,425 - __main__ - INFO - =====================================
2025-07-30 17:36:11,518 - __main__ - INFO - Audio stream closed successfully
2025-07-30 17:36:11,518 - __main__ - INFO - Sentinel value added to queue
2025-07-30 17:36:11,518 - __main__ - INFO - Recording stopped. Total chunks recorded: 0
2025-07-30 17:36:11,895 - __main__ - INFO - Starting continuous recording
2025-07-30 17:36:12,025 - __main__ - INFO - Recording started successfully
2025-07-30 17:36:12,026 - __main__ - INFO - Starting continuous audio processing thread
2025-07-30 17:36:12,026 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-30 17:36:12,026 - __main__ - INFO - Initializing Ollama client with base URL: http://localhost:11434
2025-07-30 17:36:12,456 - __main__ - INFO - Stopping recording
2025-07-30 17:36:12,487 - __main__ - INFO - === CONTINUOUS PROCESSING SUMMARY ===
2025-07-30 17:36:12,487 - __main__ - INFO - Total processing time: 0.461s
2025-07-30 17:36:12,487 - __main__ - INFO - Chunks processed: 0
2025-07-30 17:36:12,487 - __main__ - INFO - Utterances processed: 0
2025-07-30 17:36:12,487 - __main__ - INFO - =====================================
2025-07-30 17:36:12,629 - __main__ - INFO - Audio stream closed successfully
2025-07-30 17:36:12,629 - __main__ - INFO - Sentinel value added to queue
2025-07-30 17:36:12,629 - __main__ - INFO - Recording stopped. Total chunks recorded: 0
2025-07-30 17:43:18,792 - __main__ - INFO - Loading all models
2025-07-30 17:43:30,218 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-large-v3-turbo
2025-07-30 17:43:49,393 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-30 17:43:49,393 - __main__ - INFO - ASR model loaded in 30.600s
2025-07-30 17:43:56,050 - __main__ - INFO - LLM model selected in 6.657s
2025-07-30 17:43:56,050 - __main__ - INFO - Initializing TTS Processor
2025-07-30 17:44:00,061 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-30 17:44:00,061 - __main__ - INFO - TTS model loaded in 4.011s
2025-07-30 17:44:00,061 - __main__ - INFO - Initializing VAD Processor
2025-07-30 17:44:00,749 - __main__ - INFO - VAD model loaded successfully
2025-07-30 17:44:00,749 - __main__ - INFO - VAD model loaded in 0.688s
2025-07-30 17:44:00,749 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-30 17:44:00,749 - __main__ - INFO - ASR: 30.600s
2025-07-30 17:44:00,749 - __main__ - INFO - LLM Selection: 6.657s
2025-07-30 17:44:00,749 - __main__ - INFO - TTS: 4.011s
2025-07-30 17:44:00,749 - __main__ - INFO - VAD: 0.688s
2025-07-30 17:44:00,749 - __main__ - INFO - TOTAL: 41.957s
2025-07-30 17:44:00,749 - __main__ - INFO - =============================
2025-07-30 17:44:00,749 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-30 17:44:01,106 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-30 17:44:04,465 - __main__ - INFO - Starting continuous recording
2025-07-30 17:44:04,891 - __main__ - INFO - Recording started successfully
2025-07-30 17:44:04,891 - __main__ - INFO - Starting continuous audio processing thread
2025-07-30 17:44:04,891 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-30 17:44:04,891 - __main__ - INFO - Initializing Ollama client with base URL: http://localhost:11434
2025-07-30 17:44:08,337 - __main__ - INFO - Utterance collected: duration=0.51s, chunks=16
2025-07-30 17:44:08,372 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 17:44:08,372 - __main__ - INFO - Starting utterance processing
2025-07-30 17:44:08,372 - __main__ - INFO - Starting transcription: audio_length=8192, duration=0.51s
2025-07-30 17:44:08,423 - __main__ - INFO - ASR preprocessing completed in 0.051s
2025-07-30 17:44:10,488 - __main__ - INFO - ASR generation completed in 2.065s
2025-07-30 17:44:10,498 - __main__ - INFO - ASR decoding completed in 0.010s
2025-07-30 17:44:10,498 - __main__ - INFO - ASR transcription completed in 2.126s: 'hello'
2025-07-30 17:44:10,499 - __main__ - ERROR - ASR error after 2.126s: Unable to render {'hello'}; A str, Segment or object with __rich_console__ method is required
2025-07-30 17:44:10,500 - __main__ - INFO - ASR processing completed in 2.128s
2025-07-30 17:44:10,500 - __main__ - WARNING - ASR returned empty transcription after 2.128s
2025-07-30 17:44:10,500 - __main__ - INFO - Utterance processing completed in 2.128s
2025-07-30 17:45:43,460 - __main__ - INFO - Utterance collected: duration=0.32s, chunks=10
2025-07-30 17:45:43,515 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 17:45:43,515 - __main__ - INFO - Starting utterance processing
2025-07-30 17:45:43,515 - __main__ - INFO - Starting transcription: audio_length=5120, duration=0.32s
2025-07-30 17:45:43,529 - __main__ - INFO - ASR preprocessing completed in 0.015s
2025-07-30 17:45:44,642 - __main__ - INFO - ASR generation completed in 1.113s
2025-07-30 17:45:44,643 - __main__ - INFO - ASR decoding completed in 0.001s
2025-07-30 17:45:44,643 - __main__ - INFO - ASR transcription completed in 1.129s: 'bye'
2025-07-30 17:45:44,644 - __main__ - ERROR - ASR error after 1.130s: Unable to render {'bye'}; A str, Segment or object with __rich_console__ method is required
2025-07-30 17:45:44,644 - __main__ - INFO - ASR processing completed in 1.130s
2025-07-30 17:45:44,644 - __main__ - WARNING - ASR returned empty transcription after 1.130s
2025-07-30 17:45:44,644 - __main__ - INFO - Utterance processing completed in 1.130s
2025-07-30 17:45:44,666 - __main__ - INFO - Utterance collected: duration=0.10s, chunks=3
2025-07-30 17:45:44,666 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-30 17:45:48,617 - __main__ - INFO - Utterance collected: duration=0.22s, chunks=7
2025-07-30 17:45:48,622 - __main__ - INFO - VAD rejected but audio level is significant (0.0145), processing anyway
2025-07-30 17:45:48,622 - __main__ - INFO - Starting utterance processing
2025-07-30 17:45:48,622 - __main__ - INFO - Starting transcription: audio_length=3584, duration=0.22s
2025-07-30 17:45:48,637 - __main__ - INFO - ASR preprocessing completed in 0.014s
2025-07-30 17:45:49,615 - __main__ - INFO - ASR generation completed in 0.979s
2025-07-30 17:45:49,617 - __main__ - INFO - ASR decoding completed in 0.001s
2025-07-30 17:45:49,617 - __main__ - INFO - ASR transcription completed in 0.995s: 'you'
2025-07-30 17:45:49,617 - __main__ - ERROR - ASR error after 0.995s: Unable to render {'you'}; A str, Segment or object with __rich_console__ method is required
2025-07-30 17:45:49,618 - __main__ - INFO - ASR processing completed in 0.996s
2025-07-30 17:45:49,618 - __main__ - WARNING - ASR returned empty transcription after 0.996s
2025-07-30 17:45:49,618 - __main__ - INFO - Utterance processing completed in 0.996s
2025-07-30 17:45:49,823 - __main__ - INFO - Utterance collected: duration=0.13s, chunks=4
2025-07-30 17:45:49,824 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-30 17:45:53,936 - __main__ - INFO - Utterance collected: duration=0.42s, chunks=13
2025-07-30 17:45:53,945 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 17:45:53,945 - __main__ - INFO - Starting utterance processing
2025-07-30 17:45:53,945 - __main__ - INFO - Starting transcription: audio_length=6656, duration=0.42s
2025-07-30 17:45:53,959 - __main__ - INFO - ASR preprocessing completed in 0.014s
2025-07-30 17:45:55,000 - __main__ - INFO - ASR generation completed in 1.041s
2025-07-30 17:45:55,001 - __main__ - INFO - ASR decoding completed in 0.001s
2025-07-30 17:45:55,001 - __main__ - INFO - ASR transcription completed in 1.056s: 'peace'
2025-07-30 17:45:55,001 - __main__ - ERROR - ASR error after 1.056s: Unable to render {'peace'}; A str, Segment or object with __rich_console__ method is required
2025-07-30 17:45:55,001 - __main__ - INFO - ASR processing completed in 1.056s
2025-07-30 17:45:55,001 - __main__ - WARNING - ASR returned empty transcription after 1.056s
2025-07-30 17:45:55,002 - __main__ - INFO - Utterance processing completed in 1.057s
2025-07-30 17:46:15,502 - __main__ - INFO - Utterance collected: duration=0.10s, chunks=3
2025-07-30 17:46:15,502 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-30 17:47:01,884 - __main__ - INFO - Loading all models
2025-07-30 17:47:05,977 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-large-v3-turbo
2025-07-30 17:47:18,369 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-30 17:47:18,369 - __main__ - INFO - ASR model loaded in 16.483s
2025-07-30 17:47:29,238 - __main__ - INFO - LLM model selected in 10.870s
2025-07-30 17:47:29,238 - __main__ - INFO - Initializing TTS Processor
2025-07-30 17:47:33,068 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-30 17:47:33,068 - __main__ - INFO - TTS model loaded in 3.830s
2025-07-30 17:47:33,068 - __main__ - INFO - Initializing VAD Processor
2025-07-30 17:47:33,715 - __main__ - INFO - VAD model loaded successfully
2025-07-30 17:47:33,715 - __main__ - INFO - VAD model loaded in 0.646s
2025-07-30 17:47:33,721 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-30 17:47:33,721 - __main__ - INFO - ASR: 16.483s
2025-07-30 17:47:33,721 - __main__ - INFO - LLM Selection: 10.870s
2025-07-30 17:47:33,721 - __main__ - INFO - TTS: 3.830s
2025-07-30 17:47:33,721 - __main__ - INFO - VAD: 0.646s
2025-07-30 17:47:33,721 - __main__ - INFO - TOTAL: 31.830s
2025-07-30 17:47:33,721 - __main__ - INFO - =============================
2025-07-30 17:47:33,721 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-30 17:47:34,030 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-30 17:47:37,137 - __main__ - INFO - Starting continuous recording
2025-07-30 17:47:37,478 - __main__ - INFO - Recording started successfully
2025-07-30 17:47:37,480 - __main__ - INFO - Starting continuous audio processing thread
2025-07-30 17:47:37,481 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-30 17:47:37,481 - __main__ - INFO - Initializing Ollama client with base URL: http://localhost:11434
2025-07-30 17:47:41,022 - __main__ - INFO - Utterance collected: duration=0.48s, chunks=15
2025-07-30 17:47:41,036 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 17:47:41,036 - __main__ - INFO - Starting utterance processing
2025-07-30 17:47:41,036 - __main__ - INFO - Starting transcription: audio_length=7680, duration=0.48s
2025-07-30 17:47:41,076 - __main__ - INFO - ASR preprocessing completed in 0.040s
2025-07-30 17:47:42,958 - __main__ - INFO - ASR generation completed in 1.883s
2025-07-30 17:47:42,966 - __main__ - INFO - ASR decoding completed in 0.008s
2025-07-30 17:47:42,966 - __main__ - INFO - ASR transcription completed in 1.930s: 'so'
2025-07-30 17:47:42,966 - __main__ - INFO - ASR processing completed in 1.930s
2025-07-30 17:47:42,966 - __main__ - INFO - User transcription: 'so'
2025-07-30 17:47:42,967 - __main__ - INFO - User message added to queue
2025-07-30 17:47:42,967 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 17:47:52,242 - __main__ - INFO - Ollama request sent in 9.275s
2025-07-30 17:47:52,242 - __main__ - INFO - Ollama response completed in 9.275s: length=165
2025-07-30 17:47:52,243 - __main__ - ERROR - Error in processing utterance after 11.208s: Unable to render {"It looks like you started to ask a question or make a statement, but it got cut off. Can you please complete your thought or ask a question? I'll do my best to help."}; A str, Segment or object with __rich_console__ method is required
2025-07-30 17:47:52,244 - __main__ - INFO - Utterance processing completed in 11.209s
2025-07-30 17:48:10,795 - __main__ - INFO - Utterance collected: duration=0.16s, chunks=5
2025-07-30 17:48:10,808 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 17:48:10,808 - __main__ - INFO - Starting utterance processing
2025-07-30 17:48:10,808 - __main__ - INFO - Starting transcription: audio_length=2560, duration=0.16s
2025-07-30 17:48:11,313 - __main__ - INFO - ASR preprocessing completed in 0.505s
2025-07-30 17:48:12,374 - __main__ - INFO - ASR generation completed in 1.061s
2025-07-30 17:48:12,378 - __main__ - INFO - ASR decoding completed in 0.004s
2025-07-30 17:48:12,378 - __main__ - INFO - ASR transcription completed in 1.570s: 'thank you'
2025-07-30 17:48:12,379 - __main__ - INFO - ASR processing completed in 1.571s
2025-07-30 17:48:12,379 - __main__ - INFO - User transcription: 'thank you'
2025-07-30 17:48:12,379 - __main__ - INFO - User message added to queue
2025-07-30 17:48:12,379 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 17:48:16,153 - __main__ - INFO - Ollama request sent in 3.774s
2025-07-30 17:48:16,153 - __main__ - INFO - Ollama response completed in 3.774s: length=119
2025-07-30 17:48:16,153 - __main__ - ERROR - Error in processing utterance after 5.345s: Unable to render {'It was a pleasure assisting you. Is there anything else I can help with or would you like to chat about something else?'}; A str, Segment or object with __rich_console__ method is required
2025-07-30 17:48:16,154 - __main__ - INFO - Utterance processing completed in 5.346s
2025-07-30 17:48:29,137 - __main__ - INFO - Utterance collected: duration=0.77s, chunks=24
2025-07-30 17:48:29,153 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 17:48:29,153 - __main__ - INFO - Starting utterance processing
2025-07-30 17:48:29,153 - __main__ - INFO - Starting transcription: audio_length=12288, duration=0.77s
2025-07-30 17:48:29,170 - __main__ - INFO - ASR preprocessing completed in 0.016s
2025-07-30 17:48:30,192 - __main__ - INFO - ASR generation completed in 1.022s
2025-07-30 17:48:30,197 - __main__ - INFO - ASR decoding completed in 0.005s
2025-07-30 17:48:30,197 - __main__ - INFO - ASR transcription completed in 1.044s: 'ok adios'
2025-07-30 17:48:30,197 - __main__ - INFO - ASR processing completed in 1.044s
2025-07-30 17:48:30,198 - __main__ - INFO - User transcription: 'ok adios'
2025-07-30 17:48:30,198 - __main__ - INFO - User message added to queue
2025-07-30 17:48:30,198 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 17:48:33,104 - __main__ - INFO - Ollama request sent in 2.906s
2025-07-30 17:48:33,104 - __main__ - INFO - Ollama response completed in 2.906s: length=37
2025-07-30 17:48:33,104 - __main__ - ERROR - Error in processing utterance after 3.951s: Unable to render {'Adi�s. Espero que tengas un buen d�a.'}; A str, Segment or object with __rich_console__ method is required
2025-07-30 17:48:33,105 - __main__ - INFO - Utterance processing completed in 3.952s
2025-07-30 17:48:33,136 - __main__ - INFO - Utterance collected: duration=0.06s, chunks=2
2025-07-30 17:48:33,136 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-30 17:49:10,891 - __main__ - INFO - Loading all models
2025-07-30 17:49:14,149 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-30 17:49:20,553 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-30 17:49:20,553 - __main__ - INFO - ASR model loaded in 9.662s
2025-07-30 17:49:23,768 - __main__ - INFO - LLM model selected in 3.215s
2025-07-30 17:49:23,768 - __main__ - INFO - Initializing TTS Processor
2025-07-30 17:49:27,072 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-30 17:49:27,072 - __main__ - INFO - TTS model loaded in 3.304s
2025-07-30 17:49:27,072 - __main__ - INFO - Initializing VAD Processor
2025-07-30 17:49:27,395 - __main__ - INFO - VAD model loaded successfully
2025-07-30 17:49:27,395 - __main__ - INFO - VAD model loaded in 0.323s
2025-07-30 17:49:27,395 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-30 17:49:27,395 - __main__ - INFO - ASR: 9.662s
2025-07-30 17:49:27,395 - __main__ - INFO - LLM Selection: 3.215s
2025-07-30 17:49:27,395 - __main__ - INFO - TTS: 3.304s
2025-07-30 17:49:27,395 - __main__ - INFO - VAD: 0.323s
2025-07-30 17:49:27,395 - __main__ - INFO - TOTAL: 16.504s
2025-07-30 17:49:27,395 - __main__ - INFO - =============================
2025-07-30 17:49:27,395 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-30 17:49:27,685 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-30 17:49:30,425 - __main__ - INFO - Starting continuous recording
2025-07-30 17:49:30,665 - __main__ - INFO - Recording started successfully
2025-07-30 17:49:30,665 - __main__ - INFO - Starting continuous audio processing thread
2025-07-30 17:49:30,665 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-30 17:49:30,665 - __main__ - INFO - Initializing Ollama client with base URL: http://localhost:11434
2025-07-30 17:49:34,100 - __main__ - INFO - Utterance collected: duration=0.38s, chunks=12
2025-07-30 17:49:34,113 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 17:49:34,113 - __main__ - INFO - Starting utterance processing
2025-07-30 17:49:34,113 - __main__ - INFO - Starting transcription: audio_length=6144, duration=0.38s
2025-07-30 17:49:34,152 - __main__ - INFO - ASR preprocessing completed in 0.038s
2025-07-30 17:49:35,500 - __main__ - INFO - ASR generation completed in 1.348s
2025-07-30 17:49:35,504 - __main__ - INFO - ASR decoding completed in 0.004s
2025-07-30 17:49:35,505 - __main__ - INFO - ASR transcription completed in 1.392s: 'hello'
2025-07-30 17:49:35,506 - __main__ - INFO - ASR processing completed in 1.393s
2025-07-30 17:49:35,506 - __main__ - INFO - User transcription: 'hello'
2025-07-30 17:49:35,506 - __main__ - INFO - User message added to queue
2025-07-30 17:49:35,506 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 17:49:38,652 - __main__ - INFO - Ollama request sent in 3.145s
2025-07-30 17:49:38,652 - __main__ - INFO - Ollama response completed in 3.146s: length=72
2025-07-30 17:49:38,660 - __main__ - INFO - LLM processing completed in 3.154s
2025-07-30 17:49:38,660 - __main__ - INFO - Ollama response: 'Hello. Is there something I can help you with or would you like to chat?...'
2025-07-30 17:49:38,660 - __main__ - INFO - Assistant message added to queue
2025-07-30 17:49:38,660 - __main__ - INFO - Starting TTS synthesis: text_length=72, voice=af_heart
2025-07-30 17:49:40,917 - __main__ - INFO - TTS generation completed in 2.256s
2025-07-30 17:49:40,917 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-30 17:49:40,917 - __main__ - INFO - TTS synthesis completed in 2.257s: audio_length=100800
2025-07-30 17:49:40,917 - __main__ - INFO - TTS processing completed in 2.257s
2025-07-30 17:49:40,917 - __main__ - INFO - Starting audio playback: duration=4.20s
2025-07-30 17:49:45,269 - __main__ - INFO - Audio playback completed in 4.352s
2025-07-30 17:49:45,270 - __main__ - INFO - Audio playback completed in 4.353s
2025-07-30 17:49:45,270 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-30 17:49:45,270 - __main__ - INFO - ASR: 1.393s
2025-07-30 17:49:45,270 - __main__ - INFO - LLM: 3.154s
2025-07-30 17:49:45,270 - __main__ - INFO - TTS: 2.257s
2025-07-30 17:49:45,270 - __main__ - INFO - Playback: 4.353s
2025-07-30 17:49:45,270 - __main__ - INFO - TOTAL: 11.157s
2025-07-30 17:49:45,270 - __main__ - INFO - =====================================
2025-07-30 17:49:45,270 - __main__ - INFO - Utterance processing completed in 11.157s
2025-07-30 17:49:52,938 - __main__ - INFO - Utterance collected: duration=3.30s, chunks=103
2025-07-30 17:49:52,989 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 17:49:52,989 - __main__ - INFO - Starting utterance processing
2025-07-30 17:49:52,989 - __main__ - INFO - Starting transcription: audio_length=52736, duration=3.30s
2025-07-30 17:49:53,003 - __main__ - INFO - ASR preprocessing completed in 0.013s
2025-07-30 17:49:53,670 - __main__ - INFO - ASR generation completed in 0.667s
2025-07-30 17:49:53,674 - __main__ - INFO - ASR decoding completed in 0.005s
2025-07-30 17:49:53,674 - __main__ - INFO - ASR transcription completed in 0.685s: 'can you tell me who albert einstein was'
2025-07-30 17:49:53,675 - __main__ - INFO - ASR processing completed in 0.686s
2025-07-30 17:49:53,675 - __main__ - INFO - User transcription: 'can you tell me who albert einstein was'
2025-07-30 17:49:53,675 - __main__ - INFO - User message added to queue
2025-07-30 17:49:53,675 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 17:50:25,684 - __main__ - ERROR - Ollama connection error after 32.008s: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=30)
2025-07-30 17:50:25,684 - __main__ - INFO - LLM processing completed in 32.009s
2025-07-30 17:50:25,684 - __main__ - INFO - Ollama response: 'Sorry, I'm having trouble connecting to the AI model....'
2025-07-30 17:50:25,684 - __main__ - INFO - Assistant message added to queue
2025-07-30 17:50:25,684 - __main__ - INFO - Starting TTS synthesis: text_length=53, voice=af_heart
2025-07-30 17:50:25,979 - __main__ - INFO - TTS generation completed in 0.295s
2025-07-30 17:50:25,979 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-30 17:50:25,980 - __main__ - INFO - TTS synthesis completed in 0.296s: audio_length=87600
2025-07-30 17:50:25,980 - __main__ - INFO - TTS processing completed in 0.296s
2025-07-30 17:50:25,980 - __main__ - INFO - Starting audio playback: duration=3.65s
2025-07-30 17:50:29,772 - __main__ - INFO - Audio playback completed in 3.793s
2025-07-30 17:50:29,774 - __main__ - INFO - Audio playback completed in 3.794s
2025-07-30 17:50:29,774 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-30 17:50:29,774 - __main__ - INFO - ASR: 0.686s
2025-07-30 17:50:29,774 - __main__ - INFO - LLM: 32.009s
2025-07-30 17:50:29,774 - __main__ - INFO - TTS: 0.296s
2025-07-30 17:50:29,774 - __main__ - INFO - Playback: 3.794s
2025-07-30 17:50:29,774 - __main__ - INFO - TOTAL: 36.785s
2025-07-30 17:50:29,774 - __main__ - INFO - =====================================
2025-07-30 17:50:29,774 - __main__ - INFO - Utterance processing completed in 36.785s
2025-07-30 17:50:29,802 - __main__ - INFO - Utterance collected: duration=0.03s, chunks=1
2025-07-30 17:50:29,802 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-30 17:50:35,472 - __main__ - INFO - Utterance collected: duration=0.16s, chunks=5
2025-07-30 17:50:35,479 - __main__ - INFO - VAD rejected but audio level is significant (0.0025), processing anyway
2025-07-30 17:50:35,479 - __main__ - INFO - Starting utterance processing
2025-07-30 17:50:35,479 - __main__ - INFO - Starting transcription: audio_length=2560, duration=0.16s
2025-07-30 17:50:35,494 - __main__ - INFO - ASR preprocessing completed in 0.015s
2025-07-30 17:50:35,976 - __main__ - INFO - ASR generation completed in 0.482s
2025-07-30 17:50:35,978 - __main__ - INFO - ASR decoding completed in 0.002s
2025-07-30 17:50:35,978 - __main__ - INFO - ASR transcription completed in 0.499s: 'you'
2025-07-30 17:50:35,979 - __main__ - INFO - ASR processing completed in 0.500s
2025-07-30 17:50:35,979 - __main__ - INFO - User transcription: 'you'
2025-07-30 17:50:35,980 - __main__ - INFO - User message added to queue
2025-07-30 17:50:35,980 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 23:27:00,845 - __main__ - INFO - Loading all models
2025-07-30 23:27:14,188 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-30 23:27:21,604 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-30 23:27:21,604 - __main__ - INFO - ASR model loaded in 20.757s
2025-07-30 23:27:34,079 - __main__ - INFO - LLM model selected in 12.475s
2025-07-30 23:27:34,081 - __main__ - INFO - Initializing TTS Processor
2025-07-30 23:27:37,601 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-30 23:27:37,601 - __main__ - INFO - TTS model loaded in 3.520s
2025-07-30 23:27:37,624 - __main__ - INFO - Initializing VAD Processor
2025-07-30 23:27:37,918 - __main__ - INFO - VAD model loaded successfully
2025-07-30 23:27:37,918 - __main__ - INFO - VAD model loaded in 0.293s
2025-07-30 23:27:37,918 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-30 23:27:37,918 - __main__ - INFO - ASR: 20.757s
2025-07-30 23:27:37,918 - __main__ - INFO - LLM Selection: 12.475s
2025-07-30 23:27:37,920 - __main__ - INFO - TTS: 3.520s
2025-07-30 23:27:37,920 - __main__ - INFO - VAD: 0.293s
2025-07-30 23:27:37,920 - __main__ - INFO - TOTAL: 37.072s
2025-07-30 23:27:37,920 - __main__ - INFO - =============================
2025-07-30 23:27:37,920 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-30 23:27:38,169 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-30 23:27:40,878 - __main__ - INFO - Starting continuous recording
2025-07-30 23:27:41,171 - __main__ - INFO - Recording started successfully
2025-07-30 23:27:41,171 - __main__ - INFO - Starting continuous audio processing thread
2025-07-30 23:27:41,171 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-30 23:27:41,171 - __main__ - INFO - Initializing Ollama client with base URL: http://localhost:11434
2025-07-30 23:27:46,039 - __main__ - INFO - Utterance collected: duration=1.02s, chunks=32
2025-07-30 23:27:46,087 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 23:27:46,087 - __main__ - INFO - Starting utterance processing
2025-07-30 23:27:46,087 - __main__ - INFO - Starting transcription: audio_length=16384, duration=1.02s
2025-07-30 23:27:46,126 - __main__ - INFO - ASR preprocessing completed in 0.039s
2025-07-30 23:27:47,614 - __main__ - INFO - ASR generation completed in 1.488s
2025-07-30 23:27:47,622 - __main__ - INFO - ASR decoding completed in 0.007s
2025-07-30 23:27:47,622 - __main__ - INFO - ASR transcription completed in 1.535s: 'so i love you'
2025-07-30 23:27:47,622 - __main__ - INFO - ASR processing completed in 1.535s
2025-07-30 23:27:47,622 - __main__ - INFO - User transcription: 'so i love you'
2025-07-30 23:27:47,622 - __main__ - INFO - User message added to queue
2025-07-30 23:27:47,622 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 23:28:01,701 - __main__ - INFO - Ollama request sent in 14.079s
2025-07-30 23:28:01,701 - __main__ - INFO - Ollama response completed in 14.079s: length=506
2025-07-30 23:28:01,703 - __main__ - INFO - LLM processing completed in 14.081s
2025-07-30 23:28:01,703 - __main__ - INFO - Ollama response: 'I'm glad to hear that. Saying "I love you" can be a beautiful and intimate way to express your feeli...'
2025-07-30 23:28:01,703 - __main__ - INFO - Assistant message added to queue
2025-07-30 23:28:01,703 - __main__ - INFO - Starting TTS synthesis: text_length=506, voice=af_heart
2025-07-30 23:28:05,097 - __main__ - INFO - TTS generation completed in 3.394s
2025-07-30 23:28:05,098 - __main__ - INFO - TTS audio concatenation completed in 0.001s
2025-07-30 23:28:05,098 - __main__ - INFO - TTS synthesis completed in 3.395s: audio_length=702000
2025-07-30 23:28:05,098 - __main__ - INFO - TTS processing completed in 3.395s
2025-07-30 23:28:05,099 - __main__ - INFO - Starting audio playback: duration=29.25s
2025-07-30 23:28:34,527 - __main__ - INFO - Audio playback completed in 29.428s
2025-07-30 23:28:34,527 - __main__ - INFO - Audio playback completed in 29.429s
2025-07-30 23:28:34,527 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-30 23:28:34,527 - __main__ - INFO - ASR: 1.535s
2025-07-30 23:28:34,527 - __main__ - INFO - LLM: 14.081s
2025-07-30 23:28:34,527 - __main__ - INFO - TTS: 3.395s
2025-07-30 23:28:34,527 - __main__ - INFO - Playback: 29.429s
2025-07-30 23:28:34,527 - __main__ - INFO - TOTAL: 48.440s
2025-07-30 23:28:34,527 - __main__ - INFO - =====================================
2025-07-30 23:28:34,527 - __main__ - INFO - Utterance processing completed in 48.440s
2025-07-30 23:28:35,099 - __main__ - INFO - Utterance collected: duration=0.67s, chunks=21
2025-07-30 23:28:35,113 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 23:28:35,113 - __main__ - INFO - Starting utterance processing
2025-07-30 23:28:35,113 - __main__ - INFO - Starting transcription: audio_length=10752, duration=0.67s
2025-07-30 23:28:35,128 - __main__ - INFO - ASR preprocessing completed in 0.014s
2025-07-30 23:28:35,550 - __main__ - INFO - ASR generation completed in 0.422s
2025-07-30 23:28:35,552 - __main__ - INFO - ASR decoding completed in 0.001s
2025-07-30 23:28:35,552 - __main__ - INFO - ASR transcription completed in 0.439s: 'yeah'
2025-07-30 23:28:35,552 - __main__ - INFO - ASR processing completed in 0.439s
2025-07-30 23:28:35,553 - __main__ - INFO - User transcription: 'yeah'
2025-07-30 23:28:35,553 - __main__ - INFO - User message added to queue
2025-07-30 23:28:35,553 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 23:28:40,284 - __main__ - INFO - Ollama request sent in 4.731s
2025-07-30 23:28:40,284 - __main__ - INFO - Ollama response completed in 4.731s: length=203
2025-07-30 23:28:40,285 - __main__ - INFO - LLM processing completed in 4.731s
2025-07-30 23:28:40,285 - __main__ - INFO - Ollama response: 'It seems like you're just going through the motions. Would you like to talk about something specific...'
2025-07-30 23:28:40,285 - __main__ - INFO - Assistant message added to queue
2025-07-30 23:28:40,285 - __main__ - INFO - Starting TTS synthesis: text_length=203, voice=af_heart
2025-07-30 23:28:40,687 - __main__ - INFO - TTS generation completed in 0.402s
2025-07-30 23:28:40,687 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-30 23:28:40,687 - __main__ - INFO - TTS synthesis completed in 0.402s: audio_length=290400
2025-07-30 23:28:40,687 - __main__ - INFO - TTS processing completed in 0.402s
2025-07-30 23:28:40,687 - __main__ - INFO - Starting audio playback: duration=12.10s
2025-07-30 23:28:52,949 - __main__ - INFO - Audio playback completed in 12.261s
2025-07-30 23:28:52,950 - __main__ - INFO - Audio playback completed in 12.262s
2025-07-30 23:28:52,950 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-30 23:28:52,950 - __main__ - INFO - ASR: 0.439s
2025-07-30 23:28:52,950 - __main__ - INFO - LLM: 4.731s
2025-07-30 23:28:52,950 - __main__ - INFO - TTS: 0.402s
2025-07-30 23:28:52,950 - __main__ - INFO - Playback: 12.262s
2025-07-30 23:28:52,950 - __main__ - INFO - TOTAL: 17.837s
2025-07-30 23:28:52,950 - __main__ - INFO - =====================================
2025-07-30 23:28:52,950 - __main__ - INFO - Utterance processing completed in 17.837s
2025-07-30 23:28:52,984 - __main__ - INFO - Utterance collected: duration=0.06s, chunks=2
2025-07-30 23:28:52,984 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-30 23:28:55,809 - __main__ - INFO - Utterance collected: duration=0.03s, chunks=1
2025-07-30 23:28:55,810 - __main__ - INFO - VAD rejected but audio level is significant (0.0118), processing anyway
2025-07-30 23:28:55,810 - __main__ - INFO - Starting utterance processing
2025-07-30 23:28:55,810 - __main__ - INFO - Starting transcription: audio_length=512, duration=0.03s
2025-07-30 23:28:55,822 - __main__ - INFO - ASR preprocessing completed in 0.012s
2025-07-30 23:28:56,316 - __main__ - INFO - ASR generation completed in 0.493s
2025-07-30 23:28:56,317 - __main__ - INFO - ASR decoding completed in 0.001s
2025-07-30 23:28:56,317 - __main__ - INFO - ASR transcription completed in 0.507s: 'you'
2025-07-30 23:28:56,317 - __main__ - INFO - ASR processing completed in 0.507s
2025-07-30 23:28:56,317 - __main__ - INFO - User transcription: 'you'
2025-07-30 23:28:56,317 - __main__ - INFO - User message added to queue
2025-07-30 23:28:56,317 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 23:28:59,759 - __main__ - INFO - Ollama request sent in 3.442s
2025-07-30 23:28:59,759 - __main__ - INFO - Ollama response completed in 3.442s: length=101
2025-07-30 23:28:59,760 - __main__ - INFO - LLM processing completed in 3.443s
2025-07-30 23:28:59,760 - __main__ - INFO - Ollama response: 'I'm an artificial intelligence model known as Llama. Llama stands for "Large Language Model Meta AI....'
2025-07-30 23:28:59,760 - __main__ - INFO - Assistant message added to queue
2025-07-30 23:28:59,760 - __main__ - INFO - Starting TTS synthesis: text_length=101, voice=af_heart
2025-07-30 23:29:00,204 - __main__ - INFO - TTS generation completed in 0.444s
2025-07-30 23:29:00,205 - __main__ - INFO - TTS audio concatenation completed in 0.001s
2025-07-30 23:29:00,205 - __main__ - INFO - TTS synthesis completed in 0.445s: audio_length=171600
2025-07-30 23:29:00,205 - __main__ - INFO - TTS processing completed in 0.445s
2025-07-30 23:29:00,205 - __main__ - INFO - Starting audio playback: duration=7.15s
2025-07-30 23:29:07,463 - __main__ - INFO - Audio playback completed in 7.259s
2025-07-30 23:29:07,463 - __main__ - INFO - Audio playback completed in 7.259s
2025-07-30 23:29:07,463 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-30 23:29:07,463 - __main__ - INFO - ASR: 0.507s
2025-07-30 23:29:07,463 - __main__ - INFO - LLM: 3.443s
2025-07-30 23:29:07,463 - __main__ - INFO - TTS: 0.445s
2025-07-30 23:29:07,463 - __main__ - INFO - Playback: 7.259s
2025-07-30 23:29:07,463 - __main__ - INFO - TOTAL: 11.654s
2025-07-30 23:29:07,463 - __main__ - INFO - =====================================
2025-07-30 23:29:07,464 - __main__ - INFO - Utterance processing completed in 11.654s
2025-07-30 23:29:08,463 - __main__ - INFO - Utterance collected: duration=0.03s, chunks=1
2025-07-30 23:29:08,463 - __main__ - INFO - VAD rejected but audio level is significant (0.0022), processing anyway
2025-07-30 23:29:08,463 - __main__ - INFO - Starting utterance processing
2025-07-30 23:29:08,463 - __main__ - INFO - Starting transcription: audio_length=512, duration=0.03s
2025-07-30 23:29:08,476 - __main__ - INFO - ASR preprocessing completed in 0.014s
2025-07-30 23:29:08,950 - __main__ - INFO - ASR generation completed in 0.472s
2025-07-30 23:29:08,951 - __main__ - INFO - ASR decoding completed in 0.002s
2025-07-30 23:29:08,951 - __main__ - INFO - ASR transcription completed in 0.489s: 'you'
2025-07-30 23:29:08,951 - __main__ - INFO - ASR processing completed in 0.489s
2025-07-30 23:29:08,951 - __main__ - INFO - User transcription: 'you'
2025-07-30 23:29:08,951 - __main__ - INFO - User message added to queue
2025-07-30 23:29:08,951 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 23:29:12,370 - __main__ - INFO - Ollama request sent in 3.417s
2025-07-30 23:29:12,370 - __main__ - INFO - Ollama response completed in 3.419s: length=101
2025-07-30 23:29:12,371 - __main__ - INFO - LLM processing completed in 3.420s
2025-07-30 23:29:12,371 - __main__ - INFO - Ollama response: 'I'm an artificial intelligence model known as Llama. Llama stands for "Large Language Model Meta AI....'
2025-07-30 23:29:12,371 - __main__ - INFO - Assistant message added to queue
2025-07-30 23:29:12,371 - __main__ - INFO - Starting TTS synthesis: text_length=101, voice=af_heart
2025-07-30 23:29:12,659 - __main__ - INFO - TTS generation completed in 0.287s
2025-07-30 23:29:12,659 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-30 23:29:12,659 - __main__ - INFO - TTS synthesis completed in 0.287s: audio_length=171600
2025-07-30 23:29:12,659 - __main__ - INFO - TTS processing completed in 0.287s
2025-07-30 23:29:12,659 - __main__ - INFO - Starting audio playback: duration=7.15s
2025-07-30 23:29:19,964 - __main__ - INFO - Audio playback completed in 7.305s
2025-07-30 23:29:19,965 - __main__ - INFO - Audio playback completed in 7.307s
2025-07-30 23:29:19,965 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-30 23:29:19,965 - __main__ - INFO - ASR: 0.489s
2025-07-30 23:29:19,965 - __main__ - INFO - LLM: 3.420s
2025-07-30 23:29:19,965 - __main__ - INFO - TTS: 0.287s
2025-07-30 23:29:19,965 - __main__ - INFO - Playback: 7.307s
2025-07-30 23:29:19,965 - __main__ - INFO - TOTAL: 11.503s
2025-07-30 23:29:19,965 - __main__ - INFO - =====================================
2025-07-30 23:29:19,965 - __main__ - INFO - Utterance processing completed in 11.503s
2025-07-30 23:29:23,025 - __main__ - INFO - Utterance collected: duration=1.12s, chunks=35
2025-07-30 23:29:23,046 - __main__ - INFO - VAD rejected but audio level is significant (0.0023), processing anyway
2025-07-30 23:29:23,046 - __main__ - INFO - Starting utterance processing
2025-07-30 23:29:23,046 - __main__ - INFO - Starting transcription: audio_length=17920, duration=1.12s
2025-07-30 23:29:23,059 - __main__ - INFO - ASR preprocessing completed in 0.013s
2025-07-30 23:29:23,560 - __main__ - INFO - ASR generation completed in 0.500s
2025-07-30 23:29:23,561 - __main__ - INFO - ASR decoding completed in 0.002s
2025-07-30 23:29:23,561 - __main__ - INFO - ASR transcription completed in 0.515s: 'thank you'
2025-07-30 23:29:23,562 - __main__ - INFO - ASR processing completed in 0.516s
2025-07-30 23:29:23,562 - __main__ - INFO - User transcription: 'thank you'
2025-07-30 23:29:23,562 - __main__ - INFO - User message added to queue
2025-07-30 23:29:23,562 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 23:29:27,270 - __main__ - INFO - Ollama request sent in 3.707s
2025-07-30 23:29:27,271 - __main__ - INFO - Ollama response completed in 3.708s: length=123
2025-07-30 23:29:27,272 - __main__ - INFO - LLM processing completed in 3.710s
2025-07-30 23:29:27,273 - __main__ - INFO - Ollama response: 'It was nice chatting with you. Is there anything else I can help you with or would you like to just ...'
2025-07-30 23:29:27,273 - __main__ - INFO - Assistant message added to queue
2025-07-30 23:29:27,273 - __main__ - INFO - Starting TTS synthesis: text_length=123, voice=af_heart
2025-07-30 23:29:27,819 - __main__ - INFO - TTS generation completed in 0.547s
2025-07-30 23:29:27,819 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-30 23:29:27,819 - __main__ - INFO - TTS synthesis completed in 0.547s: audio_length=169200
2025-07-30 23:29:27,819 - __main__ - INFO - TTS processing completed in 0.547s
2025-07-30 23:29:27,819 - __main__ - INFO - Starting audio playback: duration=7.05s
2025-07-30 23:29:34,983 - __main__ - INFO - Audio playback completed in 7.164s
2025-07-30 23:29:34,985 - __main__ - INFO - Audio playback completed in 7.166s
2025-07-30 23:29:34,985 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-30 23:29:34,985 - __main__ - INFO - ASR: 0.516s
2025-07-30 23:29:34,985 - __main__ - INFO - LLM: 3.710s
2025-07-30 23:29:34,985 - __main__ - INFO - TTS: 0.547s
2025-07-30 23:29:34,985 - __main__ - INFO - Playback: 7.166s
2025-07-30 23:29:34,985 - __main__ - INFO - TOTAL: 11.939s
2025-07-30 23:29:34,985 - __main__ - INFO - =====================================
2025-07-30 23:29:34,985 - __main__ - INFO - Utterance processing completed in 11.939s
2025-07-30 23:29:36,300 - __main__ - INFO - Utterance collected: duration=0.42s, chunks=13
2025-07-30 23:29:36,308 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-30 23:29:36,308 - __main__ - INFO - Starting utterance processing
2025-07-30 23:29:36,308 - __main__ - INFO - Starting transcription: audio_length=6656, duration=0.42s
2025-07-30 23:29:36,321 - __main__ - INFO - ASR preprocessing completed in 0.014s
2025-07-30 23:29:36,735 - __main__ - INFO - ASR generation completed in 0.414s
2025-07-30 23:29:36,737 - __main__ - INFO - ASR decoding completed in 0.002s
2025-07-30 23:29:36,738 - __main__ - INFO - ASR transcription completed in 0.429s: 'yeah'
2025-07-30 23:29:36,738 - __main__ - INFO - ASR processing completed in 0.430s
2025-07-30 23:29:36,738 - __main__ - INFO - User transcription: 'yeah'
2025-07-30 23:29:36,739 - __main__ - INFO - User message added to queue
2025-07-30 23:29:36,739 - __main__ - INFO - Sending chat request to Ollama: model=llama3.2:1b, messages_count=1
2025-07-30 23:29:40,507 - __main__ - INFO - Ollama request sent in 3.769s
2025-07-30 23:29:40,507 - __main__ - INFO - Ollama response completed in 3.769s: length=133
2025-07-30 23:29:40,509 - __main__ - INFO - LLM processing completed in 3.770s
2025-07-30 23:29:40,509 - __main__ - INFO - Ollama response: 'It seems like you were about to say something, but it got cut off. Would you like to finish your tho...'
2025-07-30 23:29:40,509 - __main__ - INFO - Assistant message added to queue
2025-07-30 23:29:40,509 - __main__ - INFO - Starting TTS synthesis: text_length=133, voice=af_heart
2025-07-30 23:29:40,868 - __main__ - INFO - TTS generation completed in 0.359s
2025-07-30 23:29:40,869 - __main__ - INFO - TTS audio concatenation completed in 0.001s
2025-07-30 23:29:40,869 - __main__ - INFO - TTS synthesis completed in 0.360s: audio_length=179400
2025-07-30 23:29:40,869 - __main__ - INFO - TTS processing completed in 0.360s
2025-07-30 23:29:40,869 - __main__ - INFO - Starting audio playback: duration=7.47s
2025-07-30 23:29:48,467 - __main__ - INFO - Audio playback completed in 7.598s
2025-07-30 23:29:48,467 - __main__ - INFO - Audio playback completed in 7.598s
2025-07-30 23:29:48,467 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-30 23:29:48,467 - __main__ - INFO - ASR: 0.430s
2025-07-30 23:29:48,467 - __main__ - INFO - LLM: 3.770s
2025-07-30 23:29:48,467 - __main__ - INFO - TTS: 0.360s
2025-07-30 23:29:48,467 - __main__ - INFO - Playback: 7.598s
2025-07-30 23:29:48,467 - __main__ - INFO - TOTAL: 12.159s
2025-07-30 23:29:48,468 - __main__ - INFO - =====================================
2025-07-30 23:29:48,468 - __main__ - INFO - Utterance processing completed in 12.160s
2025-07-30 23:29:48,523 - __main__ - INFO - Utterance collected: duration=0.03s, chunks=1
2025-07-30 23:29:48,523 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-30 23:29:50,799 - __main__ - INFO - Stopping recording
2025-07-30 23:29:50,891 - __main__ - INFO - === CONTINUOUS PROCESSING SUMMARY ===
2025-07-30 23:29:50,891 - __main__ - INFO - Total processing time: 129.719s
2025-07-30 23:29:50,891 - __main__ - INFO - Chunks processed: 776
2025-07-30 23:29:50,891 - __main__ - INFO - Utterances processed: 6
2025-07-30 23:29:50,891 - __main__ - INFO - Average time per utterance: 21.620s
2025-07-30 23:29:50,891 - __main__ - INFO - =====================================
2025-07-30 23:29:50,905 - __main__ - INFO - Audio stream closed successfully
2025-07-30 23:29:50,905 - __main__ - INFO - Sentinel value added to queue
2025-07-30 23:29:50,905 - __main__ - INFO - Recording stopped. Total chunks recorded: 3975
2025-07-31 01:58:13,466 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 01:58:18,693 - __main__ - INFO - Loading all models
2025-07-31 01:58:27,023 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-large-v3
2025-07-31 01:59:19,586 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 01:59:19,597 - __main__ - INFO - ASR model loaded in 60.902s
2025-07-31 01:59:29,580 - __main__ - INFO - LLM model selected in 9.983s
2025-07-31 01:59:29,580 - __main__ - INFO - Initializing TTS Processor
2025-07-31 01:59:34,747 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 01:59:34,749 - __main__ - INFO - TTS model loaded in 5.168s
2025-07-31 01:59:34,749 - __main__ - INFO - Initializing VAD Processor
2025-07-31 01:59:35,375 - __main__ - INFO - VAD model loaded successfully
2025-07-31 01:59:35,375 - __main__ - INFO - VAD model loaded in 0.626s
2025-07-31 01:59:35,375 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 01:59:35,375 - __main__ - INFO - ASR: 60.902s
2025-07-31 01:59:35,375 - __main__ - INFO - LLM Selection: 9.983s
2025-07-31 01:59:35,376 - __main__ - INFO - TTS: 5.168s
2025-07-31 01:59:35,376 - __main__ - INFO - VAD: 0.626s
2025-07-31 01:59:35,376 - __main__ - INFO - TOTAL: 76.682s
2025-07-31 01:59:35,376 - __main__ - INFO - =============================
2025-07-31 01:59:35,376 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 01:59:35,969 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 01:59:58,287 - __main__ - INFO - Starting continuous recording
2025-07-31 01:59:58,965 - __main__ - INFO - Recording started successfully
2025-07-31 01:59:58,967 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 01:59:58,967 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 01:59:58,967 - __main__ - INFO - Initializing LiteLLM client
2025-07-31 02:00:02,403 - __main__ - INFO - Utterance collected: duration=0.77s, chunks=24
2025-07-31 02:00:02,421 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 02:00:02,421 - __main__ - INFO - Starting utterance processing
2025-07-31 02:00:02,421 - __main__ - INFO - Starting transcription: audio_length=12288, duration=0.77s
2025-07-31 02:00:02,474 - __main__ - INFO - ASR preprocessing completed in 0.054s
2025-07-31 02:00:31,323 - __main__ - INFO - ASR generation completed in 28.849s
2025-07-31 02:00:31,375 - __main__ - INFO - ASR decoding completed in 0.051s
2025-07-31 02:00:31,375 - __main__ - INFO - ASR transcription completed in 28.954s: 'hello'
2025-07-31 02:00:31,376 - __main__ - INFO - ASR processing completed in 28.955s
2025-07-31 02:00:31,376 - __main__ - INFO - User transcription: 'hello'
2025-07-31 02:00:31,376 - __main__ - INFO - User message added to queue
2025-07-31 02:00:31,376 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 02:00:31,636 - __main__ - ERROR - LiteLLM error after 0.260s: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama3.2:1b
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-07-31 02:00:31,636 - __main__ - INFO - LLM processing completed in 0.260s
2025-07-31 02:00:31,636 - __main__ - INFO - Ollama response: 'Sorry, I'm having trouble connecting to the AI model....'
2025-07-31 02:00:31,636 - __main__ - INFO - Assistant message added to queue
2025-07-31 02:00:31,636 - __main__ - INFO - Starting TTS synthesis: text_length=53, voice=af_heart
2025-07-31 02:00:35,514 - __main__ - INFO - TTS generation completed in 3.877s
2025-07-31 02:00:35,514 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 02:00:35,514 - __main__ - INFO - TTS synthesis completed in 3.878s: audio_length=87600
2025-07-31 02:00:35,514 - __main__ - INFO - TTS processing completed in 3.878s
2025-07-31 02:00:35,515 - __main__ - INFO - Starting audio playback: duration=3.65s
2025-07-31 02:00:39,376 - __main__ - INFO - Audio playback completed in 3.861s
2025-07-31 02:00:39,376 - __main__ - INFO - Audio playback completed in 3.861s
2025-07-31 02:00:39,376 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 02:00:39,376 - __main__ - INFO - ASR: 28.955s
2025-07-31 02:00:39,376 - __main__ - INFO - LLM: 0.260s
2025-07-31 02:00:39,376 - __main__ - INFO - TTS: 3.878s
2025-07-31 02:00:39,376 - __main__ - INFO - Playback: 3.861s
2025-07-31 02:00:39,376 - __main__ - INFO - TOTAL: 36.955s
2025-07-31 02:00:39,376 - __main__ - INFO - =====================================
2025-07-31 02:00:39,376 - __main__ - INFO - Utterance processing completed in 36.955s
2025-07-31 02:00:39,418 - __main__ - INFO - Utterance collected: duration=0.10s, chunks=3
2025-07-31 02:00:39,418 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 02:05:16,853 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 02:05:22,243 - __main__ - INFO - Loading all models
2025-07-31 02:05:30,830 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 02:05:37,025 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 02:05:37,025 - __main__ - INFO - ASR model loaded in 14.782s
2025-07-31 02:05:39,387 - __main__ - INFO - LLM model selected in 2.362s
2025-07-31 02:05:39,387 - __main__ - INFO - Initializing TTS Processor
2025-07-31 02:05:42,860 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 02:05:42,860 - __main__ - INFO - TTS model loaded in 3.473s
2025-07-31 02:05:42,860 - __main__ - INFO - Initializing VAD Processor
2025-07-31 02:05:43,442 - __main__ - INFO - VAD model loaded successfully
2025-07-31 02:05:43,442 - __main__ - INFO - VAD model loaded in 0.582s
2025-07-31 02:05:43,442 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 02:05:43,443 - __main__ - INFO - ASR: 14.782s
2025-07-31 02:05:43,443 - __main__ - INFO - LLM Selection: 2.362s
2025-07-31 02:05:43,443 - __main__ - INFO - TTS: 3.473s
2025-07-31 02:05:43,443 - __main__ - INFO - VAD: 0.582s
2025-07-31 02:05:43,443 - __main__ - INFO - TOTAL: 21.200s
2025-07-31 02:05:43,443 - __main__ - INFO - =============================
2025-07-31 02:05:43,443 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 02:05:43,527 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 02:05:45,125 - __main__ - INFO - Starting continuous recording
2025-07-31 02:05:45,348 - __main__ - INFO - Recording started successfully
2025-07-31 02:05:45,348 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 02:05:45,348 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 02:05:45,348 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 02:05:52,245 - __main__ - INFO - Utterance collected: duration=0.64s, chunks=20
2025-07-31 02:05:52,264 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 02:05:52,265 - __main__ - INFO - Starting utterance processing
2025-07-31 02:05:52,265 - __main__ - INFO - Starting transcription: audio_length=10240, duration=0.64s
2025-07-31 02:05:52,309 - __main__ - INFO - ASR preprocessing completed in 0.044s
2025-07-31 02:05:53,802 - __main__ - INFO - ASR generation completed in 1.493s
2025-07-31 02:05:53,807 - __main__ - INFO - ASR decoding completed in 0.005s
2025-07-31 02:05:53,807 - __main__ - INFO - ASR transcription completed in 1.542s: 'you'
2025-07-31 02:05:53,808 - __main__ - INFO - ASR processing completed in 1.543s
2025-07-31 02:05:53,808 - __main__ - INFO - User transcription: 'you'
2025-07-31 02:05:53,808 - __main__ - INFO - User message added to queue
2025-07-31 02:05:53,808 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 02:05:53,844 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 02:06:02,445 - __main__ - ERROR - LiteLLM error after 8.638s: litellm.APIConnectionError: OllamaException - [WinError 10061] No connection could be made because the target machine actively refused it
2025-07-31 02:06:02,445 - __main__ - INFO - LLM processing completed in 8.638s
2025-07-31 02:06:02,445 - __main__ - INFO - Ollama response: 'Sorry, I'm having trouble connecting to the AI model....'
2025-07-31 02:06:02,445 - __main__ - INFO - Assistant message added to queue
2025-07-31 02:06:02,446 - __main__ - INFO - Starting TTS synthesis: text_length=53, voice=af_heart
2025-07-31 02:06:04,585 - __main__ - INFO - TTS generation completed in 2.138s
2025-07-31 02:06:04,585 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 02:06:04,585 - __main__ - INFO - TTS synthesis completed in 2.139s: audio_length=87600
2025-07-31 02:06:04,586 - __main__ - INFO - TTS processing completed in 2.139s
2025-07-31 02:06:04,586 - __main__ - INFO - Starting audio playback: duration=3.65s
2025-07-31 02:06:08,436 - __main__ - INFO - Audio playback completed in 3.850s
2025-07-31 02:06:08,436 - __main__ - INFO - Audio playback completed in 3.850s
2025-07-31 02:06:08,436 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 02:06:08,436 - __main__ - INFO - ASR: 1.543s
2025-07-31 02:06:08,436 - __main__ - INFO - LLM: 8.638s
2025-07-31 02:06:08,436 - __main__ - INFO - TTS: 2.139s
2025-07-31 02:06:08,436 - __main__ - INFO - Playback: 3.850s
2025-07-31 02:06:08,437 - __main__ - INFO - TOTAL: 16.171s
2025-07-31 02:06:08,437 - __main__ - INFO - =====================================
2025-07-31 02:06:08,437 - __main__ - INFO - Utterance processing completed in 16.172s
2025-07-31 02:06:08,492 - __main__ - INFO - Utterance collected: duration=0.13s, chunks=4
2025-07-31 02:06:08,493 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 02:06:47,952 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 02:06:52,405 - __main__ - INFO - Loading all models
2025-07-31 02:06:53,761 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 02:06:59,561 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 02:06:59,561 - __main__ - INFO - ASR model loaded in 7.156s
2025-07-31 02:07:02,534 - __main__ - INFO - LLM model selected in 2.973s
2025-07-31 02:07:02,534 - __main__ - INFO - Initializing TTS Processor
2025-07-31 02:07:05,784 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 02:07:05,784 - __main__ - INFO - TTS model loaded in 3.250s
2025-07-31 02:07:05,784 - __main__ - INFO - Initializing VAD Processor
2025-07-31 02:07:06,354 - __main__ - INFO - VAD model loaded successfully
2025-07-31 02:07:06,354 - __main__ - INFO - VAD model loaded in 0.570s
2025-07-31 02:07:06,354 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 02:07:06,354 - __main__ - INFO - ASR: 7.156s
2025-07-31 02:07:06,354 - __main__ - INFO - LLM Selection: 2.973s
2025-07-31 02:07:06,354 - __main__ - INFO - TTS: 3.250s
2025-07-31 02:07:06,355 - __main__ - INFO - VAD: 0.570s
2025-07-31 02:07:06,355 - __main__ - INFO - TOTAL: 13.949s
2025-07-31 02:07:06,355 - __main__ - INFO - =============================
2025-07-31 02:07:06,355 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 02:07:06,649 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 02:07:07,445 - __main__ - INFO - Starting continuous recording
2025-07-31 02:07:07,696 - __main__ - INFO - Recording started successfully
2025-07-31 02:07:07,697 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 02:07:07,697 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 02:07:07,697 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 02:07:10,683 - __main__ - INFO - Utterance collected: duration=0.64s, chunks=20
2025-07-31 02:07:10,708 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 02:07:10,708 - __main__ - INFO - Starting utterance processing
2025-07-31 02:07:10,708 - __main__ - INFO - Starting transcription: audio_length=10240, duration=0.64s
2025-07-31 02:07:10,748 - __main__ - INFO - ASR preprocessing completed in 0.039s
2025-07-31 02:07:12,172 - __main__ - INFO - ASR generation completed in 1.425s
2025-07-31 02:07:12,177 - __main__ - INFO - ASR decoding completed in 0.005s
2025-07-31 02:07:12,177 - __main__ - INFO - ASR transcription completed in 1.469s: 'channel'
2025-07-31 02:07:12,178 - __main__ - INFO - ASR processing completed in 1.470s
2025-07-31 02:07:12,178 - __main__ - INFO - User transcription: 'channel'
2025-07-31 02:07:12,178 - __main__ - INFO - User message added to queue
2025-07-31 02:07:12,178 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 02:07:12,211 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 02:07:14,363 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:07:21,575 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 02:07:23,716 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:07:23,717 - __main__ - ERROR - LiteLLM error after 11.539s: 'CustomStreamWrapper' object is not subscriptable
2025-07-31 02:07:23,718 - __main__ - INFO - LLM processing completed in 11.540s
2025-07-31 02:07:23,718 - __main__ - INFO - Ollama response: 'Sorry, I'm having trouble connecting to the AI model....'
2025-07-31 02:07:23,718 - __main__ - INFO - Assistant message added to queue
2025-07-31 02:07:23,718 - __main__ - INFO - Starting TTS synthesis: text_length=53, voice=af_heart
2025-07-31 02:07:25,656 - __main__ - INFO - TTS generation completed in 1.938s
2025-07-31 02:07:25,656 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 02:07:25,656 - __main__ - INFO - TTS synthesis completed in 1.938s: audio_length=87600
2025-07-31 02:07:25,656 - __main__ - INFO - TTS processing completed in 1.938s
2025-07-31 02:07:25,656 - __main__ - INFO - Starting audio playback: duration=3.65s
2025-07-31 02:07:29,519 - __main__ - INFO - Audio playback completed in 3.864s
2025-07-31 02:07:29,519 - __main__ - INFO - Audio playback completed in 3.864s
2025-07-31 02:07:29,519 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 02:07:29,519 - __main__ - INFO - ASR: 1.470s
2025-07-31 02:07:29,519 - __main__ - INFO - LLM: 11.540s
2025-07-31 02:07:29,519 - __main__ - INFO - TTS: 1.938s
2025-07-31 02:07:29,519 - __main__ - INFO - Playback: 3.864s
2025-07-31 02:07:29,519 - __main__ - INFO - TOTAL: 18.811s
2025-07-31 02:07:29,519 - __main__ - INFO - =====================================
2025-07-31 02:07:29,519 - __main__ - INFO - Utterance processing completed in 18.811s
2025-07-31 02:07:29,561 - __main__ - INFO - Utterance collected: duration=0.06s, chunks=2
2025-07-31 02:07:29,562 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 02:10:30,441 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 02:10:35,205 - __main__ - INFO - Loading all models
2025-07-31 02:10:39,108 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 02:10:45,156 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 02:10:45,157 - __main__ - INFO - ASR model loaded in 9.952s
2025-07-31 02:10:48,389 - __main__ - INFO - LLM model selected in 3.232s
2025-07-31 02:10:48,389 - __main__ - INFO - Initializing TTS Processor
2025-07-31 02:10:51,820 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 02:10:51,821 - __main__ - INFO - TTS model loaded in 3.432s
2025-07-31 02:10:51,821 - __main__ - INFO - Initializing VAD Processor
2025-07-31 02:10:52,425 - __main__ - INFO - VAD model loaded successfully
2025-07-31 02:10:52,426 - __main__ - INFO - VAD model loaded in 0.605s
2025-07-31 02:10:52,426 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 02:10:52,426 - __main__ - INFO - ASR: 9.952s
2025-07-31 02:10:52,426 - __main__ - INFO - LLM Selection: 3.232s
2025-07-31 02:10:52,426 - __main__ - INFO - TTS: 3.432s
2025-07-31 02:10:52,426 - __main__ - INFO - VAD: 0.605s
2025-07-31 02:10:52,426 - __main__ - INFO - TOTAL: 17.221s
2025-07-31 02:10:52,426 - __main__ - INFO - =============================
2025-07-31 02:10:52,426 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 02:10:52,862 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 02:10:53,317 - __main__ - INFO - Starting continuous recording
2025-07-31 02:10:53,584 - __main__ - INFO - Recording started successfully
2025-07-31 02:10:53,587 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 02:10:53,587 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 02:10:53,587 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 02:10:56,318 - __main__ - INFO - Utterance collected: duration=0.70s, chunks=22
2025-07-31 02:10:56,338 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 02:10:56,338 - __main__ - INFO - Starting utterance processing
2025-07-31 02:10:56,338 - __main__ - INFO - Starting transcription: audio_length=11264, duration=0.70s
2025-07-31 02:10:56,385 - __main__ - INFO - ASR preprocessing completed in 0.046s
2025-07-31 02:10:58,024 - __main__ - INFO - ASR generation completed in 1.640s
2025-07-31 02:10:58,033 - __main__ - INFO - ASR decoding completed in 0.008s
2025-07-31 02:10:58,033 - __main__ - INFO - ASR transcription completed in 1.695s: 'hello'
2025-07-31 02:10:58,035 - __main__ - INFO - ASR processing completed in 1.697s
2025-07-31 02:10:58,035 - __main__ - INFO - User transcription: 'hello'
2025-07-31 02:10:58,035 - __main__ - INFO - User message added to queue
2025-07-31 02:10:58,035 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 02:10:58,108 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 02:11:00,235 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:11:03,016 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 02:11:03,083 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:11:03,161 - __main__ - ERROR - LiteLLM error after 5.126s: sequence item 7: expected str instance, NoneType found
2025-07-31 02:11:03,161 - __main__ - INFO - LLM processing completed in 5.126s
2025-07-31 02:11:03,162 - __main__ - INFO - Ollama response: 'Sorry, I'm having trouble connecting to the AI model....'
2025-07-31 02:11:03,162 - __main__ - INFO - Assistant message added to queue
2025-07-31 02:11:03,162 - __main__ - INFO - Starting TTS synthesis: text_length=53, voice=af_heart
2025-07-31 02:11:03,216 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:11:03,274 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:11:05,208 - __main__ - INFO - TTS generation completed in 2.046s
2025-07-31 02:11:05,208 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 02:11:05,208 - __main__ - INFO - TTS synthesis completed in 2.046s: audio_length=87600
2025-07-31 02:11:05,208 - __main__ - INFO - TTS processing completed in 2.046s
2025-07-31 02:11:05,208 - __main__ - INFO - Starting audio playback: duration=3.65s
2025-07-31 02:11:08,647 - __main__ - INFO - Audio playback completed in 3.439s
2025-07-31 02:11:08,647 - __main__ - INFO - Audio playback completed in 3.439s
2025-07-31 02:11:08,647 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 02:11:08,647 - __main__ - INFO - ASR: 1.697s
2025-07-31 02:11:08,647 - __main__ - INFO - LLM: 5.126s
2025-07-31 02:11:08,647 - __main__ - INFO - TTS: 2.046s
2025-07-31 02:11:08,647 - __main__ - INFO - Playback: 3.439s
2025-07-31 02:11:08,647 - __main__ - INFO - TOTAL: 12.309s
2025-07-31 02:11:08,647 - __main__ - INFO - =====================================
2025-07-31 02:11:08,647 - __main__ - INFO - Utterance processing completed in 12.309s
2025-07-31 02:12:39,012 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 02:12:43,542 - __main__ - INFO - Loading all models
2025-07-31 02:12:45,683 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 02:12:51,620 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 02:12:51,620 - __main__ - INFO - ASR model loaded in 8.077s
2025-07-31 02:12:55,586 - __main__ - INFO - LLM model selected in 3.966s
2025-07-31 02:12:55,587 - __main__ - INFO - Initializing TTS Processor
2025-07-31 02:12:59,457 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 02:12:59,457 - __main__ - INFO - TTS model loaded in 3.870s
2025-07-31 02:12:59,457 - __main__ - INFO - Initializing VAD Processor
2025-07-31 02:13:00,084 - __main__ - INFO - VAD model loaded successfully
2025-07-31 02:13:00,084 - __main__ - INFO - VAD model loaded in 0.627s
2025-07-31 02:13:00,084 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 02:13:00,084 - __main__ - INFO - ASR: 8.077s
2025-07-31 02:13:00,084 - __main__ - INFO - LLM Selection: 3.966s
2025-07-31 02:13:00,084 - __main__ - INFO - TTS: 3.870s
2025-07-31 02:13:00,084 - __main__ - INFO - VAD: 0.627s
2025-07-31 02:13:00,084 - __main__ - INFO - TOTAL: 16.541s
2025-07-31 02:13:00,084 - __main__ - INFO - =============================
2025-07-31 02:13:00,084 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 02:13:00,367 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 02:13:00,660 - __main__ - INFO - Starting continuous recording
2025-07-31 02:13:00,915 - __main__ - INFO - Recording started successfully
2025-07-31 02:13:00,916 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 02:13:00,917 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 02:13:00,917 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 02:13:03,839 - __main__ - INFO - Utterance collected: duration=0.74s, chunks=23
2025-07-31 02:13:03,867 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 02:13:03,867 - __main__ - INFO - Starting utterance processing
2025-07-31 02:13:03,867 - __main__ - INFO - Starting transcription: audio_length=11776, duration=0.74s
2025-07-31 02:13:03,904 - __main__ - INFO - ASR preprocessing completed in 0.037s
2025-07-31 02:13:05,273 - __main__ - INFO - ASR generation completed in 1.368s
2025-07-31 02:13:05,277 - __main__ - INFO - ASR decoding completed in 0.004s
2025-07-31 02:13:05,277 - __main__ - INFO - ASR transcription completed in 1.411s: 'hello'
2025-07-31 02:13:05,280 - __main__ - INFO - ASR processing completed in 1.413s
2025-07-31 02:13:05,280 - __main__ - INFO - User transcription: 'hello'
2025-07-31 02:13:05,280 - __main__ - INFO - User message added to queue
2025-07-31 02:13:05,280 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 02:13:05,320 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 02:13:07,414 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:13:10,179 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 02:13:10,194 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 02:13:10,257 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:13:10,316 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:13:10,367 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:13:10,424 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:13:12,294 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:13:12,296 - __main__ - INFO - LiteLLM response completed in 7.016s: length=0
2025-07-31 02:13:12,297 - __main__ - INFO - LLM processing completed in 7.017s
2025-07-31 02:13:12,297 - __main__ - WARNING - Ollama returned empty response after 8.430s
2025-07-31 02:13:12,297 - __main__ - INFO - Utterance processing completed in 8.430s
2025-07-31 02:15:09,476 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 02:15:13,671 - __main__ - INFO - Loading all models
2025-07-31 02:15:16,011 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 02:15:21,823 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 02:15:21,823 - __main__ - INFO - ASR model loaded in 8.152s
2025-07-31 02:15:23,246 - __main__ - INFO - LLM model selected in 1.423s
2025-07-31 02:15:23,247 - __main__ - INFO - Initializing TTS Processor
2025-07-31 02:15:26,303 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 02:15:26,303 - __main__ - INFO - TTS model loaded in 3.056s
2025-07-31 02:15:26,303 - __main__ - INFO - Initializing VAD Processor
2025-07-31 02:15:26,900 - __main__ - INFO - VAD model loaded successfully
2025-07-31 02:15:26,900 - __main__ - INFO - VAD model loaded in 0.597s
2025-07-31 02:15:26,900 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 02:15:26,900 - __main__ - INFO - ASR: 8.152s
2025-07-31 02:15:26,900 - __main__ - INFO - LLM Selection: 1.423s
2025-07-31 02:15:26,900 - __main__ - INFO - TTS: 3.056s
2025-07-31 02:15:26,900 - __main__ - INFO - VAD: 0.597s
2025-07-31 02:15:26,900 - __main__ - INFO - TOTAL: 13.229s
2025-07-31 02:15:26,900 - __main__ - INFO - =============================
2025-07-31 02:15:26,900 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 02:15:27,187 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 02:15:33,912 - __main__ - INFO - Starting continuous recording
2025-07-31 02:15:34,432 - __main__ - INFO - Recording started successfully
2025-07-31 02:15:34,433 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 02:15:34,433 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 02:15:34,433 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 02:15:37,966 - __main__ - INFO - Utterance collected: duration=0.29s, chunks=9
2025-07-31 02:15:37,976 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 02:15:37,976 - __main__ - INFO - Starting utterance processing
2025-07-31 02:15:37,976 - __main__ - INFO - Starting transcription: audio_length=4608, duration=0.29s
2025-07-31 02:15:38,014 - __main__ - INFO - ASR preprocessing completed in 0.038s
2025-07-31 02:15:39,370 - __main__ - INFO - ASR generation completed in 1.356s
2025-07-31 02:15:39,375 - __main__ - INFO - ASR decoding completed in 0.005s
2025-07-31 02:15:39,375 - __main__ - INFO - ASR transcription completed in 1.399s: 'good'
2025-07-31 02:15:39,376 - __main__ - INFO - ASR processing completed in 1.400s
2025-07-31 02:15:39,376 - __main__ - INFO - User transcription: 'good'
2025-07-31 02:15:39,376 - __main__ - INFO - User message added to queue
2025-07-31 02:15:39,376 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 02:15:39,411 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 02:15:41,499 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:15:45,013 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 02:15:45,016 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 02:15:45,089 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:15:45,157 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:15:45,216 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:15:45,272 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:15:47,120 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:15:47,123 - __main__ - INFO - LiteLLM response completed in 7.746s: length=0
2025-07-31 02:15:47,125 - __main__ - INFO - LLM processing completed in 7.748s
2025-07-31 02:15:47,125 - __main__ - WARNING - Ollama returned empty response after 9.148s
2025-07-31 02:15:47,125 - __main__ - INFO - Utterance processing completed in 9.148s
2025-07-31 02:15:47,156 - __main__ - INFO - Utterance collected: duration=0.03s, chunks=1
2025-07-31 02:15:47,156 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 02:49:08,153 - __main__ - INFO - Loading all models
2025-07-31 02:49:14,177 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 02:49:22,180 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 02:49:22,180 - __main__ - INFO - ASR model loaded in 14.027s
2025-07-31 02:49:24,023 - __main__ - INFO - LLM model selected in 1.843s
2025-07-31 02:49:24,023 - __main__ - INFO - Initializing TTS Processor
2025-07-31 02:49:27,623 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 02:49:27,623 - __main__ - INFO - TTS model loaded in 3.600s
2025-07-31 02:49:27,623 - __main__ - INFO - Initializing VAD Processor
2025-07-31 02:49:28,218 - __main__ - INFO - VAD model loaded successfully
2025-07-31 02:49:28,218 - __main__ - INFO - VAD model loaded in 0.594s
2025-07-31 02:49:28,218 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 02:49:28,218 - __main__ - INFO - ASR: 14.027s
2025-07-31 02:49:28,218 - __main__ - INFO - LLM Selection: 1.843s
2025-07-31 02:49:28,218 - __main__ - INFO - TTS: 3.600s
2025-07-31 02:49:28,218 - __main__ - INFO - VAD: 0.594s
2025-07-31 02:49:28,218 - __main__ - INFO - TOTAL: 20.065s
2025-07-31 02:49:28,218 - __main__ - INFO - =============================
2025-07-31 02:49:28,218 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 02:49:28,283 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 02:49:29,377 - __main__ - INFO - Starting continuous recording
2025-07-31 02:49:29,598 - __main__ - INFO - Recording started successfully
2025-07-31 02:49:29,601 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 02:49:29,601 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 02:49:29,601 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 02:49:33,198 - __main__ - INFO - Utterance collected: duration=0.61s, chunks=19
2025-07-31 02:49:33,216 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 02:49:33,216 - __main__ - INFO - Starting utterance processing
2025-07-31 02:49:33,216 - __main__ - INFO - Starting transcription: audio_length=9728, duration=0.61s
2025-07-31 02:49:33,271 - __main__ - INFO - ASR preprocessing completed in 0.055s
2025-07-31 02:49:34,654 - __main__ - INFO - ASR generation completed in 1.383s
2025-07-31 02:49:34,659 - __main__ - INFO - ASR decoding completed in 0.005s
2025-07-31 02:49:34,659 - __main__ - INFO - ASR transcription completed in 1.443s: 'hello'
2025-07-31 02:49:34,660 - __main__ - INFO - ASR processing completed in 1.444s
2025-07-31 02:49:34,660 - __main__ - INFO - User transcription: 'hello'
2025-07-31 02:49:34,660 - __main__ - INFO - User message added to queue
2025-07-31 02:49:34,661 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 02:49:34,701 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 02:49:36,954 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:49:45,108 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 02:49:45,111 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 02:49:47,251 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:49:47,264 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:49:47,265 - __main__ - INFO - LiteLLM response completed in 12.604s: length=0
2025-07-31 02:49:47,265 - __main__ - INFO - LLM processing completed in 12.604s
2025-07-31 02:49:47,265 - __main__ - WARNING - Ollama returned empty response after 14.050s
2025-07-31 02:49:47,265 - __main__ - INFO - Utterance processing completed in 14.050s
2025-07-31 02:49:47,309 - __main__ - INFO - Utterance collected: duration=0.10s, chunks=3
2025-07-31 02:49:47,309 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 02:49:47,359 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:49:47,465 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:52:41,751 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 02:52:46,653 - __main__ - INFO - Loading all models
2025-07-31 02:52:56,457 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 02:53:04,752 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 02:53:04,753 - __main__ - INFO - ASR model loaded in 18.099s
2025-07-31 02:53:06,876 - __main__ - INFO - LLM model selected in 2.123s
2025-07-31 02:53:06,876 - __main__ - INFO - Initializing TTS Processor
2025-07-31 02:53:17,867 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 02:53:17,867 - __main__ - INFO - TTS model loaded in 10.991s
2025-07-31 02:53:17,867 - __main__ - INFO - Initializing VAD Processor
2025-07-31 02:53:18,466 - __main__ - INFO - VAD model loaded successfully
2025-07-31 02:53:18,466 - __main__ - INFO - VAD model loaded in 0.599s
2025-07-31 02:53:18,466 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 02:53:18,466 - __main__ - INFO - ASR: 18.099s
2025-07-31 02:53:18,466 - __main__ - INFO - LLM Selection: 2.123s
2025-07-31 02:53:18,466 - __main__ - INFO - TTS: 10.991s
2025-07-31 02:53:18,466 - __main__ - INFO - VAD: 0.599s
2025-07-31 02:53:18,466 - __main__ - INFO - TOTAL: 31.813s
2025-07-31 02:53:18,466 - __main__ - INFO - =============================
2025-07-31 02:53:18,466 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 02:53:18,529 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 02:53:19,453 - __main__ - INFO - Starting continuous recording
2025-07-31 02:53:19,686 - __main__ - INFO - Recording started successfully
2025-07-31 02:53:19,687 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 02:53:19,687 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 02:53:19,687 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 02:53:23,060 - __main__ - INFO - Utterance collected: duration=0.58s, chunks=18
2025-07-31 02:53:23,078 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 02:53:23,078 - __main__ - INFO - Starting utterance processing
2025-07-31 02:53:23,078 - __main__ - INFO - Starting transcription: audio_length=9216, duration=0.58s
2025-07-31 02:53:23,125 - __main__ - INFO - ASR preprocessing completed in 0.046s
2025-07-31 02:53:24,600 - __main__ - INFO - ASR generation completed in 1.475s
2025-07-31 02:53:24,610 - __main__ - INFO - ASR decoding completed in 0.010s
2025-07-31 02:53:24,610 - __main__ - INFO - ASR transcription completed in 1.532s: 'hello'
2025-07-31 02:53:24,612 - __main__ - INFO - ASR processing completed in 1.534s
2025-07-31 02:53:24,612 - __main__ - INFO - User transcription: 'hello'
2025-07-31 02:53:24,612 - __main__ - INFO - User message added to queue
2025-07-31 02:53:24,612 - __main__ - INFO - Sending chat request via LiteLLM: model=gemma3:1b, messages_count=1
2025-07-31 02:53:24,697 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:1b; provider = ollama
2025-07-31 02:53:26,958 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:53:33,967 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 02:53:33,971 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 02:53:36,241 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:53:36,243 - __main__ - INFO - LiteLLM response completed in 11.631s: length=0
2025-07-31 02:53:36,244 - __main__ - INFO - LLM processing completed in 11.633s
2025-07-31 02:53:36,245 - __main__ - WARNING - Ollama returned empty response after 13.167s
2025-07-31 02:53:36,245 - __main__ - INFO - Utterance processing completed in 13.167s
2025-07-31 02:53:36,250 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:53:36,288 - __main__ - INFO - Utterance collected: duration=0.06s, chunks=2
2025-07-31 02:53:36,288 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 02:53:36,478 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 02:53:36,683 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:04:57,715 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 03:05:03,475 - __main__ - INFO - Loading all models
2025-07-31 03:05:06,281 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 03:05:12,545 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 03:05:12,545 - __main__ - INFO - ASR model loaded in 9.069s
2025-07-31 03:05:13,639 - __main__ - INFO - LLM model selected in 1.094s
2025-07-31 03:05:13,641 - __main__ - INFO - Initializing TTS Processor
2025-07-31 03:05:16,956 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 03:05:16,956 - __main__ - INFO - TTS model loaded in 3.315s
2025-07-31 03:05:16,956 - __main__ - INFO - Initializing VAD Processor
2025-07-31 03:05:17,524 - __main__ - INFO - VAD model loaded successfully
2025-07-31 03:05:17,524 - __main__ - INFO - VAD model loaded in 0.569s
2025-07-31 03:05:17,524 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 03:05:17,524 - __main__ - INFO - ASR: 9.069s
2025-07-31 03:05:17,524 - __main__ - INFO - LLM Selection: 1.094s
2025-07-31 03:05:17,524 - __main__ - INFO - TTS: 3.315s
2025-07-31 03:05:17,524 - __main__ - INFO - VAD: 0.569s
2025-07-31 03:05:17,524 - __main__ - INFO - TOTAL: 14.049s
2025-07-31 03:05:17,524 - __main__ - INFO - =============================
2025-07-31 03:05:17,524 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 03:05:17,745 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 03:05:20,318 - __main__ - INFO - Starting continuous recording
2025-07-31 03:05:20,649 - __main__ - INFO - Recording started successfully
2025-07-31 03:05:20,649 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 03:05:20,649 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 03:05:20,649 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 03:05:23,382 - __main__ - INFO - Utterance collected: duration=0.45s, chunks=14
2025-07-31 03:05:23,399 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:05:23,399 - __main__ - INFO - Starting utterance processing
2025-07-31 03:05:23,399 - __main__ - INFO - Starting transcription: audio_length=7168, duration=0.45s
2025-07-31 03:05:23,452 - __main__ - INFO - ASR preprocessing completed in 0.053s
2025-07-31 03:05:24,964 - __main__ - INFO - ASR generation completed in 1.512s
2025-07-31 03:05:24,970 - __main__ - INFO - ASR decoding completed in 0.000s
2025-07-31 03:05:24,970 - __main__ - INFO - ASR transcription completed in 1.571s: 'hello'
2025-07-31 03:05:24,972 - __main__ - INFO - ASR processing completed in 1.573s
2025-07-31 03:05:24,972 - __main__ - INFO - User transcription: 'hello'
2025-07-31 03:05:24,972 - __main__ - INFO - User message added to queue
2025-07-31 03:05:24,972 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 03:05:25,010 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:05:27,171 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:05:36,444 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:05:36,447 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:05:38,535 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:05:38,535 - __main__ - INFO - LLM processing completed in 13.563s
2025-07-31 03:05:38,535 - __main__ - INFO - Ollama response: 'Hello. How are you today?...'
2025-07-31 03:05:38,535 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:05:38,535 - __main__ - INFO - Starting TTS synthesis: text_length=25, voice=af_heart
2025-07-31 03:05:38,553 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:05:38,626 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:05:38,682 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:05:40,906 - __main__ - INFO - TTS generation completed in 2.370s
2025-07-31 03:05:40,906 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 03:05:40,906 - __main__ - INFO - TTS synthesis completed in 2.370s: audio_length=47400
2025-07-31 03:05:40,906 - __main__ - INFO - TTS processing completed in 2.370s
2025-07-31 03:05:40,906 - __main__ - INFO - Starting audio playback: duration=1.98s
2025-07-31 03:05:43,071 - __main__ - INFO - Audio playback completed in 2.165s
2025-07-31 03:05:43,071 - __main__ - INFO - Audio playback completed in 2.165s
2025-07-31 03:05:43,071 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:05:43,071 - __main__ - INFO - ASR: 1.573s
2025-07-31 03:05:43,071 - __main__ - INFO - LLM: 13.563s
2025-07-31 03:05:43,071 - __main__ - INFO - TTS: 2.370s
2025-07-31 03:05:43,071 - __main__ - INFO - Playback: 2.165s
2025-07-31 03:05:43,071 - __main__ - INFO - TOTAL: 19.672s
2025-07-31 03:05:43,071 - __main__ - INFO - =====================================
2025-07-31 03:05:43,071 - __main__ - INFO - Utterance processing completed in 19.672s
2025-07-31 03:05:43,108 - __main__ - INFO - Utterance collected: duration=0.13s, chunks=4
2025-07-31 03:05:43,108 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:05:48,823 - __main__ - INFO - Utterance collected: duration=0.32s, chunks=10
2025-07-31 03:05:48,833 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:05:48,833 - __main__ - INFO - Starting utterance processing
2025-07-31 03:05:48,833 - __main__ - INFO - Starting transcription: audio_length=5120, duration=0.32s
2025-07-31 03:05:48,849 - __main__ - INFO - ASR preprocessing completed in 0.015s
2025-07-31 03:05:49,386 - __main__ - INFO - ASR generation completed in 0.537s
2025-07-31 03:05:49,388 - __main__ - INFO - ASR decoding completed in 0.002s
2025-07-31 03:05:49,388 - __main__ - INFO - ASR transcription completed in 0.554s: 'thank you'
2025-07-31 03:05:49,390 - __main__ - INFO - ASR processing completed in 0.554s
2025-07-31 03:05:49,390 - __main__ - INFO - User transcription: 'thank you'
2025-07-31 03:05:49,390 - __main__ - INFO - User message added to queue
2025-07-31 03:05:49,390 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 03:05:49,390 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:05:51,457 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:05:53,980 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:05:53,980 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:05:54,040 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:05:54,040 - __main__ - INFO - LLM processing completed in 4.651s
2025-07-31 03:05:54,040 - __main__ - INFO - Ollama response: 'It seems like the conversation has just started. Is there something I can help you with, or would yo...'
2025-07-31 03:05:54,040 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:05:54,040 - __main__ - INFO - Starting TTS synthesis: text_length=131, voice=af_heart
2025-07-31 03:05:54,297 - __main__ - INFO - TTS generation completed in 0.256s
2025-07-31 03:05:54,297 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 03:05:54,297 - __main__ - INFO - TTS synthesis completed in 0.256s: audio_length=187200
2025-07-31 03:05:54,297 - __main__ - INFO - TTS processing completed in 0.256s
2025-07-31 03:05:54,297 - __main__ - INFO - Starting audio playback: duration=7.80s
2025-07-31 03:05:56,063 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:05:56,123 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:05:56,180 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:06:02,211 - __main__ - INFO - Audio playback completed in 7.915s
2025-07-31 03:06:02,211 - __main__ - INFO - Audio playback completed in 7.915s
2025-07-31 03:06:02,211 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:06:02,211 - __main__ - INFO - ASR: 0.554s
2025-07-31 03:06:02,211 - __main__ - INFO - LLM: 4.651s
2025-07-31 03:06:02,211 - __main__ - INFO - TTS: 0.256s
2025-07-31 03:06:02,211 - __main__ - INFO - Playback: 7.915s
2025-07-31 03:06:02,211 - __main__ - INFO - TOTAL: 13.378s
2025-07-31 03:06:02,211 - __main__ - INFO - =====================================
2025-07-31 03:06:02,211 - __main__ - INFO - Utterance processing completed in 13.378s
2025-07-31 03:06:02,247 - __main__ - INFO - Utterance collected: duration=0.10s, chunks=3
2025-07-31 03:06:02,247 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:06:10,394 - __main__ - INFO - Utterance collected: duration=3.74s, chunks=117
2025-07-31 03:06:10,449 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:06:10,449 - __main__ - INFO - Starting utterance processing
2025-07-31 03:06:10,449 - __main__ - INFO - Starting transcription: audio_length=59904, duration=3.74s
2025-07-31 03:06:10,466 - __main__ - INFO - ASR preprocessing completed in 0.017s
2025-07-31 03:06:11,037 - __main__ - INFO - ASR generation completed in 0.572s
2025-07-31 03:06:11,037 - __main__ - INFO - ASR decoding completed in 0.000s
2025-07-31 03:06:11,037 - __main__ - INFO - ASR transcription completed in 0.588s: 'can you tell me who albert einstein was'
2025-07-31 03:06:11,037 - __main__ - INFO - ASR processing completed in 0.588s
2025-07-31 03:06:11,037 - __main__ - INFO - User transcription: 'can you tell me who albert einstein was'
2025-07-31 03:06:11,037 - __main__ - INFO - User message added to queue
2025-07-31 03:06:11,037 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 03:06:11,044 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:06:13,117 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:06:20,378 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:06:20,378 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:06:22,466 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:06:22,466 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:06:22,466 - __main__ - INFO - LLM processing completed in 11.428s
2025-07-31 03:06:22,466 - __main__ - INFO - Ollama response: 'Albert Einstein (1879-1955) was a renowned German-born physicist who is widely regarded as one of th...'
2025-07-31 03:06:22,466 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:06:22,466 - __main__ - INFO - Starting TTS synthesis: text_length=2379, voice=af_heart
2025-07-31 03:06:22,548 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:06:22,644 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:06:24,302 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-07-31 03:06:28,430 - __main__ - INFO - TTS generation completed in 5.964s
2025-07-31 03:06:28,435 - __main__ - INFO - TTS audio concatenation completed in 0.006s
2025-07-31 03:06:28,435 - __main__ - INFO - TTS synthesis completed in 5.969s: audio_length=3789600
2025-07-31 03:06:28,436 - __main__ - INFO - TTS processing completed in 5.970s
2025-07-31 03:06:28,436 - __main__ - INFO - Starting audio playback: duration=157.90s
2025-07-31 03:09:06,505 - __main__ - INFO - Audio playback completed in 158.069s
2025-07-31 03:09:06,505 - __main__ - INFO - Audio playback completed in 158.069s
2025-07-31 03:09:06,509 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:09:06,509 - __main__ - INFO - ASR: 0.588s
2025-07-31 03:09:06,509 - __main__ - INFO - LLM: 11.428s
2025-07-31 03:09:06,509 - __main__ - INFO - TTS: 5.970s
2025-07-31 03:09:06,509 - __main__ - INFO - Playback: 158.069s
2025-07-31 03:09:06,509 - __main__ - INFO - TOTAL: 176.060s
2025-07-31 03:09:06,509 - __main__ - INFO - =====================================
2025-07-31 03:09:06,509 - __main__ - INFO - Utterance processing completed in 176.060s
2025-07-31 03:09:06,544 - __main__ - INFO - Utterance collected: duration=0.19s, chunks=6
2025-07-31 03:09:06,549 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:09:18,779 - __main__ - INFO - Utterance collected: duration=3.04s, chunks=95
2025-07-31 03:09:18,845 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:09:18,845 - __main__ - INFO - Starting utterance processing
2025-07-31 03:09:18,845 - __main__ - INFO - Starting transcription: audio_length=48640, duration=3.04s
2025-07-31 03:09:18,866 - __main__ - INFO - ASR preprocessing completed in 0.020s
2025-07-31 03:09:19,386 - __main__ - INFO - ASR generation completed in 0.520s
2025-07-31 03:09:19,391 - __main__ - INFO - ASR decoding completed in 0.005s
2025-07-31 03:09:19,391 - __main__ - INFO - ASR transcription completed in 0.545s: 'okay what was the biggest achievement'
2025-07-31 03:09:19,394 - __main__ - INFO - ASR processing completed in 0.549s
2025-07-31 03:09:19,394 - __main__ - INFO - User transcription: 'okay what was the biggest achievement'
2025-07-31 03:09:19,394 - __main__ - INFO - User message added to queue
2025-07-31 03:09:19,394 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=1
2025-07-31 03:09:19,401 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:09:21,525 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:09:25,231 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:09:25,232 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:09:25,294 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:09:25,296 - __main__ - INFO - LLM processing completed in 5.902s
2025-07-31 03:09:25,296 - __main__ - INFO - Ollama response: 'I'm happy to chat with you. However, I want to clarify that this is the beginning of our conversatio...'
2025-07-31 03:09:25,296 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:09:25,296 - __main__ - INFO - Starting TTS synthesis: text_length=278, voice=af_heart
2025-07-31 03:09:26,339 - __main__ - INFO - TTS generation completed in 1.043s
2025-07-31 03:09:26,339 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 03:09:26,339 - __main__ - INFO - TTS synthesis completed in 1.043s: audio_length=376200
2025-07-31 03:09:26,340 - __main__ - INFO - TTS processing completed in 1.044s
2025-07-31 03:09:26,340 - __main__ - INFO - Starting audio playback: duration=15.68s
2025-07-31 03:09:27,351 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:09:27,416 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:09:27,476 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:09:42,154 - __main__ - INFO - Audio playback completed in 15.814s
2025-07-31 03:09:42,154 - __main__ - INFO - Audio playback completed in 15.814s
2025-07-31 03:09:42,154 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:09:42,154 - __main__ - INFO - ASR: 0.549s
2025-07-31 03:09:42,154 - __main__ - INFO - LLM: 5.902s
2025-07-31 03:09:42,154 - __main__ - INFO - TTS: 1.044s
2025-07-31 03:09:42,154 - __main__ - INFO - Playback: 15.814s
2025-07-31 03:09:42,154 - __main__ - INFO - TOTAL: 23.309s
2025-07-31 03:09:42,154 - __main__ - INFO - =====================================
2025-07-31 03:09:42,154 - __main__ - INFO - Utterance processing completed in 23.309s
2025-07-31 03:09:42,198 - __main__ - INFO - Utterance collected: duration=0.16s, chunks=5
2025-07-31 03:09:42,198 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:26:13,362 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 03:26:18,335 - __main__ - INFO - Loading all models
2025-07-31 03:26:27,018 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 03:26:33,966 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 03:26:33,966 - __main__ - INFO - ASR model loaded in 15.631s
2025-07-31 03:26:36,006 - __main__ - INFO - LLM model selected in 2.038s
2025-07-31 03:26:36,006 - __main__ - INFO - Initializing TTS Processor
2025-07-31 03:26:39,353 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 03:26:39,353 - __main__ - INFO - TTS model loaded in 3.347s
2025-07-31 03:26:39,353 - __main__ - INFO - Initializing VAD Processor
2025-07-31 03:26:39,914 - __main__ - INFO - VAD model loaded successfully
2025-07-31 03:26:39,914 - __main__ - INFO - VAD model loaded in 0.561s
2025-07-31 03:26:39,914 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 03:26:39,914 - __main__ - INFO - ASR: 15.631s
2025-07-31 03:26:39,914 - __main__ - INFO - LLM Selection: 2.038s
2025-07-31 03:26:39,914 - __main__ - INFO - TTS: 3.347s
2025-07-31 03:26:39,914 - __main__ - INFO - VAD: 0.561s
2025-07-31 03:26:39,914 - __main__ - INFO - TOTAL: 21.578s
2025-07-31 03:26:39,914 - __main__ - INFO - =============================
2025-07-31 03:26:39,914 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 03:26:39,986 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 03:26:41,036 - __main__ - INFO - Starting continuous recording
2025-07-31 03:26:41,272 - __main__ - INFO - Recording started successfully
2025-07-31 03:26:41,272 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 03:26:41,272 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 03:26:41,272 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 03:26:43,555 - __main__ - INFO - Utterance collected: duration=0.45s, chunks=14
2025-07-31 03:26:43,571 - __main__ - INFO - VAD rejected but audio level is significant (0.0035), processing anyway
2025-07-31 03:26:43,571 - __main__ - INFO - Starting utterance processing
2025-07-31 03:26:43,571 - __main__ - INFO - Starting transcription: audio_length=7168, duration=0.45s
2025-07-31 03:26:43,612 - __main__ - INFO - ASR preprocessing completed in 0.040s
2025-07-31 03:26:45,007 - __main__ - INFO - ASR generation completed in 1.396s
2025-07-31 03:26:45,016 - __main__ - INFO - ASR decoding completed in 0.008s
2025-07-31 03:26:45,016 - __main__ - INFO - ASR transcription completed in 1.444s: 'thank you'
2025-07-31 03:26:45,018 - __main__ - INFO - ASR processing completed in 1.446s
2025-07-31 03:26:45,018 - __main__ - INFO - User transcription: 'thank you'
2025-07-31 03:26:45,018 - __main__ - INFO - User message added to queue
2025-07-31 03:26:45,018 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=2
2025-07-31 03:26:45,063 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:26:47,154 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:26:54,408 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:26:54,418 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:26:56,521 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:26:56,522 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:26:56,522 - __main__ - INFO - LLM processing completed in 11.504s
2025-07-31 03:26:56,522 - __main__ - INFO - Ollama response: 'It's nice to hear that you're available for an interview.

Can you tell me what inspired you to take...'
2025-07-31 03:26:56,527 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:26:56,527 - __main__ - INFO - Starting TTS synthesis: text_length=268, voice=af_heart
2025-07-31 03:26:56,594 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:26:56,654 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:26:59,334 - __main__ - INFO - TTS generation completed in 2.807s
2025-07-31 03:26:59,336 - __main__ - INFO - TTS audio concatenation completed in 0.002s
2025-07-31 03:26:59,336 - __main__ - INFO - TTS synthesis completed in 2.809s: audio_length=362400
2025-07-31 03:26:59,336 - __main__ - INFO - TTS processing completed in 2.809s
2025-07-31 03:26:59,336 - __main__ - INFO - Starting audio playback: duration=15.10s
2025-07-31 03:27:14,646 - __main__ - INFO - Audio playback completed in 15.311s
2025-07-31 03:27:14,646 - __main__ - INFO - Audio playback completed in 15.311s
2025-07-31 03:27:14,648 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:27:14,648 - __main__ - INFO - ASR: 1.446s
2025-07-31 03:27:14,648 - __main__ - INFO - LLM: 11.504s
2025-07-31 03:27:14,648 - __main__ - INFO - TTS: 2.809s
2025-07-31 03:27:14,648 - __main__ - INFO - Playback: 15.311s
2025-07-31 03:27:14,648 - __main__ - INFO - TOTAL: 31.076s
2025-07-31 03:27:14,648 - __main__ - INFO - =====================================
2025-07-31 03:27:14,648 - __main__ - INFO - Utterance processing completed in 31.076s
2025-07-31 03:27:18,916 - __main__ - INFO - Utterance collected: duration=0.29s, chunks=9
2025-07-31 03:27:18,916 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:27:20,068 - __main__ - INFO - Utterance collected: duration=0.03s, chunks=1
2025-07-31 03:27:20,068 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:27:23,556 - __main__ - INFO - Utterance collected: duration=1.28s, chunks=40
2025-07-31 03:27:23,582 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:27:23,582 - __main__ - INFO - Starting utterance processing
2025-07-31 03:27:23,582 - __main__ - INFO - Starting transcription: audio_length=20480, duration=1.28s
2025-07-31 03:27:23,597 - __main__ - INFO - ASR preprocessing completed in 0.015s
2025-07-31 03:27:24,200 - __main__ - INFO - ASR generation completed in 0.602s
2025-07-31 03:27:24,203 - __main__ - INFO - ASR decoding completed in 0.001s
2025-07-31 03:27:24,203 - __main__ - INFO - ASR transcription completed in 0.620s: 'the alienation'
2025-07-31 03:27:24,203 - __main__ - INFO - ASR processing completed in 0.620s
2025-07-31 03:27:24,203 - __main__ - INFO - User transcription: 'the alienation'
2025-07-31 03:27:24,205 - __main__ - INFO - User message added to queue
2025-07-31 03:27:24,205 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=2
2025-07-31 03:27:24,205 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:27:26,280 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:27:29,837 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:27:29,837 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:27:29,894 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:27:29,897 - __main__ - INFO - LLM processing completed in 5.692s
2025-07-31 03:27:29,897 - __main__ - INFO - Ollama response: 'It seems there might be some confusion in the prompt. However, assuming the context is about underst...'
2025-07-31 03:27:29,897 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:27:29,897 - __main__ - INFO - Starting TTS synthesis: text_length=688, voice=af_heart
2025-07-31 03:27:31,349 - __main__ - INFO - TTS generation completed in 1.452s
2025-07-31 03:27:31,349 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 03:27:31,349 - __main__ - INFO - TTS synthesis completed in 1.452s: audio_length=1051800
2025-07-31 03:27:31,349 - __main__ - INFO - TTS processing completed in 1.452s
2025-07-31 03:27:31,349 - __main__ - INFO - Starting audio playback: duration=43.83s
2025-07-31 03:27:31,912 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:27:31,971 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:27:32,025 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:28:15,288 - __main__ - INFO - Audio playback completed in 43.939s
2025-07-31 03:28:15,288 - __main__ - INFO - Audio playback completed in 43.939s
2025-07-31 03:28:15,288 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:28:15,288 - __main__ - INFO - ASR: 0.620s
2025-07-31 03:28:15,288 - __main__ - INFO - LLM: 5.692s
2025-07-31 03:28:15,288 - __main__ - INFO - TTS: 1.452s
2025-07-31 03:28:15,288 - __main__ - INFO - Playback: 43.939s
2025-07-31 03:28:15,288 - __main__ - INFO - TOTAL: 51.705s
2025-07-31 03:28:15,288 - __main__ - INFO - =====================================
2025-07-31 03:28:15,288 - __main__ - INFO - Utterance processing completed in 51.705s
2025-07-31 03:28:26,039 - __main__ - INFO - Utterance collected: duration=2.46s, chunks=77
2025-07-31 03:28:26,085 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:28:26,086 - __main__ - INFO - Starting utterance processing
2025-07-31 03:28:26,086 - __main__ - INFO - Starting transcription: audio_length=39424, duration=2.46s
2025-07-31 03:28:26,099 - __main__ - INFO - ASR preprocessing completed in 0.013s
2025-07-31 03:28:26,520 - __main__ - INFO - ASR generation completed in 0.421s
2025-07-31 03:28:26,521 - __main__ - INFO - ASR decoding completed in 0.001s
2025-07-31 03:28:26,521 - __main__ - INFO - ASR transcription completed in 0.435s: 'a'
2025-07-31 03:28:26,522 - __main__ - INFO - ASR processing completed in 0.437s
2025-07-31 03:28:26,522 - __main__ - INFO - User transcription: 'a'
2025-07-31 03:28:26,522 - __main__ - INFO - User message added to queue
2025-07-31 03:28:26,522 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=2
2025-07-31 03:28:26,523 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:28:28,609 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:28:31,678 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:28:31,679 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:28:31,736 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:28:31,737 - __main__ - INFO - LLM processing completed in 5.215s
2025-07-31 03:28:31,737 - __main__ - INFO - Ollama response: 'It seems like you're starting from scratch or have limited experience with system-related tasks. Cou...'
2025-07-31 03:28:31,737 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:28:31,737 - __main__ - INFO - Starting TTS synthesis: text_length=402, voice=af_heart
2025-07-31 03:28:32,790 - __main__ - INFO - TTS generation completed in 1.053s
2025-07-31 03:28:32,792 - __main__ - INFO - TTS audio concatenation completed in 0.002s
2025-07-31 03:28:32,792 - __main__ - INFO - TTS synthesis completed in 1.055s: audio_length=545400
2025-07-31 03:28:32,792 - __main__ - INFO - TTS processing completed in 1.055s
2025-07-31 03:28:32,792 - __main__ - INFO - Starting audio playback: duration=22.73s
2025-07-31 03:28:33,768 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:28:33,824 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:28:33,881 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:28:38,480 - __main__ - INFO - Audio playback completed in 5.688s
2025-07-31 03:28:38,480 - __main__ - INFO - Audio playback completed in 5.688s
2025-07-31 03:28:38,480 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:28:38,488 - __main__ - INFO - ASR: 0.437s
2025-07-31 03:28:38,488 - __main__ - INFO - LLM: 5.215s
2025-07-31 03:28:38,488 - __main__ - INFO - TTS: 1.055s
2025-07-31 03:28:38,488 - __main__ - INFO - Playback: 5.688s
2025-07-31 03:28:38,488 - __main__ - INFO - TOTAL: 12.394s
2025-07-31 03:28:38,488 - __main__ - INFO - =====================================
2025-07-31 03:28:38,488 - __main__ - INFO - Utterance processing completed in 12.402s
2025-07-31 03:31:59,939 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 03:32:04,653 - __main__ - INFO - Loading all models
2025-07-31 03:32:08,409 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 03:32:14,818 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 03:32:14,818 - __main__ - INFO - ASR model loaded in 10.166s
2025-07-31 03:32:17,281 - __main__ - INFO - LLM model selected in 2.462s
2025-07-31 03:32:17,281 - __main__ - INFO - Initializing TTS Processor
2025-07-31 03:32:20,968 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 03:32:20,968 - __main__ - INFO - TTS model loaded in 3.688s
2025-07-31 03:32:20,968 - __main__ - INFO - Initializing VAD Processor
2025-07-31 03:32:21,558 - __main__ - INFO - VAD model loaded successfully
2025-07-31 03:32:21,558 - __main__ - INFO - VAD model loaded in 0.589s
2025-07-31 03:32:21,558 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 03:32:21,558 - __main__ - INFO - ASR: 10.166s
2025-07-31 03:32:21,558 - __main__ - INFO - LLM Selection: 2.462s
2025-07-31 03:32:21,558 - __main__ - INFO - TTS: 3.688s
2025-07-31 03:32:21,558 - __main__ - INFO - VAD: 0.589s
2025-07-31 03:32:21,558 - __main__ - INFO - TOTAL: 16.905s
2025-07-31 03:32:21,558 - __main__ - INFO - =============================
2025-07-31 03:32:21,558 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 03:32:21,843 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 03:32:23,574 - __main__ - INFO - Starting continuous recording
2025-07-31 03:32:23,910 - __main__ - INFO - Recording started successfully
2025-07-31 03:32:23,912 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 03:32:23,912 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 03:32:23,912 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 03:32:27,154 - __main__ - INFO - Utterance collected: duration=0.74s, chunks=23
2025-07-31 03:32:27,174 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:32:27,174 - __main__ - INFO - Starting utterance processing
2025-07-31 03:32:27,174 - __main__ - INFO - Starting transcription: audio_length=11776, duration=0.74s
2025-07-31 03:32:27,237 - __main__ - INFO - ASR preprocessing completed in 0.063s
2025-07-31 03:32:28,743 - __main__ - INFO - ASR generation completed in 1.506s
2025-07-31 03:32:28,749 - __main__ - INFO - ASR decoding completed in 0.006s
2025-07-31 03:32:28,749 - __main__ - INFO - ASR transcription completed in 1.574s: 'hello'
2025-07-31 03:32:28,750 - __main__ - INFO - ASR processing completed in 1.576s
2025-07-31 03:32:28,750 - __main__ - INFO - User transcription: 'hello'
2025-07-31 03:32:28,750 - __main__ - INFO - User message added to queue
2025-07-31 03:32:28,750 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=2
2025-07-31 03:32:28,794 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:32:30,935 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:32:33,969 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:32:33,973 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:32:34,034 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:32:34,089 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:32:34,143 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:32:34,196 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:32:36,072 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:32:36,072 - __main__ - INFO - LLM processing completed in 7.322s
2025-07-31 03:32:36,072 - __main__ - INFO - Ollama response: 'Hello! How can I assist you today?...'
2025-07-31 03:32:36,072 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:32:36,072 - __main__ - INFO - Starting TTS synthesis: text_length=34, voice=af_heart
2025-07-31 03:32:37,767 - __main__ - INFO - TTS generation completed in 1.695s
2025-07-31 03:32:37,767 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 03:32:37,767 - __main__ - INFO - TTS synthesis completed in 1.695s: audio_length=59400
2025-07-31 03:32:37,767 - __main__ - INFO - TTS processing completed in 1.695s
2025-07-31 03:32:37,767 - __main__ - INFO - Starting audio playback: duration=2.48s
2025-07-31 03:32:40,402 - __main__ - INFO - Audio playback completed in 2.635s
2025-07-31 03:32:40,402 - __main__ - INFO - Audio playback completed in 2.635s
2025-07-31 03:32:40,402 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:32:40,402 - __main__ - INFO - ASR: 1.576s
2025-07-31 03:32:40,402 - __main__ - INFO - LLM: 7.322s
2025-07-31 03:32:40,402 - __main__ - INFO - TTS: 1.695s
2025-07-31 03:32:40,402 - __main__ - INFO - Playback: 2.635s
2025-07-31 03:32:40,402 - __main__ - INFO - TOTAL: 13.228s
2025-07-31 03:32:40,402 - __main__ - INFO - =====================================
2025-07-31 03:32:40,402 - __main__ - INFO - Utterance processing completed in 13.228s
2025-07-31 03:32:40,439 - __main__ - INFO - Utterance collected: duration=0.06s, chunks=2
2025-07-31 03:32:40,440 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:32:51,396 - __main__ - INFO - Utterance collected: duration=4.16s, chunks=130
2025-07-31 03:32:51,468 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:32:51,468 - __main__ - INFO - Starting utterance processing
2025-07-31 03:32:51,468 - __main__ - INFO - Starting transcription: audio_length=66560, duration=4.16s
2025-07-31 03:32:51,484 - __main__ - INFO - ASR preprocessing completed in 0.015s
2025-07-31 03:32:52,237 - __main__ - INFO - ASR generation completed in 0.754s
2025-07-31 03:32:52,243 - __main__ - INFO - ASR decoding completed in 0.004s
2025-07-31 03:32:52,243 - __main__ - INFO - ASR transcription completed in 0.775s: 'i am working on a machine learning project'
2025-07-31 03:32:52,243 - __main__ - INFO - ASR processing completed in 0.775s
2025-07-31 03:32:52,244 - __main__ - INFO - User transcription: 'i am working on a machine learning project'
2025-07-31 03:32:52,244 - __main__ - INFO - User message added to queue
2025-07-31 03:32:52,244 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=2
2025-07-31 03:32:52,244 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:32:54,348 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:32:57,283 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:32:57,284 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:32:57,344 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:32:57,346 - __main__ - INFO - LLM processing completed in 5.102s
2025-07-31 03:32:57,346 - __main__ - INFO - Ollama response: 'It sounds like you're getting started with your machine learning project. That's a great first step!...'
2025-07-31 03:32:57,346 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:32:57,346 - __main__ - INFO - Starting TTS synthesis: text_length=349, voice=af_heart
2025-07-31 03:32:57,927 - __main__ - INFO - TTS generation completed in 0.580s
2025-07-31 03:32:57,929 - __main__ - INFO - TTS audio concatenation completed in 0.002s
2025-07-31 03:32:57,929 - __main__ - INFO - TTS synthesis completed in 0.583s: audio_length=482400
2025-07-31 03:32:57,929 - __main__ - INFO - TTS processing completed in 0.583s
2025-07-31 03:32:57,929 - __main__ - INFO - Starting audio playback: duration=20.10s
2025-07-31 03:32:59,374 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:32:59,438 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:32:59,503 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:33:18,174 - __main__ - INFO - Audio playback completed in 20.245s
2025-07-31 03:33:18,174 - __main__ - INFO - Audio playback completed in 20.245s
2025-07-31 03:33:18,174 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:33:18,174 - __main__ - INFO - ASR: 0.775s
2025-07-31 03:33:18,174 - __main__ - INFO - LLM: 5.102s
2025-07-31 03:33:18,174 - __main__ - INFO - TTS: 0.583s
2025-07-31 03:33:18,174 - __main__ - INFO - Playback: 20.245s
2025-07-31 03:33:18,174 - __main__ - INFO - TOTAL: 26.706s
2025-07-31 03:33:18,174 - __main__ - INFO - =====================================
2025-07-31 03:33:18,174 - __main__ - INFO - Utterance processing completed in 26.706s
2025-07-31 03:33:18,216 - __main__ - INFO - Utterance collected: duration=0.06s, chunks=2
2025-07-31 03:33:18,216 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:33:25,956 - __main__ - INFO - Utterance collected: duration=3.52s, chunks=110
2025-07-31 03:33:26,036 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:33:26,036 - __main__ - INFO - Starting utterance processing
2025-07-31 03:33:26,036 - __main__ - INFO - Starting transcription: audio_length=56320, duration=3.52s
2025-07-31 03:33:26,054 - __main__ - INFO - ASR preprocessing completed in 0.018s
2025-07-31 03:33:26,562 - __main__ - INFO - ASR generation completed in 0.508s
2025-07-31 03:33:26,565 - __main__ - INFO - ASR decoding completed in 0.004s
2025-07-31 03:33:26,565 - __main__ - INFO - ASR transcription completed in 0.529s: 'i am working on a classification model'
2025-07-31 03:33:26,567 - __main__ - INFO - ASR processing completed in 0.531s
2025-07-31 03:33:26,567 - __main__ - INFO - User transcription: 'i am working on a classification model'
2025-07-31 03:33:26,567 - __main__ - INFO - User message added to queue
2025-07-31 03:33:26,567 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=2
2025-07-31 03:33:26,568 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:33:28,665 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:33:31,704 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:33:31,705 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:33:31,768 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:33:31,770 - __main__ - INFO - LLM processing completed in 5.203s
2025-07-31 03:33:31,770 - __main__ - INFO - Ollama response: '### Serin: Hi there! It sounds like you're working on a classification model. Can you tell me more a...'
2025-07-31 03:33:31,770 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:33:31,770 - __main__ - INFO - Starting TTS synthesis: text_length=356, voice=af_heart
2025-07-31 03:33:32,377 - __main__ - INFO - TTS generation completed in 0.607s
2025-07-31 03:33:32,377 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 03:33:32,377 - __main__ - INFO - TTS synthesis completed in 0.607s: audio_length=528000
2025-07-31 03:33:32,379 - __main__ - INFO - TTS processing completed in 0.609s
2025-07-31 03:33:32,379 - __main__ - INFO - Starting audio playback: duration=22.00s
2025-07-31 03:33:33,800 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:33:33,853 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:33:33,907 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:33:54,522 - __main__ - INFO - Audio playback completed in 22.143s
2025-07-31 03:33:54,522 - __main__ - INFO - Audio playback completed in 22.143s
2025-07-31 03:33:54,522 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:33:54,522 - __main__ - INFO - ASR: 0.531s
2025-07-31 03:33:54,522 - __main__ - INFO - LLM: 5.203s
2025-07-31 03:33:54,522 - __main__ - INFO - TTS: 0.609s
2025-07-31 03:33:54,522 - __main__ - INFO - Playback: 22.143s
2025-07-31 03:33:54,522 - __main__ - INFO - TOTAL: 28.485s
2025-07-31 03:33:54,522 - __main__ - INFO - =====================================
2025-07-31 03:33:54,522 - __main__ - INFO - Utterance processing completed in 28.485s
2025-07-31 03:33:54,563 - __main__ - INFO - Utterance collected: duration=0.13s, chunks=4
2025-07-31 03:33:54,563 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:34:03,910 - __main__ - INFO - Utterance collected: duration=4.96s, chunks=155
2025-07-31 03:34:03,996 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:34:03,996 - __main__ - INFO - Starting utterance processing
2025-07-31 03:34:03,996 - __main__ - INFO - Starting transcription: audio_length=79360, duration=4.96s
2025-07-31 03:34:04,010 - __main__ - INFO - ASR preprocessing completed in 0.014s
2025-07-31 03:34:04,561 - __main__ - INFO - ASR generation completed in 0.551s
2025-07-31 03:34:04,566 - __main__ - INFO - ASR decoding completed in 0.005s
2025-07-31 03:34:04,567 - __main__ - INFO - ASR transcription completed in 0.571s: 'it is to identify cancer cells in an image'
2025-07-31 03:34:04,567 - __main__ - INFO - ASR processing completed in 0.571s
2025-07-31 03:34:04,568 - __main__ - INFO - User transcription: 'it is to identify cancer cells in an image'
2025-07-31 03:34:04,568 - __main__ - INFO - User message added to queue
2025-07-31 03:34:04,568 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=2
2025-07-31 03:34:04,568 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:34:06,662 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:34:09,693 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:34:09,694 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:34:09,752 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:34:09,754 - __main__ - INFO - LLM processing completed in 5.186s
2025-07-31 03:34:09,754 - __main__ - INFO - Ollama response: 'It sounds like you're working on a project related to computer vision and machine learning. To get s...'
2025-07-31 03:34:09,754 - __main__ - INFO - Assistant message added to queue
2025-07-31 03:34:09,754 - __main__ - INFO - Starting TTS synthesis: text_length=357, voice=af_heart
2025-07-31 03:34:10,333 - __main__ - INFO - TTS generation completed in 0.579s
2025-07-31 03:34:10,333 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 03:34:10,334 - __main__ - INFO - TTS synthesis completed in 0.580s: audio_length=531600
2025-07-31 03:34:10,334 - __main__ - INFO - TTS processing completed in 0.580s
2025-07-31 03:34:10,334 - __main__ - INFO - Starting audio playback: duration=22.15s
2025-07-31 03:34:11,790 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:34:11,843 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:34:11,898 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:34:32,608 - __main__ - INFO - Audio playback completed in 22.274s
2025-07-31 03:34:32,608 - __main__ - INFO - Audio playback completed in 22.274s
2025-07-31 03:34:32,608 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:34:32,608 - __main__ - INFO - ASR: 0.571s
2025-07-31 03:34:32,609 - __main__ - INFO - LLM: 5.186s
2025-07-31 03:34:32,609 - __main__ - INFO - TTS: 0.580s
2025-07-31 03:34:32,609 - __main__ - INFO - Playback: 22.274s
2025-07-31 03:34:32,609 - __main__ - INFO - TOTAL: 28.612s
2025-07-31 03:34:32,609 - __main__ - INFO - =====================================
2025-07-31 03:34:32,609 - __main__ - INFO - Utterance processing completed in 28.614s
2025-07-31 03:34:32,651 - __main__ - INFO - Utterance collected: duration=0.10s, chunks=3
2025-07-31 03:34:32,651 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:34:34,813 - __main__ - INFO - Stopping recording
2025-07-31 03:34:34,866 - __main__ - INFO - Audio stream closed successfully
2025-07-31 03:34:34,866 - __main__ - INFO - Sentinel value added to queue
2025-07-31 03:34:34,866 - __main__ - INFO - Received sentinel value, stopping processing
2025-07-31 03:34:34,866 - __main__ - INFO - Recording stopped. Total chunks recorded: 4085
2025-07-31 03:34:34,866 - __main__ - INFO - === CONTINUOUS PROCESSING SUMMARY ===
2025-07-31 03:34:34,868 - __main__ - INFO - Total processing time: 130.954s
2025-07-31 03:34:34,868 - __main__ - INFO - Chunks processed: 1252
2025-07-31 03:34:34,868 - __main__ - INFO - Utterances processed: 4
2025-07-31 03:34:34,868 - __main__ - INFO - Average time per utterance: 32.739s
2025-07-31 03:34:34,868 - __main__ - INFO - =====================================
2025-07-31 03:40:39,340 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 03:45:21,975 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 03:45:26,888 - __main__ - INFO - Loading all models
2025-07-31 03:45:29,189 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 03:45:35,495 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 03:45:35,495 - __main__ - INFO - ASR model loaded in 8.607s
2025-07-31 03:45:37,438 - __main__ - INFO - LLM model selected in 1.943s
2025-07-31 03:45:37,439 - __main__ - INFO - Initializing TTS Processor
2025-07-31 03:45:41,327 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 03:45:41,327 - __main__ - INFO - TTS model loaded in 3.889s
2025-07-31 03:45:41,327 - __main__ - INFO - Initializing VAD Processor
2025-07-31 03:45:41,912 - __main__ - INFO - VAD model loaded successfully
2025-07-31 03:45:41,914 - __main__ - INFO - VAD model loaded in 0.587s
2025-07-31 03:45:41,914 - __main__ - INFO - === MODEL LOADING SUMMARY ===
2025-07-31 03:45:41,914 - __main__ - INFO - ASR: 8.607s
2025-07-31 03:45:41,914 - __main__ - INFO - LLM Selection: 1.943s
2025-07-31 03:45:41,914 - __main__ - INFO - TTS: 3.889s
2025-07-31 03:45:41,914 - __main__ - INFO - VAD: 0.587s
2025-07-31 03:45:41,914 - __main__ - INFO - TOTAL: 15.026s
2025-07-31 03:45:41,914 - __main__ - INFO - =============================
2025-07-31 03:45:41,914 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 03:45:42,145 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 03:45:44,147 - __main__ - INFO - Starting continuous recording
2025-07-31 03:45:44,460 - __main__ - INFO - Recording started successfully
2025-07-31 03:45:44,462 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 03:45:44,462 - __main__ - INFO - Processing parameters: chunks_per_second=31.25, silent_chunks_threshold=31
2025-07-31 03:45:44,462 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 03:45:48,428 - __main__ - INFO - Utterance collected: duration=0.93s, chunks=29
2025-07-31 03:45:48,447 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:45:48,448 - __main__ - INFO - Starting utterance processing
2025-07-31 03:45:48,448 - __main__ - INFO - Starting transcription: audio_length=14848, duration=0.93s
2025-07-31 03:45:48,492 - __main__ - INFO - ASR preprocessing completed in 0.044s
2025-07-31 03:45:49,839 - __main__ - INFO - ASR generation completed in 1.346s
2025-07-31 03:45:49,848 - __main__ - INFO - ASR decoding completed in 0.009s
2025-07-31 03:45:49,848 - __main__ - INFO - ASR transcription completed in 1.400s: 'handle'
2025-07-31 03:45:49,849 - __main__ - INFO - ASR processing completed in 1.401s
2025-07-31 03:45:49,849 - __main__ - INFO - User transcription: 'handle'
2025-07-31 03:45:49,849 - __main__ - INFO - User message added to queue and history
2025-07-31 03:45:49,849 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=2
2025-07-31 03:45:49,902 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:45:52,147 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:45:59,251 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:45:59,257 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:46:01,411 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:46:01,423 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:46:01,425 - __main__ - INFO - LLM processing completed in 11.576s
2025-07-31 03:46:01,425 - __main__ - INFO - Ollama response: 'It seems like we're starting our conversation about a handle. Can you please tell me more about it? ...'
2025-07-31 03:46:01,425 - __main__ - INFO - Assistant message added to queue and history
2025-07-31 03:46:01,425 - __main__ - INFO - Starting TTS synthesis: text_length=181, voice=af_heart
2025-07-31 03:46:01,522 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:46:01,613 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:46:03,772 - __main__ - INFO - TTS generation completed in 2.347s
2025-07-31 03:46:03,772 - __main__ - INFO - TTS audio concatenation completed in 0.000s
2025-07-31 03:46:03,772 - __main__ - INFO - TTS synthesis completed in 2.347s: audio_length=255000
2025-07-31 03:46:03,772 - __main__ - INFO - TTS processing completed in 2.347s
2025-07-31 03:46:03,772 - __main__ - INFO - Starting audio playback: duration=10.62s
2025-07-31 03:46:14,571 - __main__ - INFO - Audio playback completed in 10.799s
2025-07-31 03:46:14,571 - __main__ - INFO - Audio playback completed in 10.799s
2025-07-31 03:46:14,571 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:46:14,571 - __main__ - INFO - ASR: 1.401s
2025-07-31 03:46:14,571 - __main__ - INFO - LLM: 11.576s
2025-07-31 03:46:14,571 - __main__ - INFO - TTS: 2.347s
2025-07-31 03:46:14,571 - __main__ - INFO - Playback: 10.799s
2025-07-31 03:46:14,571 - __main__ - INFO - TOTAL: 26.123s
2025-07-31 03:46:14,571 - __main__ - INFO - =====================================
2025-07-31 03:46:14,571 - __main__ - INFO - Utterance processing completed in 26.123s
2025-07-31 03:46:14,598 - __main__ - INFO - Utterance collected: duration=0.03s, chunks=1
2025-07-31 03:46:14,598 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:46:31,150 - __main__ - INFO - Utterance collected: duration=10.59s, chunks=331
2025-07-31 03:46:31,323 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:46:31,323 - __main__ - INFO - Starting utterance processing
2025-07-31 03:46:31,323 - __main__ - INFO - Starting transcription: audio_length=169472, duration=10.59s
2025-07-31 03:46:31,337 - __main__ - INFO - ASR preprocessing completed in 0.014s
2025-07-31 03:46:32,201 - __main__ - INFO - ASR generation completed in 0.864s
2025-07-31 03:46:32,211 - __main__ - INFO - ASR decoding completed in 0.010s
2025-07-31 03:46:32,211 - __main__ - INFO - ASR transcription completed in 0.888s: 'that was a mis annunciation i was saying hello i am making a machine learning model'
2025-07-31 03:46:32,212 - __main__ - INFO - ASR processing completed in 0.889s
2025-07-31 03:46:32,212 - __main__ - INFO - User transcription: 'that was a mis annunciation i was saying hello i am making a machine learning model'
2025-07-31 03:46:32,212 - __main__ - INFO - User message added to queue and history
2025-07-31 03:46:32,212 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=4
2025-07-31 03:46:32,213 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:46:34,340 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:46:37,322 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:46:37,322 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:46:37,424 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:46:37,426 - __main__ - INFO - LLM processing completed in 5.214s
2025-07-31 03:46:37,426 - __main__ - INFO - Ollama response: 'It seems like we're starting our conversation about handles. Can you please tell me more about it. W...'
2025-07-31 03:46:37,426 - __main__ - INFO - Assistant message added to queue and history
2025-07-31 03:46:37,427 - __main__ - INFO - Starting TTS synthesis: text_length=180, voice=af_heart
2025-07-31 03:46:37,796 - __main__ - INFO - TTS generation completed in 0.370s
2025-07-31 03:46:37,797 - __main__ - INFO - TTS audio concatenation completed in 0.001s
2025-07-31 03:46:37,797 - __main__ - INFO - TTS synthesis completed in 0.370s: audio_length=257400
2025-07-31 03:46:37,797 - __main__ - INFO - TTS processing completed in 0.370s
2025-07-31 03:46:37,797 - __main__ - INFO - Starting audio playback: duration=10.72s
2025-07-31 03:46:39,476 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:46:39,573 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:46:39,671 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:46:48,646 - __main__ - INFO - Audio playback completed in 10.849s
2025-07-31 03:46:48,646 - __main__ - INFO - Audio playback completed in 10.850s
2025-07-31 03:46:48,646 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:46:48,646 - __main__ - INFO - ASR: 0.889s
2025-07-31 03:46:48,646 - __main__ - INFO - LLM: 5.214s
2025-07-31 03:46:48,646 - __main__ - INFO - TTS: 0.370s
2025-07-31 03:46:48,646 - __main__ - INFO - Playback: 10.850s
2025-07-31 03:46:48,646 - __main__ - INFO - TOTAL: 17.323s
2025-07-31 03:46:48,646 - __main__ - INFO - =====================================
2025-07-31 03:46:48,646 - __main__ - INFO - Utterance processing completed in 17.323s
2025-07-31 03:46:48,683 - __main__ - INFO - Utterance collected: duration=0.10s, chunks=3
2025-07-31 03:46:48,683 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:46:56,721 - __main__ - INFO - Utterance collected: duration=2.08s, chunks=65
2025-07-31 03:46:56,761 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:46:56,761 - __main__ - INFO - Starting utterance processing
2025-07-31 03:46:56,761 - __main__ - INFO - Starting transcription: audio_length=33280, duration=2.08s
2025-07-31 03:46:56,774 - __main__ - INFO - ASR preprocessing completed in 0.013s
2025-07-31 03:46:57,276 - __main__ - INFO - ASR generation completed in 0.501s
2025-07-31 03:46:57,281 - __main__ - INFO - ASR decoding completed in 0.005s
2025-07-31 03:46:57,281 - __main__ - INFO - ASR transcription completed in 0.520s: 'it is for our website'
2025-07-31 03:46:57,281 - __main__ - INFO - ASR processing completed in 0.520s
2025-07-31 03:46:57,281 - __main__ - INFO - User transcription: 'it is for our website'
2025-07-31 03:46:57,281 - __main__ - INFO - User message added to queue and history
2025-07-31 03:46:57,282 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=6
2025-07-31 03:46:57,282 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:46:59,410 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:47:02,819 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:47:02,820 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:47:02,911 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:47:02,913 - __main__ - INFO - LLM processing completed in 5.631s
2025-07-31 03:47:02,913 - __main__ - INFO - Ollama response: 'Here's my next question:

### User:
that was a mis annunciation i was saying hello i am making a mac...'
2025-07-31 03:47:02,913 - __main__ - INFO - Assistant message added to queue and history
2025-07-31 03:47:02,913 - __main__ - INFO - Starting TTS synthesis: text_length=518, voice=af_heart
2025-07-31 03:47:04,170 - __main__ - INFO - TTS generation completed in 1.256s
2025-07-31 03:47:04,171 - __main__ - INFO - TTS audio concatenation completed in 0.001s
2025-07-31 03:47:04,171 - __main__ - INFO - TTS synthesis completed in 1.258s: audio_length=798000
2025-07-31 03:47:04,172 - __main__ - INFO - TTS processing completed in 1.258s
2025-07-31 03:47:04,172 - __main__ - INFO - Starting audio playback: duration=33.25s
2025-07-31 03:47:04,960 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:47:05,052 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:47:05,147 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:47:37,548 - __main__ - INFO - Audio playback completed in 33.377s
2025-07-31 03:47:37,548 - __main__ - INFO - Audio playback completed in 33.377s
2025-07-31 03:47:37,549 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:47:37,549 - __main__ - INFO - ASR: 0.520s
2025-07-31 03:47:37,549 - __main__ - INFO - LLM: 5.631s
2025-07-31 03:47:37,549 - __main__ - INFO - TTS: 1.258s
2025-07-31 03:47:37,549 - __main__ - INFO - Playback: 33.377s
2025-07-31 03:47:37,549 - __main__ - INFO - TOTAL: 40.788s
2025-07-31 03:47:37,549 - __main__ - INFO - =====================================
2025-07-31 03:47:37,549 - __main__ - INFO - Utterance processing completed in 40.788s
2025-07-31 03:47:37,574 - __main__ - INFO - Utterance collected: duration=0.13s, chunks=4
2025-07-31 03:47:37,574 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:47:47,408 - __main__ - INFO - Utterance collected: duration=3.17s, chunks=99
2025-07-31 03:47:47,465 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:47:47,465 - __main__ - INFO - Starting utterance processing
2025-07-31 03:47:47,465 - __main__ - INFO - Starting transcription: audio_length=50688, duration=3.17s
2025-07-31 03:47:47,479 - __main__ - INFO - ASR preprocessing completed in 0.014s
2025-07-31 03:47:48,054 - __main__ - INFO - ASR generation completed in 0.575s
2025-07-31 03:47:48,057 - __main__ - INFO - ASR decoding completed in 0.004s
2025-07-31 03:47:48,057 - __main__ - INFO - ASR transcription completed in 0.593s: 'it is classification model'
2025-07-31 03:47:48,058 - __main__ - INFO - ASR processing completed in 0.594s
2025-07-31 03:47:48,058 - __main__ - INFO - User transcription: 'it is classification model'
2025-07-31 03:47:48,058 - __main__ - INFO - User message added to queue and history
2025-07-31 03:47:48,058 - __main__ - INFO - Sending chat request via LiteLLM: model=llama3.2:1b, messages_count=8
2025-07-31 03:47:48,058 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:47:50,182 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:47:54,576 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:47:54,577 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:47:54,659 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:47:54,662 - __main__ - INFO - LLM processing completed in 6.603s
2025-07-31 03:47:54,663 - __main__ - INFO - Ollama response: '### Assistant:

It seems like we're starting our conversation about handles. Can you please tell me ...'
2025-07-31 03:47:54,663 - __main__ - INFO - Assistant message added to queue and history
2025-07-31 03:47:54,663 - __main__ - INFO - Starting TTS synthesis: text_length=860, voice=af_heart
2025-07-31 03:47:56,703 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:47:56,790 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:47:56,875 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:47:56,880 - __main__ - INFO - TTS generation completed in 2.217s
2025-07-31 03:47:56,882 - __main__ - INFO - TTS audio concatenation completed in 0.003s
2025-07-31 03:47:56,882 - __main__ - INFO - TTS synthesis completed in 2.220s: audio_length=1336800
2025-07-31 03:47:56,883 - __main__ - INFO - TTS processing completed in 2.220s
2025-07-31 03:47:56,883 - __main__ - INFO - Starting audio playback: duration=55.70s
2025-07-31 03:48:15,700 - __main__ - INFO - Audio playback completed in 18.817s
2025-07-31 03:48:15,700 - __main__ - INFO - Audio playback completed in 18.817s
2025-07-31 03:48:15,700 - __main__ - INFO - === UTTERANCE PROCESSING SUMMARY ===
2025-07-31 03:48:15,701 - __main__ - INFO - ASR: 0.594s
2025-07-31 03:48:15,701 - __main__ - INFO - LLM: 6.603s
2025-07-31 03:48:15,701 - __main__ - INFO - TTS: 2.220s
2025-07-31 03:48:15,701 - __main__ - INFO - Playback: 18.817s
2025-07-31 03:48:15,701 - __main__ - INFO - TOTAL: 28.236s
2025-07-31 03:48:15,701 - __main__ - INFO - =====================================
2025-07-31 03:48:15,701 - __main__ - INFO - Utterance processing completed in 28.237s
2025-07-31 03:48:15,756 - __main__ - INFO - Utterance collected: duration=0.16s, chunks=5
2025-07-31 03:48:15,757 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:54:36,373 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 03:55:32,931 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 03:56:14,585 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 03:56:25,145 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 03:56:32,456 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 03:56:36,514 - __main__ - INFO - Initializing TTS Processor
2025-07-31 03:56:40,216 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 03:56:40,216 - __main__ - INFO - Initializing VAD Processor
2025-07-31 03:56:40,820 - __main__ - INFO - VAD model loaded successfully
2025-07-31 03:56:40,820 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 03:56:41,248 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 03:56:42,280 - __main__ - INFO - Starting continuous recording
2025-07-31 03:56:42,582 - __main__ - INFO - Recording started successfully
2025-07-31 03:56:42,582 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 03:56:42,582 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 03:56:45,915 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:56:45,915 - __main__ - INFO - Starting utterance processing
2025-07-31 03:56:47,500 - __main__ - INFO - ASR transcription completed in 1.585s: 'and no'
2025-07-31 03:56:47,553 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:56:49,730 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:56:56,370 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:56:56,372 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:56:58,463 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:56:58,463 - __main__ - INFO - LiteLLM chat completed in 10.961s
2025-07-31 03:56:58,465 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:56:58,530 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:56:58,587 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:57:07,855 - __main__ - INFO - TTS synthesis completed in 9.390s
2025-07-31 03:57:23,194 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:57:23,194 - __main__ - INFO - Starting utterance processing
2025-07-31 03:57:23,674 - __main__ - INFO - ASR transcription completed in 0.480s: 'i was telling you about'
2025-07-31 03:57:23,679 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:57:25,753 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:57:28,742 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:57:28,744 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:57:28,802 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:57:28,803 - __main__ - INFO - LiteLLM chat completed in 5.124s
2025-07-31 03:57:29,508 - __main__ - INFO - TTS synthesis completed in 0.704s
2025-07-31 03:57:30,821 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:57:30,878 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:57:30,930 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:57:53,812 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:57:53,812 - __main__ - INFO - Starting utterance processing
2025-07-31 03:57:54,274 - __main__ - INFO - ASR transcription completed in 0.463s: 'still a fair'
2025-07-31 03:57:54,279 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:57:56,355 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:57:59,605 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:57:59,606 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:57:59,683 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:57:59,683 - __main__ - INFO - LiteLLM chat completed in 5.404s
2025-07-31 03:58:00,802 - __main__ - INFO - TTS synthesis completed in 1.117s
2025-07-31 03:58:01,685 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:58:01,743 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:58:01,798 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:58:04,699 - __main__ - INFO - Stopping recording
2025-07-31 03:58:04,745 - __main__ - INFO - Audio stream closed successfully
2025-07-31 03:58:05,752 - __main__ - WARNING - Queue was full when adding sentinel value
2025-07-31 03:58:05,752 - __main__ - INFO - Recording stopped. Total chunks recorded: 2561
2025-07-31 03:58:33,521 - __main__ - INFO - Starting continuous recording
2025-07-31 03:58:33,750 - __main__ - INFO - Recording started successfully
2025-07-31 03:58:33,752 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 03:58:33,752 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 03:58:39,928 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:58:39,928 - __main__ - INFO - Starting utterance processing
2025-07-31 03:58:41,697 - __main__ - INFO - ASR transcription completed in 1.768s: 'i am working on a machine learning model'
2025-07-31 03:58:41,702 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:58:43,807 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:58:46,495 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:58:46,495 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:58:46,550 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:58:46,551 - __main__ - INFO - LiteLLM chat completed in 4.854s
2025-07-31 03:58:46,844 - __main__ - INFO - TTS synthesis completed in 0.293s
2025-07-31 03:58:48,564 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:58:48,618 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:58:48,676 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:58:55,673 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:59:04,762 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:59:04,762 - __main__ - INFO - Starting utterance processing
2025-07-31 03:59:06,687 - __main__ - INFO - ASR transcription completed in 1.925s: 'it is a classification model to detect cancer cells in the body photo'
2025-07-31 03:59:06,692 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:59:08,772 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:59:11,643 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:59:11,644 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:59:11,699 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:59:11,699 - __main__ - INFO - LiteLLM chat completed in 5.008s
2025-07-31 03:59:12,202 - __main__ - INFO - TTS synthesis completed in 0.503s
2025-07-31 03:59:13,715 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:59:13,771 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:59:13,826 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:59:23,429 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:59:30,474 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:59:30,474 - __main__ - INFO - Starting utterance processing
2025-07-31 03:59:32,342 - __main__ - INFO - ASR transcription completed in 1.869s: 'it is a strong project that i am doing'
2025-07-31 03:59:32,345 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 03:59:34,411 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:59:37,299 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 03:59:37,300 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 03:59:37,357 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:59:37,358 - __main__ - INFO - LiteLLM chat completed in 5.014s
2025-07-31 03:59:37,801 - __main__ - INFO - TTS synthesis completed in 0.442s
2025-07-31 03:59:39,378 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:59:39,428 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:59:39,484 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 03:59:50,825 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 03:59:58,642 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 03:59:58,642 - __main__ - INFO - Starting utterance processing
2025-07-31 04:00:00,448 - __main__ - INFO - ASR transcription completed in 1.806s: 'it is a small project that i am'
2025-07-31 04:00:00,448 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 04:00:02,514 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:00:05,294 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:00:05,294 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:00:05,351 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:00:05,352 - __main__ - INFO - LiteLLM chat completed in 4.904s
2025-07-31 04:00:05,873 - __main__ - INFO - TTS synthesis completed in 0.520s
2025-07-31 04:00:07,372 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:00:07,432 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:00:07,491 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:00:20,329 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:00:26,482 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:00:26,482 - __main__ - INFO - Starting utterance processing
2025-07-31 04:00:28,289 - __main__ - INFO - ASR transcription completed in 1.807s: 'i am working on a computer vision project'
2025-07-31 04:00:28,294 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 04:00:30,403 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:00:33,551 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:00:33,552 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:00:33,609 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:00:33,609 - __main__ - INFO - LiteLLM chat completed in 5.316s
2025-07-31 04:00:34,358 - __main__ - INFO - TTS synthesis completed in 0.747s
2025-07-31 04:00:35,632 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:00:35,686 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:00:35,743 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:00:58,244 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:01:08,817 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:01:08,817 - __main__ - INFO - Starting utterance processing
2025-07-31 04:01:10,707 - __main__ - INFO - ASR transcription completed in 1.890s: 'the computer vision project is a work cycle cell detection'
2025-07-31 04:01:10,709 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 04:01:12,793 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:01:15,853 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:01:15,854 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:01:15,906 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:01:15,907 - __main__ - INFO - LiteLLM chat completed in 5.200s
2025-07-31 04:01:16,632 - __main__ - INFO - TTS synthesis completed in 0.724s
2025-07-31 04:01:17,925 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:01:17,982 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:01:18,044 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:01:23,510 - __main__ - INFO - Stopping recording
2025-07-31 04:01:23,575 - __main__ - INFO - Audio stream closed successfully
2025-07-31 04:01:24,582 - __main__ - WARNING - Queue was full when adding sentinel value
2025-07-31 04:01:24,582 - __main__ - INFO - Recording stopped. Total chunks recorded: 5297
2025-07-31 04:05:55,082 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 04:06:42,022 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 04:06:52,580 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 04:06:58,922 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 04:07:05,094 - __main__ - INFO - Initializing TTS Processor
2025-07-31 04:07:09,190 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 04:07:09,190 - __main__ - INFO - Initializing VAD Processor
2025-07-31 04:07:09,814 - __main__ - INFO - VAD model loaded successfully
2025-07-31 04:07:09,814 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 04:07:10,352 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 04:07:12,839 - __main__ - INFO - Starting continuous recording
2025-07-31 04:07:13,221 - __main__ - INFO - Recording started successfully
2025-07-31 04:07:13,224 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 04:07:13,224 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 04:07:19,093 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:07:19,093 - __main__ - INFO - Starting utterance processing
2025-07-31 04:07:20,832 - __main__ - INFO - ASR transcription completed in 1.740s: 'happy gene reveal'
2025-07-31 04:07:20,896 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:1b; provider = ollama
2025-07-31 04:07:23,135 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:07:30,858 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:07:30,871 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:07:33,030 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:07:33,032 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:07:33,033 - __main__ - INFO - LiteLLM chat completed in 12.199s
2025-07-31 04:07:33,168 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:07:33,291 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:07:35,341 - __main__ - INFO - TTS synthesis completed in 2.306s
2025-07-31 04:08:01,474 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:08:01,474 - __main__ - INFO - Starting utterance processing
2025-07-31 04:08:02,314 - __main__ - INFO - ASR transcription completed in 0.840s: 'that i was trying to say that i am working on a machine in the order'
2025-07-31 04:08:02,316 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:1b; provider = ollama
2025-07-31 04:08:04,451 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:07,226 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:08:07,227 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:08:07,333 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:07,333 - __main__ - INFO - LiteLLM chat completed in 5.020s
2025-07-31 04:08:07,657 - __main__ - INFO - TTS synthesis completed in 0.321s
2025-07-31 04:08:09,359 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:09,473 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:09,581 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:17,765 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:08:23,250 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:08:23,250 - __main__ - INFO - Starting utterance processing
2025-07-31 04:08:23,994 - __main__ - INFO - ASR transcription completed in 0.743s: 'it is a car with a v 8 engine'
2025-07-31 04:08:23,997 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:1b; provider = ollama
2025-07-31 04:08:26,127 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:28,768 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:08:28,770 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:08:28,878 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:28,880 - __main__ - INFO - LiteLLM chat completed in 4.886s
2025-07-31 04:08:29,172 - __main__ - INFO - TTS synthesis completed in 0.292s
2025-07-31 04:08:30,913 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:31,025 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:31,137 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:40,470 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:08:50,039 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:08:50,039 - __main__ - INFO - Starting utterance processing
2025-07-31 04:08:50,983 - __main__ - INFO - ASR transcription completed in 0.944s: 'it is supposed to be a drag card that is going to outperform each card that i know'
2025-07-31 04:08:50,985 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:1b; provider = ollama
2025-07-31 04:08:53,144 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:55,780 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:08:55,780 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:08:55,886 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:55,886 - __main__ - INFO - LiteLLM chat completed in 4.900s
2025-07-31 04:08:56,201 - __main__ - INFO - TTS synthesis completed in 0.312s
2025-07-31 04:08:57,911 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:58,020 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:08:58,132 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:09:08,085 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:09:16,678 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:09:16,678 - __main__ - INFO - Starting utterance processing
2025-07-31 04:09:17,349 - __main__ - INFO - ASR transcription completed in 0.671s: 'i am going to take this time to connect hard'
2025-07-31 04:09:17,350 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:1b; provider = ollama
2025-07-31 04:09:19,482 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:09:22,215 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:09:22,215 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:09:22,328 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:09:22,328 - __main__ - INFO - LiteLLM chat completed in 4.979s
2025-07-31 04:09:22,650 - __main__ - INFO - TTS synthesis completed in 0.320s
2025-07-31 04:09:24,360 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:09:24,473 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:09:24,586 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:09:34,787 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:09:38,249 - __main__ - INFO - Stopping recording
2025-07-31 04:09:38,320 - __main__ - INFO - Audio stream closed successfully
2025-07-31 04:09:38,320 - __main__ - INFO - Sentinel value added to queue
2025-07-31 04:09:38,320 - __main__ - INFO - Recording stopped. Total chunks recorded: 4524
2025-07-31 04:09:40,824 - __main__ - INFO - Starting continuous recording
2025-07-31 04:09:41,022 - __main__ - INFO - Recording started successfully
2025-07-31 04:09:41,022 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 04:09:41,022 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 04:09:49,488 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:09:49,488 - __main__ - INFO - Starting utterance processing
2025-07-31 04:09:50,032 - __main__ - INFO - ASR transcription completed in 0.544s: 'hello'
2025-07-31 04:09:50,034 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:1b; provider = ollama
2025-07-31 04:09:52,167 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:09:54,609 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:09:54,610 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:09:54,721 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:09:54,722 - __main__ - INFO - LiteLLM chat completed in 4.689s
2025-07-31 04:09:54,877 - __main__ - INFO - TTS synthesis completed in 0.154s
2025-07-31 04:09:56,375 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:09:56,755 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:09:56,866 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:09:56,979 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:02,405 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:10:02,407 - __main__ - INFO - Starting utterance processing
2025-07-31 04:10:02,958 - __main__ - INFO - ASR transcription completed in 0.551s: 'can you recall what we were talking about'
2025-07-31 04:10:02,958 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:1b; provider = ollama
2025-07-31 04:10:05,100 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:07,534 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:10:07,535 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:10:07,649 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:07,649 - __main__ - INFO - LiteLLM chat completed in 4.691s
2025-07-31 04:10:07,828 - __main__ - INFO - TTS synthesis completed in 0.179s
2025-07-31 04:10:09,671 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:09,782 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:09,897 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:12,130 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:10:21,494 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:10:21,494 - __main__ - INFO - Starting utterance processing
2025-07-31 04:10:22,332 - __main__ - INFO - ASR transcription completed in 0.839s: 'yes tell me what we were talking about was it cars cars what was it'
2025-07-31 04:10:22,336 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:1b; provider = ollama
2025-07-31 04:10:24,462 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:26,990 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:10:26,990 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:10:27,101 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:27,102 - __main__ - INFO - LiteLLM chat completed in 4.770s
2025-07-31 04:10:27,236 - __main__ - INFO - TTS synthesis completed in 0.134s
2025-07-31 04:10:29,123 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:29,231 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:29,342 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:10:31,547 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:10:37,546 - __main__ - INFO - Stopping recording
2025-07-31 04:10:37,630 - __main__ - INFO - Audio stream closed successfully
2025-07-31 04:10:37,630 - __main__ - INFO - Sentinel value added to queue
2025-07-31 04:10:37,631 - __main__ - INFO - Recording stopped. Total chunks recorded: 1764
2025-07-31 04:11:14,667 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 04:11:25,116 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-large-v3
2025-07-31 04:11:49,583 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 04:11:58,165 - __main__ - INFO - Initializing TTS Processor
2025-07-31 04:12:04,536 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 04:12:04,536 - __main__ - INFO - Initializing VAD Processor
2025-07-31 04:12:05,112 - __main__ - INFO - VAD model loaded successfully
2025-07-31 04:12:05,112 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 04:12:05,402 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 04:12:06,605 - __main__ - INFO - Starting continuous recording
2025-07-31 04:12:06,953 - __main__ - INFO - Recording started successfully
2025-07-31 04:12:06,953 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 04:12:06,953 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 04:12:10,350 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:12:10,351 - __main__ - INFO - Starting utterance processing
2025-07-31 04:12:40,958 - __main__ - INFO - ASR transcription completed in 30.601s: 'hello'
2025-07-31 04:12:41,104 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-07-31 04:12:43,375 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:13:26,688 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:13:26,862 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:13:30,440 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:13:30,440 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:13:30,474 - __main__ - INFO - LiteLLM chat completed in 49.514s
2025-07-31 04:13:30,796 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:13:31,448 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:13:45,049 - __main__ - INFO - TTS synthesis completed in 14.489s
2025-07-31 04:13:54,832 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:13:57,086 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:13:57,086 - __main__ - INFO - Starting utterance processing
2025-07-31 04:14:26,013 - __main__ - INFO - ASR transcription completed in 28.927s: 'thank you'
2025-07-31 04:14:26,059 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-07-31 04:14:28,335 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:14:35,328 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:14:35,346 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:14:37,515 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:14:37,515 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:14:37,772 - __main__ - INFO - LiteLLM chat completed in 11.740s
2025-07-31 04:14:38,112 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:14:38,472 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:14:41,840 - __main__ - INFO - TTS synthesis completed in 4.046s
2025-07-31 04:14:54,094 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:14:54,094 - __main__ - INFO - Starting utterance processing
2025-07-31 04:15:23,344 - __main__ - INFO - ASR transcription completed in 29.241s: 'i am making a machine learning model'
2025-07-31 04:15:23,346 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-07-31 04:15:25,486 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:15:30,361 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:15:30,373 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:15:30,516 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:15:30,519 - __main__ - INFO - LiteLLM chat completed in 7.172s
2025-07-31 04:15:32,444 - __main__ - INFO - TTS synthesis completed in 1.922s
2025-07-31 04:15:32,548 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:15:32,690 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:15:32,807 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:15:41,557 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:15:41,557 - __main__ - INFO - Starting utterance processing
2025-07-31 04:16:10,285 - __main__ - INFO - ASR transcription completed in 28.729s: 'it is a classification model'
2025-07-31 04:16:10,289 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-07-31 04:16:12,444 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:16:17,465 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:16:17,468 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:16:19,632 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:16:19,633 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:16:19,636 - __main__ - INFO - LiteLLM chat completed in 9.347s
2025-07-31 04:16:19,778 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:16:19,903 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:16:21,828 - __main__ - INFO - TTS synthesis completed in 2.191s
2025-07-31 04:16:30,336 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:16:34,637 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:16:34,637 - __main__ - INFO - Starting utterance processing
2025-07-31 04:17:04,239 - __main__ - INFO - ASR transcription completed in 29.601s: 'some data that i have collected'
2025-07-31 04:17:04,242 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-07-31 04:17:06,390 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:17:12,373 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:17:12,380 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:17:14,532 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:17:14,573 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:17:14,576 - __main__ - INFO - LiteLLM chat completed in 10.338s
2025-07-31 04:17:14,699 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:17:14,845 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:17:16,979 - __main__ - INFO - TTS synthesis completed in 2.401s
2025-07-31 04:17:21,669 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:17:21,669 - __main__ - INFO - Starting utterance processing
2025-07-31 04:17:51,569 - __main__ - INFO - ASR transcription completed in 29.900s: 'you'
2025-07-31 04:17:51,571 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-07-31 04:17:53,712 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:17:59,578 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:17:59,581 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:18:01,732 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:18:01,734 - __main__ - INFO - LiteLLM chat completed in 10.165s
2025-07-31 04:18:01,757 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:18:01,910 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:18:02,036 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:18:04,755 - __main__ - INFO - TTS synthesis completed in 3.020s
2025-07-31 04:18:11,944 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 04:18:17,481 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:18:17,481 - __main__ - INFO - Starting utterance processing
2025-07-31 04:18:46,572 - __main__ - INFO - ASR transcription completed in 29.092s: 'the collective state of being'
2025-07-31 04:18:46,579 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-07-31 04:18:48,728 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:18:55,872 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:18:55,878 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:18:58,053 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:18:58,057 - __main__ - INFO - LiteLLM chat completed in 11.478s
2025-07-31 04:18:58,094 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:18:58,256 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:18:58,386 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:19:01,601 - __main__ - INFO - TTS synthesis completed in 3.540s
2025-07-31 04:19:16,490 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 04:19:16,490 - __main__ - INFO - Starting utterance processing
2025-07-31 04:19:46,187 - __main__ - INFO - ASR transcription completed in 29.697s: 'can you get me'
2025-07-31 04:19:46,191 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-07-31 04:19:48,355 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:19:56,181 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 04:19:56,188 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 04:19:58,755 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:19:58,755 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:19:58,762 - __main__ - INFO - LiteLLM chat completed in 12.575s
2025-07-31 04:19:58,910 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:19:59,044 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 04:20:02,365 - __main__ - INFO - TTS synthesis completed in 3.602s
2025-07-31 04:20:03,275 - __main__ - INFO - Stopping recording
2025-07-31 04:20:03,438 - __main__ - INFO - Audio stream closed successfully
2025-07-31 04:20:04,444 - __main__ - WARNING - Queue was full when adding sentinel value
2025-07-31 04:20:04,444 - __main__ - INFO - Recording stopped. Total chunks recorded: 14718
2025-07-31 15:13:39,372 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 15:13:59,328 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 15:14:07,216 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 15:14:08,851 - __main__ - INFO - Initializing TTS Processor
2025-07-31 15:14:12,529 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 15:14:12,529 - __main__ - INFO - Initializing VAD Processor
2025-07-31 15:14:13,836 - __main__ - INFO - VAD model loaded successfully
2025-07-31 15:14:13,836 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 15:14:13,930 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 15:14:20,266 - __main__ - INFO - Starting continuous recording
2025-07-31 15:14:21,051 - __main__ - INFO - Recording started successfully
2025-07-31 15:14:21,052 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 15:14:21,052 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 15:14:23,534 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 15:14:23,534 - __main__ - INFO - Starting utterance processing
2025-07-31 15:14:24,906 - __main__ - INFO - ASR transcription completed in 1.371s: '0'
2025-07-31 15:14:24,944 - LiteLLM - INFO - 
LiteLLM completion() model= llama3; provider = ollama
2025-07-31 15:14:27,283 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 404 Not Found"
2025-07-31 15:14:27,303 - __main__ - ERROR - LiteLLM error: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3' not found"}
2025-07-31 15:14:28,972 - __main__ - INFO - TTS synthesis completed in 1.668s
2025-07-31 15:14:32,767 - __main__ - INFO - VAD rejected utterance as non-speech
2025-07-31 15:15:00,754 - __main__ - INFO - Stopping recording
2025-07-31 15:15:00,821 - __main__ - INFO - Audio stream closed successfully
2025-07-31 15:15:00,823 - __main__ - INFO - Sentinel value added to queue
2025-07-31 15:15:00,823 - __main__ - INFO - Recording stopped. Total chunks recorded: 1239
2025-07-31 15:15:21,742 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-31 15:15:38,347 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-07-31 15:15:44,252 - __main__ - INFO - ASR model loaded successfully on cuda
2025-07-31 15:15:45,879 - __main__ - INFO - Initializing TTS Processor
2025-07-31 15:15:48,469 - __main__ - INFO - TTS pipeline initialized successfully
2025-07-31 15:15:48,469 - __main__ - INFO - Initializing VAD Processor
2025-07-31 15:15:49,046 - __main__ - INFO - VAD model loaded successfully
2025-07-31 15:15:49,046 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-07-31 15:15:49,136 - __main__ - INFO - AudioRecorder initialized successfully
2025-07-31 15:15:50,393 - __main__ - INFO - Starting continuous recording
2025-07-31 15:15:51,090 - __main__ - INFO - Recording started successfully
2025-07-31 15:15:51,092 - __main__ - INFO - Starting continuous audio processing thread
2025-07-31 15:15:51,092 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-07-31 15:15:53,714 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-07-31 15:15:53,714 - __main__ - INFO - Starting utterance processing
2025-07-31 15:15:54,459 - __main__ - INFO - ASR transcription completed in 0.745s: 'so'
2025-07-31 15:15:54,498 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-07-31 15:15:56,589 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 15:16:03,089 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-07-31 15:16:03,093 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-07-31 15:16:05,213 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 15:16:05,216 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 15:16:05,217 - __main__ - INFO - LiteLLM chat completed in 10.756s
2025-07-31 15:16:05,290 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 15:16:05,363 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-07-31 15:16:07,840 - __main__ - INFO - TTS synthesis completed in 2.623s
2025-07-31 15:16:16,868 - __main__ - INFO - Stopping recording
2025-07-31 15:16:16,918 - __main__ - INFO - Audio stream closed successfully
2025-07-31 15:16:17,918 - __main__ - WARNING - Queue was full when adding sentinel value
2025-07-31 15:16:17,918 - __main__ - INFO - Recording stopped. Total chunks recorded: 803
2025-07-31 15:16:18,316 - __main__ - INFO - Session data saved to sessions\session_20250731_151618.json
2025-08-01 01:58:24,751 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 01:58:37,775 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 01:58:52,437 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-08-01 02:05:01,436 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 02:05:24,005 - __main__ - INFO - Initializing TTS Processor
2025-08-01 02:05:28,078 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 02:05:28,078 - __main__ - INFO - Initializing VAD Processor
2025-08-01 02:05:28,639 - __main__ - INFO - VAD model loaded successfully
2025-08-01 02:05:28,639 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 02:05:28,728 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 02:05:32,157 - __main__ - INFO - Starting continuous recording
2025-08-01 02:05:32,417 - __main__ - INFO - Recording started successfully
2025-08-01 02:05:32,419 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 02:05:32,419 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 02:05:35,390 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 02:05:35,390 - __main__ - INFO - Starting utterance processing
2025-08-01 02:05:37,309 - __main__ - INFO - ASR transcription completed in 1.919s: 'hello'
2025-08-01 02:05:37,390 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 02:05:41,356 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 02:05:41,361 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 02:05:41,367 - __main__ - INFO - LiteLLM chat completed in 4.058s
2025-08-01 02:05:43,645 - __main__ - INFO - TTS synthesis completed in 2.275s
2025-08-01 02:05:50,899 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 02:05:55,421 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 02:05:55,421 - __main__ - INFO - Starting utterance processing
2025-08-01 02:05:56,443 - __main__ - INFO - ASR transcription completed in 1.021s: 'i am working on a machine learning model'
2025-08-01 02:05:56,445 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 02:05:58,063 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 02:05:58,067 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 02:05:58,067 - __main__ - INFO - LiteLLM chat completed in 1.623s
2025-08-01 02:05:58,235 - __main__ - INFO - TTS synthesis completed in 0.167s
2025-08-01 02:06:02,129 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 02:06:05,880 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 02:06:05,880 - __main__ - INFO - Starting utterance processing
2025-08-01 02:06:06,849 - __main__ - INFO - ASR transcription completed in 0.969s: 'it is a classification model'
2025-08-01 02:06:06,851 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 02:06:08,572 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 02:06:08,573 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 02:06:08,574 - __main__ - INFO - LiteLLM chat completed in 1.725s
2025-08-01 02:06:08,799 - __main__ - INFO - TTS synthesis completed in 0.223s
2025-08-01 02:06:13,574 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 02:06:18,499 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 02:06:18,499 - __main__ - INFO - Starting utterance processing
2025-08-01 02:06:19,568 - __main__ - INFO - ASR transcription completed in 1.069s: 'i am using logistic regulation for this'
2025-08-01 02:06:19,570 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 02:06:20,598 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 02:06:20,599 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 02:06:20,600 - __main__ - INFO - LiteLLM chat completed in 1.031s
2025-08-01 02:06:20,784 - __main__ - INFO - TTS synthesis completed in 0.183s
2025-08-01 02:06:26,702 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 02:06:35,499 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 02:06:35,499 - __main__ - INFO - Starting utterance processing
2025-08-01 02:06:36,747 - __main__ - INFO - ASR transcription completed in 1.248s: 'it is a data that i collected and then i have to go again'
2025-08-01 02:06:36,749 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 02:06:37,881 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 02:06:37,881 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 02:06:37,883 - __main__ - INFO - LiteLLM chat completed in 1.135s
2025-08-01 02:06:38,093 - __main__ - INFO - TTS synthesis completed in 0.208s
2025-08-01 02:06:44,302 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 02:06:53,490 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 02:06:53,490 - __main__ - INFO - Starting utterance processing
2025-08-01 02:06:54,891 - __main__ - INFO - ASR transcription completed in 1.400s: 'that was a mispronunciation what i meant to say was that i collected the data from kaggle'
2025-08-01 02:06:54,892 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 02:06:56,185 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 02:06:56,186 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 02:06:56,186 - __main__ - INFO - LiteLLM chat completed in 1.296s
2025-08-01 02:06:56,427 - __main__ - INFO - TTS synthesis completed in 0.239s
2025-08-01 02:07:03,728 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 02:07:12,044 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 02:07:12,044 - __main__ - INFO - Starting utterance processing
2025-08-01 02:07:13,152 - __main__ - INFO - ASR transcription completed in 1.108s: 'it is like sickle cells in a text'
2025-08-01 02:07:13,154 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 02:07:18,716 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 02:07:18,720 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 02:07:18,721 - __main__ - INFO - LiteLLM chat completed in 5.569s
2025-08-01 02:07:19,041 - __main__ - INFO - TTS synthesis completed in 0.318s
2025-08-01 02:07:27,210 - __main__ - INFO - Stopping recording
2025-08-01 02:07:27,270 - __main__ - INFO - Audio stream closed successfully
2025-08-01 02:07:27,270 - __main__ - INFO - Sentinel value added to queue
2025-08-01 02:07:27,270 - __main__ - INFO - Recording stopped. Total chunks recorded: 3584
2025-08-01 02:07:27,274 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 10:50:24,841 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 10:50:33,371 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 10:50:48,622 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 10:50:53,962 - __main__ - INFO - Initializing TTS Processor
2025-08-01 10:50:58,628 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 10:50:58,629 - __main__ - INFO - Initializing VAD Processor
2025-08-01 10:50:59,461 - __main__ - INFO - VAD model loaded successfully
2025-08-01 10:50:59,461 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 10:50:59,593 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 10:50:59,593 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 10:51:16,035 - __main__ - INFO - Starting continuous recording
2025-08-01 10:51:16,286 - __main__ - INFO - Recording started successfully
2025-08-01 10:51:16,286 - __main__ - INFO - Starting camera recording
2025-08-01 10:51:17,973 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 10:51:17,973 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 10:51:18,712 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 10:51:20,129 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 10:51:20,130 - __main__ - INFO - Starting utterance processing
2025-08-01 10:51:20,991 - __main__ - INFO - ASR transcription completed in 0.861s: 'hello'
2025-08-01 10:51:21,103 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 10:51:21,838 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=GEMINI "HTTP/1.1 400 Bad Request"
2025-08-01 10:51:21,868 - __main__ - ERROR - LiteLLM error: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-08-01 10:51:23,638 - __main__ - INFO - TTS synthesis completed in 1.771s
2025-08-01 10:51:27,471 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 10:51:37,980 - __main__ - INFO - Stopping recording
2025-08-01 10:51:38,037 - __main__ - INFO - Audio stream closed successfully
2025-08-01 10:51:38,037 - __main__ - INFO - Sentinel value added to queue
2025-08-01 10:51:38,038 - __main__ - INFO - Recording stopped. Total chunks recorded: 676
2025-08-01 10:51:38,038 - __main__ - INFO - Stopping camera recording
2025-08-01 10:51:38,525 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 11:08:18,747 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 11:08:32,258 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 11:08:43,997 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 11:08:51,413 - __main__ - INFO - Initializing TTS Processor
2025-08-01 11:08:54,843 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 11:08:54,843 - __main__ - INFO - Initializing VAD Processor
2025-08-01 11:08:55,696 - __main__ - INFO - VAD model loaded successfully
2025-08-01 11:08:55,696 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 11:08:55,751 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 11:08:55,751 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 11:08:56,592 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:08:56,636 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 11:09:02,065 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:09:04,209 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:09:18,537 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:09:18,541 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:09:20,655 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:09:20,656 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:09:20,660 - __main__ - INFO - LiteLLM chat completed in 18.621s
2025-08-01 11:09:20,716 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:09:20,766 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:09:24,748 - __main__ - INFO - TTS synthesis completed in 4.087s
2025-08-01 11:10:10,537 - __main__ - INFO - Starting continuous recording
2025-08-01 11:10:10,791 - __main__ - INFO - Recording started successfully
2025-08-01 11:10:10,791 - __main__ - INFO - Starting camera recording
2025-08-01 11:10:12,377 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:10:12,377 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:10:12,801 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 11:10:22,211 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:10:22,211 - __main__ - INFO - Starting utterance processing
2025-08-01 11:10:24,306 - __main__ - INFO - ASR transcription completed in 2.095s: 'i have worked on many deep learning projects such as making bert and also many computer vision projects'
2025-08-01 11:10:24,309 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:10:26,415 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:10:35,004 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:10:35,009 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:10:37,118 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:10:37,119 - __main__ - INFO - LiteLLM chat completed in 12.812s
2025-08-01 11:10:37,136 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:10:37,210 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:10:37,277 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:10:38,265 - __main__ - INFO - TTS synthesis completed in 1.144s
2025-08-01 11:11:04,671 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 11:11:22,002 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:11:22,003 - __main__ - INFO - Starting utterance processing
2025-08-01 11:11:29,245 - __main__ - INFO - ASR transcription completed in 7.241s: 'i used the deep learning model to enhance the large drain based models as large drain based models can many times have hallucinations so it would be reduced by the use of deep learning model'
2025-08-01 11:11:29,247 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:11:31,347 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:11:40,625 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:11:40,627 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:11:42,745 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:11:42,753 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:11:42,755 - __main__ - INFO - LiteLLM chat completed in 13.509s
2025-08-01 11:11:42,819 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:11:42,883 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:11:44,258 - __main__ - INFO - TTS synthesis completed in 1.502s
2025-08-01 11:12:16,234 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 11:12:20,428 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:12:20,428 - __main__ - INFO - Starting utterance processing
2025-08-01 11:12:23,986 - __main__ - INFO - ASR transcription completed in 3.558s: 'and'
2025-08-01 11:12:23,988 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:12:26,110 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:12:35,999 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:12:36,002 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:12:38,102 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:12:38,103 - __main__ - INFO - LiteLLM chat completed in 14.115s
2025-08-01 11:12:38,108 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:12:38,179 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:12:38,251 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:12:43,377 - __main__ - INFO - TTS synthesis completed in 5.272s
2025-08-01 11:12:55,768 - __main__ - INFO - Stopping recording
2025-08-01 11:12:55,839 - __main__ - INFO - Audio stream closed successfully
2025-08-01 11:12:56,850 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-01 11:12:56,850 - __main__ - INFO - Recording stopped. Total chunks recorded: 5110
2025-08-01 11:12:56,850 - __main__ - INFO - Stopping camera recording
2025-08-01 11:13:16,247 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 11:13:16,928 - __main__ - INFO - Starting continuous recording
2025-08-01 11:13:17,092 - __main__ - INFO - Recording started successfully
2025-08-01 11:13:17,092 - __main__ - INFO - Starting camera recording
2025-08-01 11:13:18,387 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:13:18,387 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:13:18,411 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 11:13:21,862 - __main__ - INFO - Stopping recording
2025-08-01 11:13:21,908 - __main__ - INFO - Audio stream closed successfully
2025-08-01 11:13:21,909 - __main__ - INFO - Sentinel value added to queue
2025-08-01 11:13:21,909 - __main__ - INFO - Recording stopped. Total chunks recorded: 148
2025-08-01 11:13:21,909 - __main__ - INFO - Stopping camera recording
2025-08-01 11:13:22,326 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 11:14:58,772 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 11:15:06,940 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 11:15:18,503 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 11:15:23,932 - __main__ - INFO - Initializing TTS Processor
2025-08-01 11:15:28,061 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 11:15:28,061 - __main__ - INFO - Initializing VAD Processor
2025-08-01 11:15:28,759 - __main__ - INFO - VAD model loaded successfully
2025-08-01 11:15:28,759 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 11:15:29,106 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 11:15:29,106 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 11:15:29,965 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:15:30,016 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 11:15:36,473 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 11:15:41,478 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 11:15:41,484 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:15:41,496 - __main__ - INFO - LiteLLM chat completed in 5.117s
2025-08-01 11:15:45,222 - __main__ - INFO - TTS synthesis completed in 3.726s
2025-08-01 11:16:08,896 - __main__ - INFO - Starting continuous recording
2025-08-01 11:16:09,087 - __main__ - INFO - Recording started successfully
2025-08-01 11:16:09,087 - __main__ - INFO - Starting camera recording
2025-08-01 11:16:10,604 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:16:10,604 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:16:11,042 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 11:16:22,928 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:16:22,928 - __main__ - INFO - Starting utterance processing
2025-08-01 11:16:25,027 - __main__ - INFO - ASR transcription completed in 2.098s: 'so i used those techniques to enhance the performance of large damage models specifically models that are based on the transformer is architecture'
2025-08-01 11:16:25,030 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 11:16:28,720 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 11:16:28,723 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:16:28,723 - __main__ - INFO - LiteLLM chat completed in 3.694s
2025-08-01 11:16:29,186 - __main__ - INFO - TTS synthesis completed in 0.460s
2025-08-01 11:16:40,540 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 11:16:55,015 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:16:55,015 - __main__ - INFO - Starting utterance processing
2025-08-01 11:16:57,014 - __main__ - INFO - ASR transcription completed in 1.999s: 'okay so i pre tink the model p 5 base that is published by google and i applied fine tuning of sfd on that'
2025-08-01 11:16:57,016 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 11:16:58,459 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 11:16:58,459 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:16:58,459 - __main__ - INFO - LiteLLM chat completed in 1.444s
2025-08-01 11:16:58,878 - __main__ - INFO - TTS synthesis completed in 0.413s
2025-08-01 11:17:11,077 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 11:17:16,194 - __main__ - INFO - Stopping recording
2025-08-01 11:17:16,256 - __main__ - INFO - Audio stream closed successfully
2025-08-01 11:17:16,256 - __main__ - INFO - Sentinel value added to queue
2025-08-01 11:17:16,256 - __main__ - INFO - Recording stopped. Total chunks recorded: 2094
2025-08-01 11:17:16,256 - __main__ - INFO - Stopping camera recording
2025-08-01 11:17:16,713 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 11:26:23,047 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 11:26:36,495 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 11:26:50,535 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 11:27:03,215 - __main__ - INFO - Initializing TTS Processor
2025-08-01 11:27:07,028 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 11:27:07,028 - __main__ - INFO - Initializing VAD Processor
2025-08-01 11:27:07,230 - __main__ - INFO - VAD model loaded successfully
2025-08-01 11:27:07,230 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 11:27:07,285 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 11:27:07,285 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 11:27:08,212 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:27:08,251 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 11:27:18,724 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 11:27:20,961 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:27:56,993 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:27:56,999 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:27:59,157 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:27:59,157 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:27:59,157 - __main__ - INFO - LiteLLM chat completed in 40.464s
2025-08-01 11:27:59,288 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:27:59,403 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:28:02,435 - __main__ - INFO - TTS synthesis completed in 3.274s
2025-08-01 11:28:20,142 - __main__ - INFO - Starting continuous recording
2025-08-01 11:28:20,336 - __main__ - INFO - Recording started successfully
2025-08-01 11:28:20,336 - __main__ - INFO - Starting camera recording
2025-08-01 11:28:21,967 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:28:21,967 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:28:22,690 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 11:28:24,770 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:28:24,772 - __main__ - INFO - Starting utterance processing
2025-08-01 11:28:25,712 - __main__ - INFO - ASR transcription completed in 0.940s: 'okay so that'
2025-08-01 11:28:25,715 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 11:28:27,860 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:28:55,579 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:28:55,587 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:28:57,784 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:28:57,793 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:28:57,796 - __main__ - INFO - LiteLLM chat completed in 32.084s
2025-08-01 11:28:57,919 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:28:58,058 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:28:58,184 - __main__ - INFO - TTS synthesis completed in 0.387s
2025-08-01 11:29:14,273 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:29:14,274 - __main__ - INFO - Starting utterance processing
2025-08-01 11:29:14,999 - __main__ - INFO - ASR transcription completed in 0.725s: 'i collected from'
2025-08-01 11:29:15,001 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 11:29:17,185 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:29:49,448 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:29:49,450 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:29:51,607 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:29:51,608 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:29:51,611 - __main__ - INFO - LiteLLM chat completed in 36.610s
2025-08-01 11:29:51,745 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:29:51,881 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:29:52,562 - __main__ - INFO - TTS synthesis completed in 0.949s
2025-08-01 11:30:08,173 - __main__ - INFO - Stopping recording
2025-08-01 11:30:08,212 - __main__ - INFO - Audio stream closed successfully
2025-08-01 11:30:09,216 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-01 11:30:09,216 - __main__ - INFO - Recording stopped. Total chunks recorded: 3358
2025-08-01 11:30:09,216 - __main__ - INFO - Stopping camera recording
2025-08-01 11:30:09,665 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 11:40:58,435 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 11:41:05,465 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 11:41:19,469 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 11:41:40,443 - __main__ - INFO - Initializing TTS Processor
2025-08-01 11:41:44,365 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 11:41:44,365 - __main__ - INFO - Initializing VAD Processor
2025-08-01 11:41:44,965 - __main__ - INFO - VAD model loaded successfully
2025-08-01 11:41:44,965 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 11:41:45,081 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 11:41:45,081 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 11:41:45,909 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:41:45,969 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 11:41:51,734 - __main__ - INFO - Starting continuous recording
2025-08-01 11:41:52,045 - __main__ - INFO - Recording started successfully
2025-08-01 11:41:52,045 - __main__ - INFO - Starting camera recording
2025-08-01 11:41:53,685 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:41:53,685 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:41:53,739 - LiteLLM - INFO - 
LiteLLM completion() model= qwen3:0.6b; provider = ollama
2025-08-01 11:41:54,327 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 11:41:55,909 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:42:13,671 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:42:13,702 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:42:15,825 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:42:15,832 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:42:15,834 - __main__ - INFO - LiteLLM chat completed in 22.150s
2025-08-01 11:42:15,882 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:42:15,935 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:42:26,403 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-01 11:42:27,223 - __main__ - INFO - TTS synthesis completed in 11.386s
2025-08-01 11:43:54,784 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:43:54,784 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:44:00,321 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:44:00,321 - __main__ - INFO - Starting utterance processing
2025-08-01 11:45:00,185 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 11:45:10,007 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 11:45:22,823 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 11:45:28,710 - __main__ - INFO - Initializing TTS Processor
2025-08-01 11:45:32,398 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 11:45:32,398 - __main__ - INFO - Initializing VAD Processor
2025-08-01 11:45:32,649 - __main__ - INFO - VAD model loaded successfully
2025-08-01 11:45:32,649 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 11:45:32,709 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 11:45:32,709 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 11:45:33,702 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:45:33,757 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 11:45:42,236 - __main__ - INFO - Starting continuous recording
2025-08-01 11:45:42,441 - __main__ - INFO - Recording started successfully
2025-08-01 11:45:42,441 - __main__ - INFO - Starting camera recording
2025-08-01 11:45:44,098 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:45:44,098 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:45:44,132 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:45:44,713 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 11:45:46,263 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:46:00,163 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:46:00,165 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:46:02,269 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:46:02,271 - __main__ - INFO - LiteLLM chat completed in 18.174s
2025-08-01 11:46:02,273 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:46:02,344 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:46:02,402 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:46:08,521 - __main__ - INFO - TTS synthesis completed in 6.249s
2025-08-01 11:46:38,085 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:46:38,085 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:46:50,080 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:46:50,080 - __main__ - INFO - Starting utterance processing
2025-08-01 11:49:24,524 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 11:49:31,469 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 11:49:42,590 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 11:49:47,157 - __main__ - INFO - Initializing TTS Processor
2025-08-01 11:49:51,037 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 11:49:51,037 - __main__ - INFO - Initializing VAD Processor
2025-08-01 11:49:51,295 - __main__ - INFO - VAD model loaded successfully
2025-08-01 11:49:51,295 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 11:49:51,482 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 11:49:51,482 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 11:49:52,321 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:49:52,364 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 11:50:31,287 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:50:31,287 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:50:31,352 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:50:33,508 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:50:46,750 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:50:46,759 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:50:48,873 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:50:48,877 - __main__ - INFO - LiteLLM chat completed in 17.590s
2025-08-01 11:50:48,894 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:50:48,976 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:50:49,063 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:50:56,234 - __main__ - INFO - TTS synthesis completed in 7.356s
2025-08-01 11:51:42,008 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:51:42,008 - __main__ - INFO - Starting continuous recording
2025-08-01 11:51:42,008 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:51:42,202 - __main__ - INFO - Recording started successfully
2025-08-01 11:51:42,202 - __main__ - INFO - Starting camera recording
2025-08-01 11:51:44,143 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 11:51:49,835 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:51:49,835 - __main__ - INFO - Starting utterance processing
2025-08-01 11:51:54,567 - __main__ - INFO - ASR transcription completed in 4.732s: 'i use c in a project that i made'
2025-08-01 11:51:54,569 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:51:56,648 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:52:02,229 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:52:02,232 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:52:04,323 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:52:04,324 - __main__ - INFO - LiteLLM chat completed in 9.756s
2025-08-01 11:52:04,341 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:52:04,414 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:52:04,478 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:52:04,803 - __main__ - INFO - TTS synthesis completed in 0.478s
2025-08-01 11:52:14,788 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 11:52:23,987 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:52:23,987 - __main__ - INFO - Starting utterance processing
2025-08-01 11:52:29,188 - __main__ - INFO - ASR transcription completed in 5.201s: 'i meant to say was that i have used c in a project that i have made'
2025-08-01 11:52:29,189 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:52:31,278 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:52:36,304 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:52:36,306 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:52:38,408 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:52:38,418 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:52:38,419 - __main__ - INFO - LiteLLM chat completed in 9.230s
2025-08-01 11:52:38,497 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:52:38,571 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:52:39,075 - __main__ - INFO - TTS synthesis completed in 0.655s
2025-08-01 11:52:49,009 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 11:52:54,134 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:52:54,134 - __main__ - INFO - Starting utterance processing
2025-08-01 11:52:58,430 - __main__ - INFO - ASR transcription completed in 4.296s: 'i have not made a machine learning model'
2025-08-01 11:52:58,430 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:53:00,530 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:53:05,549 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:53:05,549 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:53:07,646 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:53:07,646 - __main__ - INFO - LiteLLM chat completed in 9.216s
2025-08-01 11:53:07,656 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:53:07,720 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:53:07,788 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:53:08,074 - __main__ - INFO - TTS synthesis completed in 0.425s
2025-08-01 11:53:20,923 - __main__ - INFO - Stopping recording
2025-08-01 11:53:20,989 - __main__ - INFO - Audio stream closed successfully
2025-08-01 11:53:20,989 - __main__ - INFO - Sentinel value added to queue
2025-08-01 11:53:20,989 - __main__ - INFO - Recording stopped. Total chunks recorded: 3081
2025-08-01 11:53:20,989 - __main__ - INFO - Stopping camera recording
2025-08-01 11:53:21,423 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 11:53:31,552 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:53:31,552 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:53:31,552 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:53:31,552 - __main__ - INFO - Starting continuous recording
2025-08-01 11:53:31,552 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:53:31,691 - __main__ - INFO - Recording started successfully
2025-08-01 11:53:31,691 - __main__ - INFO - Starting camera recording
2025-08-01 11:53:36,423 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:53:36,423 - __main__ - INFO - Starting utterance processing
2025-08-01 11:53:39,927 - __main__ - INFO - ASR transcription completed in 3.503s: 'and'
2025-08-01 11:53:39,928 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:53:42,023 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:53:47,101 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:53:47,103 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:53:49,191 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:53:49,191 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:53:49,191 - __main__ - INFO - LiteLLM chat completed in 9.263s
2025-08-01 11:53:49,255 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:53:49,330 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:53:49,684 - __main__ - INFO - TTS synthesis completed in 0.488s
2025-08-01 11:53:51,127 - __main__ - INFO - Stopping recording
2025-08-01 11:53:51,165 - __main__ - INFO - Audio stream closed successfully
2025-08-01 11:53:52,170 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-01 11:53:52,170 - __main__ - INFO - Recording stopped. Total chunks recorded: 605
2025-08-01 11:53:52,170 - __main__ - INFO - Stopping camera recording
2025-08-01 11:55:56,915 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 11:56:05,373 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 11:56:16,973 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 11:56:25,340 - __main__ - INFO - Initializing TTS Processor
2025-08-01 11:56:28,873 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 11:56:28,873 - __main__ - INFO - Initializing VAD Processor
2025-08-01 11:56:29,706 - __main__ - INFO - VAD model loaded successfully
2025-08-01 11:56:29,706 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 11:56:29,757 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 11:56:29,757 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 11:56:30,737 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:56:30,794 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 11:56:44,356 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:56:46,453 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:56:57,628 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:56:57,633 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:56:59,724 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:56:59,724 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:56:59,727 - __main__ - INFO - LiteLLM chat completed in 15.395s
2025-08-01 11:56:59,781 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:56:59,829 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:57:03,137 - __main__ - INFO - TTS synthesis completed in 3.408s
2025-08-01 11:57:35,494 - __main__ - INFO - Starting continuous recording
2025-08-01 11:57:35,659 - __main__ - INFO - Recording started successfully
2025-08-01 11:57:35,659 - __main__ - INFO - Starting camera recording
2025-08-01 11:57:37,079 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 11:57:37,079 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 11:57:37,514 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 11:57:46,734 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:57:46,734 - __main__ - INFO - Starting utterance processing
2025-08-01 11:57:52,265 - __main__ - INFO - ASR transcription completed in 5.531s: 'i have taken many problems and i also saw them using architectural changes in the model that i make'
2025-08-01 11:57:52,266 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:57:54,363 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:58:05,962 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:58:05,962 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:58:08,065 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:58:08,065 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:58:08,065 - __main__ - INFO - LiteLLM chat completed in 15.799s
2025-08-01 11:58:08,135 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:58:08,209 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:58:09,557 - __main__ - INFO - TTS synthesis completed in 1.487s
2025-08-01 11:58:39,253 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 11:58:55,311 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 11:58:55,311 - __main__ - INFO - Starting utterance processing
2025-08-01 11:59:01,060 - __main__ - INFO - ASR transcription completed in 5.749s: 'i have created a variant of the u net to have better performance by having a dynamic input size to the modern architecture'
2025-08-01 11:59:01,060 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 11:59:03,138 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:59:19,031 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 11:59:19,031 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 11:59:21,131 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:59:21,137 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:59:21,140 - __main__ - INFO - LiteLLM chat completed in 20.079s
2025-08-01 11:59:21,198 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:59:21,262 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 11:59:26,937 - __main__ - INFO - TTS synthesis completed in 5.796s
2025-08-01 12:00:11,249 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 12:00:12,206 - __main__ - INFO - Stopping recording
2025-08-01 12:00:12,273 - __main__ - INFO - Audio stream closed successfully
2025-08-01 12:00:12,273 - __main__ - INFO - Sentinel value added to queue
2025-08-01 12:00:12,273 - __main__ - INFO - Recording stopped. Total chunks recorded: 4856
2025-08-01 12:00:12,273 - __main__ - INFO - Stopping camera recording
2025-08-01 12:00:12,723 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 13:16:08,723 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 13:16:16,352 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 13:16:30,026 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 13:16:35,569 - __main__ - INFO - Initializing TTS Processor
2025-08-01 13:16:40,381 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 13:16:40,381 - __main__ - INFO - Initializing VAD Processor
2025-08-01 13:16:40,702 - __main__ - INFO - VAD model loaded successfully
2025-08-01 13:16:40,702 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 13:16:41,041 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 13:16:41,041 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 13:16:41,049 - rfdetr.main - INFO - Downloading pretrained weights for rf-detr-nano.pth
2025-08-01 13:17:19,718 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 13:17:19,777 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 13:17:29,760 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 13:17:35,042 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 13:17:35,052 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 13:17:35,067 - __main__ - INFO - LiteLLM chat completed in 5.443s
2025-08-01 13:17:38,399 - __main__ - INFO - TTS synthesis completed in 3.330s
2025-08-01 13:18:05,185 - __main__ - INFO - Starting continuous recording
2025-08-01 13:18:05,389 - __main__ - INFO - Recording started successfully
2025-08-01 13:18:05,389 - __main__ - INFO - Starting camera recording
2025-08-01 13:18:07,093 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 13:18:07,093 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 13:18:07,813 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 13:18:14,110 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 13:18:14,110 - __main__ - INFO - Starting utterance processing
2025-08-01 13:18:15,395 - __main__ - INFO - ASR transcription completed in 1.285s: 'i have made those learning algorithms into my machine learning models'
2025-08-01 13:18:15,402 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 13:18:19,978 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 13:18:19,980 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 13:18:19,980 - __main__ - INFO - LiteLLM chat completed in 4.580s
2025-08-01 13:18:20,388 - __main__ - INFO - TTS synthesis completed in 0.408s
2025-08-01 13:18:32,492 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 13:18:45,479 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 13:18:45,482 - __main__ - INFO - Starting utterance processing
2025-08-01 13:18:47,485 - __main__ - INFO - ASR transcription completed in 2.003s: 'ok so i used peft which is parameter efficient fine tuning to fine tune a model that is awful have 1000000000 parameters and it was available for 100 days'
2025-08-01 13:18:47,487 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 13:18:48,990 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 13:18:48,993 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 13:18:48,993 - __main__ - INFO - LiteLLM chat completed in 1.508s
2025-08-01 13:18:49,463 - __main__ - INFO - TTS synthesis completed in 0.469s
2025-08-01 13:19:05,880 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 13:19:17,539 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 13:19:17,539 - __main__ - INFO - Starting utterance processing
2025-08-01 13:19:19,141 - __main__ - INFO - ASR transcription completed in 1.602s: 'so that allowed me to have an efficient adapter on top of the existing model instead of having to change the model weights itself'
2025-08-01 13:19:19,141 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 13:19:23,723 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 13:19:23,723 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 13:19:23,725 - __main__ - INFO - LiteLLM chat completed in 4.584s
2025-08-01 13:19:24,205 - __main__ - INFO - TTS synthesis completed in 0.478s
2025-08-01 13:19:39,783 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 13:19:41,296 - __main__ - INFO - Stopping recording
2025-08-01 13:19:41,343 - __main__ - INFO - Audio stream closed successfully
2025-08-01 13:19:41,343 - __main__ - INFO - Sentinel value added to queue
2025-08-01 13:19:41,343 - __main__ - INFO - Recording stopped. Total chunks recorded: 2987
2025-08-01 13:19:41,343 - __main__ - INFO - Stopping camera recording
2025-08-01 13:19:41,861 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 13:51:10,084 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 13:51:18,556 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 13:51:29,420 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 13:51:38,879 - __main__ - INFO - Initializing TTS Processor
2025-08-01 13:51:42,484 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 13:51:42,484 - __main__ - INFO - Initializing VAD Processor
2025-08-01 13:51:46,731 - __main__ - INFO - VAD model loaded successfully
2025-08-01 13:51:46,731 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 13:51:47,035 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 13:51:47,035 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 13:51:47,840 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 13:51:47,880 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 13:52:00,233 - __main__ - ERROR - Error in CLI main loop: 'name'
2025-08-01 13:53:50,876 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 13:53:59,522 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 13:54:09,773 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 13:54:12,989 - __main__ - INFO - Initializing TTS Processor
2025-08-01 13:54:16,202 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 13:54:16,202 - __main__ - INFO - Initializing VAD Processor
2025-08-01 13:54:16,456 - __main__ - INFO - VAD model loaded successfully
2025-08-01 13:54:16,456 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 13:54:16,527 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 13:54:16,528 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 13:54:17,276 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 13:54:17,315 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 13:54:24,156 - __main__ - ERROR - Error in CLI main loop: 'name'
2025-08-01 14:42:46,479 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 14:42:53,851 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 14:43:07,213 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 14:43:11,039 - __main__ - INFO - Initializing TTS Processor
2025-08-01 14:43:15,580 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 14:43:15,584 - __main__ - INFO - Initializing VAD Processor
2025-08-01 14:43:16,372 - __main__ - INFO - VAD model loaded successfully
2025-08-01 14:43:41,004 - __main__ - ERROR - TTS error: 404 Client Error. (Request ID: Root=1-688c8544-29cde00d0ff56e23181d80bc;e2aabdc4-c334-475f-9ff2-3a0ed64ef11c)

Entry Not Found for url: https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/voices/en_US-kathleen-medium.pt.
2025-08-01 14:43:41,004 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 14:43:41,073 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 14:43:41,073 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 14:43:42,031 - __main__ - INFO - Starting continuous recording
2025-08-01 14:43:42,289 - __main__ - INFO - Recording started successfully
2025-08-01 14:43:42,289 - __main__ - INFO - Starting camera recording
2025-08-01 14:43:44,104 - __main__ - INFO - Video recording will be saved to data\session_20250801_144250.avi
2025-08-01 14:43:44,106 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 14:43:44,106 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 14:43:44,789 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 14:43:56,628 - __main__ - INFO - Stopping recording
2025-08-01 14:43:56,677 - __main__ - INFO - Audio stream closed successfully
2025-08-01 14:43:56,678 - __main__ - INFO - Sentinel value added to queue
2025-08-01 14:43:56,678 - __main__ - INFO - Recording stopped. Total chunks recorded: 447
2025-08-01 14:43:56,678 - __main__ - INFO - Stopping camera recording
2025-08-01 14:43:57,162 - __main__ - INFO - Video recording saved to data\session_20250801_144250.avi
2025-08-01 14:43:57,171 - __main__ - INFO - Saving full audio recording to data\session_20250801_144250.wav
2025-08-01 14:43:57,184 - __main__ - INFO - Full audio recording saved successfully to data\session_20250801_144250.wav
2025-08-01 14:43:57,184 - __main__ - INFO - Session data saved to data\session_20250801_144250.json
2025-08-01 17:13:43,976 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 17:13:52,531 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 17:14:04,838 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 17:14:16,363 - __main__ - INFO - Initializing TTS Processor
2025-08-01 17:14:20,716 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 17:14:20,716 - __main__ - INFO - Initializing VAD Processor
2025-08-01 17:14:21,470 - __main__ - INFO - VAD model loaded successfully
2025-08-01 17:14:21,470 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 17:14:21,759 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 17:14:21,759 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 17:14:22,617 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 17:14:22,670 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 17:14:42,047 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 17:14:44,280 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:15:17,119 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 17:15:17,170 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 17:15:19,361 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:15:19,361 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:15:19,364 - __main__ - INFO - LiteLLM chat completed in 37.443s
2025-08-01 17:15:19,498 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:15:19,639 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:15:19,814 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-01 17:15:22,569 - __main__ - INFO - TTS synthesis completed in 3.189s
2025-08-01 17:15:35,365 - __main__ - INFO - Starting continuous recording
2025-08-01 17:15:35,620 - __main__ - INFO - Recording started successfully
2025-08-01 17:15:35,620 - __main__ - INFO - Starting camera recording
2025-08-01 17:15:37,062 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 17:15:37,062 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 17:15:37,768 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 17:15:41,717 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 17:15:41,717 - __main__ - INFO - Starting utterance processing
2025-08-01 17:15:42,876 - __main__ - INFO - ASR transcription completed in 1.159s: 'i have used cnn in one of my projects'
2025-08-01 17:15:42,878 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 17:15:45,004 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:16:13,823 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 17:16:13,830 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 17:16:15,998 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:16:16,019 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:16:16,020 - __main__ - INFO - LiteLLM chat completed in 33.143s
2025-08-01 17:16:16,035 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-01 17:16:16,135 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:16:16,261 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:16:16,456 - __main__ - INFO - TTS synthesis completed in 0.435s
2025-08-01 17:16:38,866 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 17:16:38,866 - __main__ - INFO - Starting utterance processing
2025-08-01 17:16:40,223 - __main__ - INFO - ASR transcription completed in 1.356s: 'okay so it was a unit modification of the architecture and i used data sets from kaggle'
2025-08-01 17:16:40,224 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 17:16:42,353 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:17:10,447 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 17:17:10,451 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 17:17:12,572 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:17:12,574 - __main__ - INFO - LiteLLM chat completed in 32.349s
2025-08-01 17:17:12,575 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:17:12,695 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:17:12,819 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:17:12,941 - __main__ - INFO - TTS synthesis completed in 0.366s
2025-08-01 17:17:22,588 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 17:17:23,567 - __main__ - INFO - Stopping recording
2025-08-01 17:17:23,642 - __main__ - INFO - Audio stream closed successfully
2025-08-01 17:17:23,642 - __main__ - INFO - Sentinel value added to queue
2025-08-01 17:17:23,642 - __main__ - INFO - Recording stopped. Total chunks recorded: 3250
2025-08-01 17:17:23,642 - __main__ - INFO - Stopping camera recording
2025-08-01 17:17:24,137 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 17:36:47,644 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 17:36:53,972 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 17:37:06,512 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 17:37:11,800 - __main__ - INFO - Initializing TTS Processor
2025-08-01 17:37:15,596 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 17:37:15,596 - __main__ - INFO - Initializing VAD Processor
2025-08-01 17:37:16,216 - __main__ - INFO - VAD model loaded successfully
2025-08-01 17:37:16,217 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 17:37:16,346 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 17:37:16,346 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 17:37:17,238 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 17:37:17,278 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 17:37:23,255 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 17:37:25,441 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:37:54,890 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 17:37:54,921 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 17:37:57,131 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:37:57,138 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:37:57,140 - __main__ - INFO - LiteLLM chat completed in 33.935s
2025-08-01 17:37:57,276 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:37:57,407 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:38:01,066 - __main__ - INFO - TTS synthesis completed in 3.924s
2025-08-01 17:38:17,710 - __main__ - INFO - Starting continuous recording
2025-08-01 17:38:18,644 - __main__ - INFO - Recording started successfully
2025-08-01 17:38:18,645 - __main__ - INFO - Starting camera recording
2025-08-01 17:38:20,271 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 17:38:20,272 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 17:38:20,794 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 17:38:24,967 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 17:38:24,967 - __main__ - INFO - Starting utterance processing
2025-08-01 17:38:30,941 - __main__ - INFO - ASR transcription completed in 5.974s: 'i have used unet architecture in one of my products'
2025-08-01 17:38:30,943 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 17:38:33,068 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:38:58,749 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 17:38:58,763 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 17:39:00,890 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:39:00,890 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:39:00,894 - __main__ - INFO - LiteLLM chat completed in 29.952s
2025-08-01 17:39:01,057 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:39:01,184 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:39:02,062 - __main__ - INFO - TTS synthesis completed in 1.166s
2025-08-01 17:39:25,389 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 17:39:25,389 - __main__ - INFO - Starting utterance processing
2025-08-01 17:39:33,278 - __main__ - INFO - ASR transcription completed in 7.889s: 'challenges for many i tried to use it with pytorch but then i had to use tensorflow'
2025-08-01 17:39:33,280 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 17:39:35,418 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:40:01,349 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 17:40:01,353 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 17:40:03,526 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:40:03,529 - __main__ - INFO - LiteLLM chat completed in 30.249s
2025-08-01 17:40:03,530 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:40:03,676 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:40:03,821 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:40:04,521 - __main__ - INFO - TTS synthesis completed in 0.991s
2025-08-01 17:40:13,184 - __main__ - INFO - Stopping recording
2025-08-01 17:40:13,233 - __main__ - INFO - Audio stream closed successfully
2025-08-01 17:40:14,242 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-01 17:40:14,242 - __main__ - INFO - Recording stopped. Total chunks recorded: 3471
2025-08-01 17:40:14,242 - __main__ - INFO - Stopping camera recording
2025-08-01 17:40:14,803 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 17:40:14,804 - __main__ - INFO - Starting continuous recording
2025-08-01 17:40:15,021 - __main__ - INFO - Recording started successfully
2025-08-01 17:40:15,021 - __main__ - INFO - Starting camera recording
2025-08-01 17:40:16,045 - __main__ - INFO - Stopping recording
2025-08-01 17:40:16,140 - __main__ - INFO - Audio stream closed successfully
2025-08-01 17:40:16,140 - __main__ - INFO - Sentinel value added to queue
2025-08-01 17:40:16,140 - __main__ - INFO - Recording stopped. Total chunks recorded: 10
2025-08-01 17:40:16,140 - __main__ - INFO - Stopping camera recording
2025-08-01 17:43:43,702 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 17:43:45,684 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 17:44:04,423 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 17:44:14,832 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 17:44:26,516 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 17:44:34,168 - __main__ - INFO - Initializing TTS Processor
2025-08-01 17:44:37,901 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 17:44:37,926 - __main__ - INFO - Initializing VAD Processor
2025-08-01 17:44:38,594 - __main__ - INFO - VAD model loaded successfully
2025-08-01 17:44:38,594 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 17:44:38,721 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 17:44:38,721 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 17:44:39,501 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 17:44:39,545 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 17:44:45,908 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 17:44:48,122 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:45:08,778 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 17:45:08,797 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 17:45:10,968 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:45:10,970 - __main__ - INFO - LiteLLM chat completed in 25.129s
2025-08-01 17:45:10,977 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:45:11,089 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:45:11,202 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:45:11,347 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-01 17:45:11,350 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-01 17:45:14,534 - __main__ - INFO - TTS synthesis completed in 3.562s
2025-08-01 17:45:34,839 - __main__ - INFO - Starting continuous recording
2025-08-01 17:45:35,082 - __main__ - INFO - Recording started successfully
2025-08-01 17:45:35,082 - __main__ - INFO - Starting camera recording
2025-08-01 17:45:36,460 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 17:45:36,460 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 17:45:36,781 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 17:45:42,625 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 17:45:42,625 - __main__ - INFO - Starting utterance processing
2025-08-01 17:45:47,225 - __main__ - INFO - ASR transcription completed in 4.600s: 'i used pytorch when i was making a unit clone architecture'
2025-08-01 17:45:47,227 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 17:45:49,346 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:46:14,870 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 17:46:14,876 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 17:46:17,079 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:46:17,080 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:46:17,086 - __main__ - INFO - LiteLLM chat completed in 29.858s
2025-08-01 17:46:17,273 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:46:17,411 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:46:17,440 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-01 17:46:17,960 - __main__ - INFO - TTS synthesis completed in 0.874s
2025-08-01 17:46:44,554 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 17:46:44,555 - __main__ - INFO - Starting utterance processing
2025-08-01 17:46:49,734 - __main__ - INFO - ASR transcription completed in 5.180s: 'so the unit clone architecture was designed to clone the sickle cells in our body via computer vision and then identify them'
2025-08-01 17:46:49,736 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 17:46:51,867 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:47:22,885 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 17:47:22,888 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 17:47:25,064 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:47:25,067 - __main__ - INFO - LiteLLM chat completed in 35.332s
2025-08-01 17:47:25,080 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:47:25,219 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:47:25,304 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-01 17:47:25,343 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 17:47:27,042 - __main__ - INFO - TTS synthesis completed in 1.974s
2025-08-01 17:47:49,680 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 17:47:51,990 - __main__ - INFO - Stopping recording
2025-08-01 17:47:52,091 - __main__ - INFO - Audio stream closed successfully
2025-08-01 17:47:52,091 - __main__ - INFO - Sentinel value added to queue
2025-08-01 17:47:52,091 - __main__ - INFO - Recording stopped. Total chunks recorded: 4001
2025-08-01 17:47:52,092 - __main__ - INFO - Stopping camera recording
2025-08-01 17:47:52,499 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 17:59:16,782 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 17:59:29,087 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 17:59:41,398 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 17:59:44,393 - __main__ - INFO - Initializing TTS Processor
2025-08-01 17:59:48,758 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 17:59:48,758 - __main__ - INFO - Initializing VAD Processor
2025-08-01 17:59:49,383 - __main__ - INFO - VAD model loaded successfully
2025-08-01 17:59:49,383 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 17:59:49,647 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 17:59:49,648 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 17:59:50,611 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 17:59:50,655 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 18:00:10,080 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-01 18:00:12,300 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:32:40,488 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 18:32:48,145 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 18:33:01,502 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 18:33:05,321 - __main__ - INFO - Initializing TTS Processor
2025-08-01 18:33:09,413 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 18:33:09,413 - __main__ - INFO - Initializing VAD Processor
2025-08-01 18:33:10,071 - __main__ - INFO - VAD model loaded successfully
2025-08-01 18:33:10,071 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 18:33:10,413 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 18:33:10,413 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 18:33:11,442 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 18:33:11,524 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 18:33:18,699 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 18:33:20,800 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 400 Bad Request"
2025-08-01 18:33:20,826 - __main__ - ERROR - LiteLLM error: litellm.BadRequestError: VertexAIException BadRequestError - {
  "error": {
    "code": 400,
    "message": "* GenerateContentRequest.contents: contents is not specified\n",
    "status": "INVALID_ARGUMENT"
  }
}

2025-08-01 18:33:23,579 - __main__ - INFO - TTS synthesis completed in 2.753s
2025-08-01 18:33:27,700 - __main__ - INFO - Starting continuous recording
2025-08-01 18:33:27,986 - __main__ - INFO - Recording started successfully
2025-08-01 18:33:27,986 - __main__ - INFO - Starting camera recording
2025-08-01 18:33:29,708 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 18:33:29,708 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 18:33:30,279 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 18:33:35,604 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 18:33:35,604 - __main__ - INFO - Starting utterance processing
2025-08-01 18:33:35,727 - __main__ - INFO - Stopping recording
2025-08-01 18:33:35,842 - __main__ - INFO - Audio stream closed successfully
2025-08-01 18:33:35,842 - __main__ - INFO - Sentinel value added to queue
2025-08-01 18:33:35,842 - __main__ - INFO - Recording stopped. Total chunks recorded: 214
2025-08-01 18:33:35,842 - __main__ - INFO - Stopping camera recording
2025-08-01 18:34:02,428 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 18:34:09,529 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 18:34:19,615 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 18:34:24,071 - __main__ - INFO - Initializing TTS Processor
2025-08-01 18:34:27,365 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 18:34:27,365 - __main__ - INFO - Initializing VAD Processor
2025-08-01 18:34:27,632 - __main__ - INFO - VAD model loaded successfully
2025-08-01 18:34:27,636 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 18:34:27,708 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 18:34:27,708 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 18:34:28,513 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 18:34:28,555 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 18:34:38,618 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 18:34:40,806 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:35:17,229 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 18:35:17,247 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:35:19,423 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:35:19,423 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:35:19,426 - __main__ - INFO - LiteLLM chat completed in 40.837s
2025-08-01 18:35:19,556 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:35:19,692 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:35:22,803 - __main__ - INFO - TTS synthesis completed in 3.372s
2025-08-01 18:35:40,831 - __main__ - INFO - Starting continuous recording
2025-08-01 18:35:41,078 - __main__ - INFO - Recording started successfully
2025-08-01 18:35:41,078 - __main__ - INFO - Starting camera recording
2025-08-01 18:35:42,809 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 18:35:42,809 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 18:35:43,414 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 18:36:04,621 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 18:36:04,621 - __main__ - INFO - Starting utterance processing
2025-08-01 18:36:07,199 - __main__ - INFO - ASR transcription completed in 2.578s: 'so basically we were working on the combination of these we basically had 2 responses and based on the predefined guidelines we made models correctness by stating various cycles like information r'
2025-08-01 18:36:07,203 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 18:36:09,322 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:36:40,602 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 18:36:40,604 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:36:42,750 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:36:42,753 - __main__ - INFO - LiteLLM chat completed in 35.552s
2025-08-01 18:36:42,755 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:36:42,910 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:36:43,047 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:36:43,354 - __main__ - INFO - TTS synthesis completed in 0.599s
2025-08-01 18:36:58,902 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 18:36:58,902 - __main__ - INFO - Starting utterance processing
2025-08-01 18:36:59,824 - __main__ - INFO - ASR transcription completed in 0.922s: 'instructions been following'
2025-08-01 18:36:59,826 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 18:37:01,977 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:37:35,640 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 18:37:35,643 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:37:37,779 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:37:37,812 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:37:37,814 - __main__ - INFO - LiteLLM chat completed in 37.987s
2025-08-01 18:37:37,917 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:37:38,034 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:37:38,691 - __main__ - INFO - TTS synthesis completed in 0.876s
2025-08-01 18:38:01,208 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 18:38:01,208 - __main__ - INFO - Starting utterance processing
2025-08-01 18:38:05,581 - __main__ - INFO - ASR transcription completed in 4.373s: 'thank you'
2025-08-01 18:38:05,583 - LiteLLM - INFO - 
LiteLLM completion() model= gemma3:latest; provider = ollama
2025-08-01 18:38:07,749 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:38:08,332 - __main__ - INFO - Stopping recording
2025-08-01 18:38:08,466 - __main__ - INFO - Audio stream closed successfully
2025-08-01 18:38:09,474 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-01 18:38:09,474 - __main__ - INFO - Recording stopped. Total chunks recorded: 4355
2025-08-01 18:38:09,475 - __main__ - INFO - Stopping camera recording
2025-08-01 18:38:42,821 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 18:38:42,824 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:38:45,035 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:38:45,038 - __main__ - INFO - LiteLLM chat completed in 39.455s
2025-08-01 18:38:45,062 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:38:45,247 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:38:45,454 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 18:38:45,728 - __main__ - INFO - TTS synthesis completed in 0.686s
2025-08-01 18:41:40,796 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 18:41:47,357 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 18:42:01,221 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 18:42:05,692 - __main__ - INFO - Initializing TTS Processor
2025-08-01 18:42:10,936 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 18:42:10,936 - __main__ - INFO - Initializing VAD Processor
2025-08-01 18:42:11,368 - __main__ - INFO - VAD model loaded successfully
2025-08-01 18:42:11,368 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 18:42:11,543 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 18:42:11,543 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 18:42:12,432 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 18:42:12,488 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-01 18:43:23,813 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 18:43:27,364 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 18:43:27,371 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:43:27,386 - __main__ - INFO - LiteLLM chat completed in 3.683s
2025-08-01 18:43:30,981 - __main__ - INFO - TTS synthesis completed in 3.594s
2025-08-01 18:43:44,760 - __main__ - INFO - Starting continuous recording
2025-08-01 18:43:45,008 - __main__ - INFO - Recording started successfully
2025-08-01 18:43:45,008 - __main__ - INFO - Starting camera recording
2025-08-01 18:43:46,904 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 18:43:46,904 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 18:43:47,432 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 18:43:52,787 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 18:43:52,787 - __main__ - INFO - Starting utterance processing
2025-08-01 18:43:54,289 - __main__ - INFO - ASR transcription completed in 1.501s: 'okay so i have used those techniques to find you a model using the eft methods'
2025-08-01 18:43:54,291 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 18:43:55,775 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 18:43:55,777 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:43:55,778 - __main__ - INFO - LiteLLM chat completed in 1.487s
2025-08-01 18:43:56,101 - __main__ - INFO - TTS synthesis completed in 0.322s
2025-08-01 18:44:13,495 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 18:44:13,495 - __main__ - INFO - Starting utterance processing
2025-08-01 18:44:15,341 - __main__ - INFO - ASR transcription completed in 1.845s: 'i meant to say p e r t which is parameter efficient fine tuning'
2025-08-01 18:44:15,344 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 18:44:17,068 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 18:44:17,069 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:44:17,069 - __main__ - INFO - LiteLLM chat completed in 1.727s
2025-08-01 18:44:17,383 - __main__ - INFO - TTS synthesis completed in 0.312s
2025-08-01 18:44:25,256 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 18:44:26,085 - __main__ - INFO - Stopping recording
2025-08-01 18:44:26,168 - __main__ - INFO - Audio stream closed successfully
2025-08-01 18:44:26,168 - __main__ - INFO - Sentinel value added to queue
2025-08-01 18:44:26,168 - __main__ - INFO - Recording stopped. Total chunks recorded: 1214
2025-08-01 18:44:26,168 - __main__ - INFO - Stopping camera recording
2025-08-01 18:44:26,657 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 18:48:58,592 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 18:49:07,191 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 18:49:19,348 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 18:49:27,168 - __main__ - INFO - Initializing TTS Processor
2025-08-01 18:49:30,894 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 18:49:30,896 - __main__ - INFO - Initializing VAD Processor
2025-08-01 18:49:31,460 - __main__ - INFO - VAD model loaded successfully
2025-08-01 18:49:31,460 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 18:49:31,705 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 18:49:31,705 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 18:49:32,629 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 18:49:32,679 - __main__ - INFO - Successfully read resume from Resume (Heet).pdf
2025-08-01 18:49:41,432 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 18:49:44,678 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 18:49:44,679 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:49:44,688 - __main__ - INFO - LiteLLM chat completed in 3.306s
2025-08-01 18:49:47,714 - __main__ - INFO - TTS synthesis completed in 3.026s
2025-08-01 18:50:01,687 - __main__ - INFO - Starting continuous recording
2025-08-01 18:50:02,082 - __main__ - INFO - Recording started successfully
2025-08-01 18:50:02,082 - __main__ - INFO - Starting camera recording
2025-08-01 18:50:03,729 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 18:50:03,729 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 18:50:04,237 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 18:50:27,680 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 18:50:27,680 - __main__ - INFO - Starting utterance processing
2025-08-01 18:50:30,279 - __main__ - INFO - ASR transcription completed in 2.599s: 'so for designing a disease prediction system i have made use of regression models multiple regression models and based on their performance i have selected the final model and used the hyperparameter tuning for that'
2025-08-01 18:50:30,281 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 18:50:31,590 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 18:50:31,592 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:50:31,592 - __main__ - INFO - LiteLLM chat completed in 1.313s
2025-08-01 18:50:31,948 - __main__ - INFO - TTS synthesis completed in 0.353s
2025-08-01 18:50:55,538 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 18:50:55,538 - __main__ - INFO - Starting utterance processing
2025-08-01 18:50:57,401 - __main__ - INFO - ASR transcription completed in 1.863s: 'okay so i have used linear regression model random forest and xgboost model'
2025-08-01 18:50:57,405 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 18:50:59,109 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 18:50:59,110 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:50:59,111 - __main__ - INFO - LiteLLM chat completed in 1.708s
2025-08-01 18:50:59,124 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-01 18:50:59,895 - __main__ - INFO - TTS synthesis completed in 0.782s
2025-08-01 18:51:15,748 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 18:51:35,805 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 18:51:35,806 - __main__ - INFO - Starting utterance processing
2025-08-01 18:51:42,523 - __main__ - INFO - ASR transcription completed in 6.717s: 'so basically i have used mean absolute error and mean square error and also ir 2 score and based on this matrix the random forest model performs well amongst all other'
2025-08-01 18:51:42,524 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 18:51:45,489 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 18:51:45,490 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:51:45,492 - __main__ - INFO - LiteLLM chat completed in 2.967s
2025-08-01 18:51:46,368 - __main__ - INFO - TTS synthesis completed in 0.875s
2025-08-01 18:52:39,564 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 18:52:39,564 - __main__ - INFO - Starting utterance processing
2025-08-01 18:52:49,250 - __main__ - INFO - ASR transcription completed in 9.686s: 'okay so as i got the best evaluation metrics for this random forest model so i further hyperparameter tuning and found the best hyperparameters for this model and based on that parameters i will retrain the model and got a better accuracy score i mean evaluation metrics'
2025-08-01 18:52:49,253 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 18:52:52,933 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 18:52:52,934 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 18:52:52,934 - __main__ - INFO - LiteLLM chat completed in 3.683s
2025-08-01 18:52:53,464 - __main__ - INFO - TTS synthesis completed in 0.526s
2025-08-01 18:53:00,173 - __main__ - INFO - Stopping recording
2025-08-01 18:53:01,236 - __main__ - INFO - Audio stream closed successfully
2025-08-01 18:53:01,237 - __main__ - INFO - Sentinel value added to queue
2025-08-01 18:53:01,237 - __main__ - INFO - Recording stopped. Total chunks recorded: 4759
2025-08-01 18:53:01,237 - __main__ - INFO - Stopping camera recording
2025-08-01 18:53:01,813 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 18:58:28,284 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 18:58:38,013 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 18:58:47,929 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 18:58:56,568 - __main__ - INFO - Initializing TTS Processor
2025-08-01 18:58:59,921 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 18:58:59,921 - __main__ - INFO - Initializing VAD Processor
2025-08-01 18:59:00,502 - __main__ - INFO - VAD model loaded successfully
2025-08-01 18:59:00,502 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 18:59:00,730 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 18:59:00,730 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 18:59:01,550 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 18:59:01,587 - __main__ - INFO - Successfully read resume from Resume_Meet Bhatt_June25.pdf
2025-08-01 19:37:23,510 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 19:37:30,992 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 19:37:43,492 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 19:37:52,869 - __main__ - INFO - Initializing TTS Processor
2025-08-01 19:37:56,700 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 19:37:56,700 - __main__ - INFO - Initializing VAD Processor
2025-08-01 19:37:57,297 - __main__ - INFO - VAD model loaded successfully
2025-08-01 19:37:57,297 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 19:37:57,803 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 19:37:57,803 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 19:37:58,818 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 19:37:58,848 - __main__ - INFO - Successfully read resume from Resume_Meet Bhatt_June25.pdf
2025-08-01 20:16:59,034 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 20:17:04,788 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 20:17:04,799 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:17:04,821 - __main__ - INFO - LiteLLM chat completed in 6.053s
2025-08-01 20:17:09,343 - __main__ - INFO - TTS synthesis completed in 4.507s
2025-08-01 20:17:25,914 - __main__ - INFO - Starting continuous recording
2025-08-01 20:17:26,352 - __main__ - INFO - Recording started successfully
2025-08-01 20:17:26,352 - __main__ - INFO - Starting camera recording
2025-08-01 20:17:28,086 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 20:17:28,086 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 20:17:28,707 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 20:17:52,412 - __main__ - INFO - Stopping recording
2025-08-01 20:17:52,514 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 20:17:52,514 - __main__ - INFO - Starting utterance processing
2025-08-01 20:17:52,538 - __main__ - INFO - Audio stream closed successfully
2025-08-01 20:17:52,538 - __main__ - INFO - Sentinel value added to queue
2025-08-01 20:17:52,539 - __main__ - INFO - Recording stopped. Total chunks recorded: 780
2025-08-01 20:17:52,539 - __main__ - INFO - Stopping camera recording
2025-08-01 20:18:00,261 - __main__ - INFO - ASR transcription completed in 7.746s: 'so the analytics the algorithm implementation process basically was conducted by as the model gave a particular response and i was expected to rate the model is response based on positive if it was positive or negative we used the leibniz scale for weight living'
2025-08-01 20:18:00,264 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 20:18:03,303 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 20:18:03,303 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:18:03,305 - __main__ - INFO - LiteLLM chat completed in 3.041s
2025-08-01 20:18:03,878 - __main__ - INFO - TTS synthesis completed in 0.573s
2025-08-01 20:18:16,295 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 20:18:19,878 - __main__ - INFO - Starting continuous recording
2025-08-01 20:18:20,258 - __main__ - INFO - Recording started successfully
2025-08-01 20:18:20,258 - __main__ - INFO - Starting camera recording
2025-08-01 20:18:21,768 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 20:18:21,768 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 20:18:44,543 - __main__ - INFO - Stopping recording
2025-08-01 20:18:45,401 - __main__ - INFO - Audio stream closed successfully
2025-08-01 20:18:45,401 - __main__ - INFO - Sentinel value added to queue
2025-08-01 20:18:45,401 - __main__ - INFO - Recording stopped. Total chunks recorded: 0
2025-08-01 20:18:45,403 - __main__ - INFO - Stopping camera recording
2025-08-01 20:18:45,829 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 20:18:47,229 - __main__ - INFO - Starting continuous recording
2025-08-01 20:18:47,525 - __main__ - INFO - Recording started successfully
2025-08-01 20:18:47,525 - __main__ - INFO - Starting camera recording
2025-08-01 20:18:48,932 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 20:18:48,932 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 20:19:03,595 - __main__ - INFO - Stopping recording
2025-08-01 20:19:03,595 - __main__ - INFO - Sentinel value added to queue
2025-08-01 20:19:03,597 - __main__ - INFO - Recording stopped. Total chunks recorded: 1
2025-08-01 20:19:03,597 - __main__ - INFO - Stopping camera recording
2025-08-01 20:19:04,024 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 20:19:43,520 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 20:19:50,122 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 20:20:01,271 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 20:20:03,878 - __main__ - INFO - Initializing TTS Processor
2025-08-01 20:20:07,505 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 20:20:07,505 - __main__ - INFO - Initializing VAD Processor
2025-08-01 20:20:08,131 - __main__ - INFO - VAD model loaded successfully
2025-08-01 20:20:08,131 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 20:20:08,389 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 20:20:08,389 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 20:20:09,187 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 20:20:09,229 - __main__ - INFO - Successfully read resume from Resume_Meet Bhatt_June25.pdf
2025-08-01 20:20:25,371 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 20:20:28,765 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 20:20:28,768 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:20:28,776 - __main__ - INFO - LiteLLM chat completed in 3.463s
2025-08-01 20:20:32,018 - __main__ - INFO - TTS synthesis completed in 3.239s
2025-08-01 20:20:51,727 - __main__ - INFO - Starting continuous recording
2025-08-01 20:20:52,174 - __main__ - INFO - Recording started successfully
2025-08-01 20:20:52,174 - __main__ - INFO - Starting camera recording
2025-08-01 20:20:53,729 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 20:20:53,729 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 20:20:54,329 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 20:21:23,926 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 20:21:23,926 - __main__ - INFO - Starting utterance processing
2025-08-01 20:21:28,740 - __main__ - INFO - ASR transcription completed in 4.814s: 'so one of the specific challenges we faced while implementing our lecture was ambiguity in the model responses many times the model will provide responses that were very much simple but we still had to read them and compare them based on objective measurements and not include subjective stuff which was challenging this was addressed by using the leikert is tail to an extent as it allowed us to compare 2 model responses with one another'
2025-08-01 20:21:28,744 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 20:21:31,680 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 20:21:31,680 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:21:31,682 - __main__ - INFO - LiteLLM chat completed in 2.939s
2025-08-01 20:21:32,104 - __main__ - INFO - TTS synthesis completed in 0.422s
2025-08-01 20:21:42,864 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 20:22:11,240 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 20:22:11,240 - __main__ - INFO - Starting utterance processing
2025-08-01 20:22:14,967 - __main__ - INFO - ASR transcription completed in 3.727s: 'so some specific criteria that were used were the likert skill basically rated to compare 2 models with one another we compared on the basis of model correctness instruction following verbosity and some of the metrics like those and rated if one model performance was better than the other much better or both were not good'
2025-08-01 20:22:14,968 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 20:22:18,631 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 20:22:18,631 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:22:18,631 - __main__ - INFO - LiteLLM chat completed in 3.663s
2025-08-01 20:22:19,109 - __main__ - INFO - TTS synthesis completed in 0.475s
2025-08-01 20:22:31,568 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 20:22:43,107 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 20:22:43,107 - __main__ - INFO - Starting utterance processing
2025-08-01 20:22:44,721 - __main__ - INFO - ASR transcription completed in 1.614s: 'my job did not include reviewing the models responses but over time we could see that the models were getting smarter'
2025-08-01 20:22:44,724 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 20:22:47,037 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 20:22:47,037 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:22:47,041 - __main__ - INFO - LiteLLM chat completed in 2.319s
2025-08-01 20:22:47,518 - __main__ - INFO - TTS synthesis completed in 0.476s
2025-08-01 20:23:02,507 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 20:23:22,673 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 20:23:22,673 - __main__ - INFO - Starting utterance processing
2025-08-01 20:23:26,081 - __main__ - INFO - ASR transcription completed in 3.409s: 'so we were supposed to prompt the models sometimes and every time we were supposed to prompt them in a way that they would beat the responses of sota models and over time we could see that it was getting more and more difficult to prompt the model in a manner that they would break them'
2025-08-01 20:23:26,081 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 20:23:28,550 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 20:23:28,551 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:23:28,551 - __main__ - INFO - LiteLLM chat completed in 2.470s
2025-08-01 20:23:29,318 - __main__ - INFO - TTS synthesis completed in 0.767s
2025-08-01 20:23:49,372 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 20:24:01,278 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 20:24:01,278 - __main__ - INFO - Starting utterance processing
2025-08-01 20:24:08,007 - __main__ - INFO - ASR transcription completed in 6.729s: 'we did not have any quantitative measurements to judge that but we could make a fair approximation based on the responses of the models and our experiences'
2025-08-01 20:24:08,009 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 20:24:09,356 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 20:24:09,357 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:24:09,357 - __main__ - INFO - LiteLLM chat completed in 1.347s
2025-08-01 20:24:09,895 - __main__ - INFO - TTS synthesis completed in 0.536s
2025-08-01 20:24:21,912 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 20:24:41,893 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 20:24:41,893 - __main__ - INFO - Starting utterance processing
2025-08-01 20:24:50,952 - __main__ - INFO - ASR transcription completed in 9.059s: 'sometimes the sort of models would fail and certain kinds of problems and they would not be able to provide the answers that we are looking for but some of the models in our testing were also providing excellent responses in terms of providing code and also in terms of reasoning'
2025-08-01 20:24:50,956 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 20:24:53,014 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 20:24:53,015 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:24:53,016 - __main__ - INFO - LiteLLM chat completed in 2.060s
2025-08-01 20:24:53,683 - __main__ - INFO - TTS synthesis completed in 0.667s
2025-08-01 20:25:04,784 - __main__ - INFO - Stopping recording
2025-08-01 20:25:04,829 - __main__ - INFO - Audio stream closed successfully
2025-08-01 20:25:05,832 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-01 20:25:05,832 - __main__ - INFO - Recording stopped. Total chunks recorded: 7868
2025-08-01 20:25:05,832 - __main__ - INFO - Stopping camera recording
2025-08-01 20:25:08,786 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-01 20:31:42,834 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 20:32:05,599 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-08-01 20:32:14,457 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 20:32:20,315 - __main__ - INFO - Initializing TTS Processor
2025-08-01 20:32:24,221 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 20:32:24,221 - __main__ - INFO - Initializing VAD Processor
2025-08-01 20:32:24,836 - __main__ - INFO - VAD model loaded successfully
2025-08-01 20:32:24,836 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 20:32:25,127 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 20:32:25,127 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 20:32:26,018 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 20:32:26,047 - __main__ - INFO - Successfully read resume from Resume_Meet Bhatt_June25.pdf
2025-08-01 20:33:02,114 - LiteLLM - INFO - 
LiteLLM completion() model= qwen3:0.6b; provider = ollama
2025-08-01 20:33:04,242 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 20:33:17,583 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 20:33:17,587 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:33:19,710 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 20:33:19,711 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 20:33:19,714 - __main__ - INFO - LiteLLM chat completed in 17.640s
2025-08-01 20:33:19,773 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 20:33:19,820 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 20:33:25,692 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-01 20:33:26,086 - __main__ - INFO - TTS synthesis completed in 6.370s
2025-08-01 20:34:30,941 - __main__ - INFO - Starting continuous recording
2025-08-01 20:34:31,422 - __main__ - INFO - Recording started successfully
2025-08-01 20:34:31,422 - __main__ - INFO - Starting camera recording
2025-08-01 20:34:33,107 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 20:34:33,109 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 20:34:33,768 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 20:34:39,740 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 20:34:41,836 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 20:34:57,420 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 20:34:57,420 - __main__ - INFO - Starting utterance processing
2025-08-01 20:34:58,122 - __main__ - INFO - ASR transcription completed in 0.702s: 'i do not even know what to say'
2025-08-01 20:34:58,131 - LiteLLM - INFO - 
LiteLLM completion() model= qwen3:0.6b; provider = ollama
2025-08-01 20:35:00,214 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 20:35:03,243 - __main__ - INFO - Stopping recording
2025-08-01 20:35:03,321 - __main__ - INFO - Audio stream closed successfully
2025-08-01 20:35:04,327 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-01 20:35:04,329 - __main__ - INFO - Recording stopped. Total chunks recorded: 993
2025-08-01 20:35:04,329 - __main__ - INFO - Stopping camera recording
2025-08-01 20:35:06,484 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-01 20:35:06,495 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:35:08,583 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 20:35:08,593 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 20:35:08,595 - __main__ - INFO - LiteLLM chat completed in 10.473s
2025-08-01 20:35:08,632 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 20:35:08,679 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-01 20:35:09,781 - __main__ - INFO - TTS synthesis completed in 1.184s
2025-08-01 20:58:09,975 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-01 20:58:20,799 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-01 20:58:32,949 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-01 20:58:38,480 - __main__ - INFO - Initializing TTS Processor
2025-08-01 20:58:42,786 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-01 20:58:42,786 - __main__ - INFO - Initializing VAD Processor
2025-08-01 20:58:43,432 - __main__ - INFO - VAD model loaded successfully
2025-08-01 20:58:43,432 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-01 20:58:43,752 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-01 20:58:43,752 - __main__ - INFO - Initializing CameraRecorder
2025-08-01 20:58:44,775 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 20:58:44,782 - __main__ - INFO - Successfully read resume from r.pdf
2025-08-01 20:59:28,614 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 20:59:31,476 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 20:59:31,504 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 20:59:31,514 - __main__ - INFO - LiteLLM chat completed in 2.983s
2025-08-01 20:59:38,471 - __main__ - INFO - TTS synthesis completed in 6.954s
2025-08-01 20:59:53,303 - __main__ - INFO - Starting continuous recording
2025-08-01 20:59:53,773 - __main__ - INFO - Recording started successfully
2025-08-01 20:59:53,773 - __main__ - INFO - Starting camera recording
2025-08-01 20:59:55,318 - __main__ - INFO - Starting continuous audio processing thread
2025-08-01 20:59:55,318 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-01 20:59:55,633 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-01 21:00:07,833 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 21:00:07,833 - __main__ - INFO - Starting utterance processing
2025-08-01 21:00:20,397 - __main__ - INFO - ASR transcription completed in 12.564s: 'so i implemented a software that was designed for management tasks we looked at inventory management menu management and employee management of the restaurant'
2025-08-01 21:00:20,400 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 21:00:22,114 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 21:00:22,117 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 21:00:22,117 - __main__ - INFO - LiteLLM chat completed in 1.717s
2025-08-01 21:00:25,554 - __main__ - INFO - TTS synthesis completed in 3.435s
2025-08-01 21:00:35,549 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 21:00:54,059 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 21:00:54,059 - __main__ - INFO - Starting utterance processing
2025-08-01 21:01:05,902 - __main__ - INFO - ASR transcription completed in 11.844s: 'we looked at which dishes were more popular on which days of the week and which dishes were more in demand that was used to infer the inventory requirements and set the prices of the dishes'
2025-08-01 21:01:05,910 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 21:01:08,760 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 21:01:08,762 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 21:01:08,763 - __main__ - INFO - LiteLLM chat completed in 2.853s
2025-08-01 21:01:11,972 - __main__ - INFO - TTS synthesis completed in 3.207s
2025-08-01 21:01:23,714 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-01 21:01:51,324 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-01 21:01:51,324 - __main__ - INFO - Starting utterance processing
2025-08-01 21:01:59,316 - __main__ - INFO - ASR transcription completed in 7.992s: 'blue'
2025-08-01 21:01:59,321 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-01 21:02:01,571 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-01 21:02:01,573 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-01 21:02:01,573 - __main__ - INFO - LiteLLM chat completed in 2.255s
2025-08-01 21:02:03,928 - __main__ - INFO - TTS synthesis completed in 2.350s
2025-08-01 21:02:11,656 - __main__ - INFO - Stopping recording
2025-08-01 21:02:11,723 - __main__ - INFO - Audio stream closed successfully
2025-08-01 21:02:11,723 - __main__ - INFO - Sentinel value added to queue
2025-08-01 21:02:11,725 - __main__ - INFO - Recording stopped. Total chunks recorded: 4262
2025-08-01 21:02:11,725 - __main__ - INFO - Stopping camera recording
2025-08-02 16:00:16,766 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-02 16:00:26,984 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-tiny
2025-08-02 16:00:32,785 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-02 16:00:37,029 - __main__ - INFO - Initializing TTS Processor
2025-08-02 16:00:41,790 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-02 16:00:41,790 - __main__ - INFO - Initializing VAD Processor
2025-08-02 16:00:42,421 - __main__ - INFO - VAD model loaded successfully
2025-08-02 16:00:42,422 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-02 16:00:42,597 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-02 16:00:42,597 - __main__ - INFO - Initializing CameraRecorder
2025-08-02 16:00:43,697 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:00:43,703 - __main__ - INFO - Successfully read resume from r.pdf
2025-08-02 16:01:04,833 - LiteLLM - INFO - 
LiteLLM completion() model= qwen3:0.6b; provider = ollama
2025-08-02 16:01:07,014 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:01:13,249 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:01:13,253 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:01:15,392 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:01:15,392 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:01:15,397 - __main__ - INFO - LiteLLM chat completed in 10.594s
2025-08-02 16:01:15,439 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:01:15,482 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:01:19,100 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-02 16:01:19,439 - __main__ - INFO - TTS synthesis completed in 4.041s
2025-08-02 16:02:08,291 - __main__ - INFO - Starting continuous recording
2025-08-02 16:02:08,494 - __main__ - INFO - Recording started successfully
2025-08-02 16:02:08,494 - __main__ - INFO - Starting camera recording
2025-08-02 16:02:10,144 - __main__ - INFO - Starting continuous audio processing thread
2025-08-02 16:02:10,144 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:02:10,861 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-02 16:02:13,562 - __main__ - INFO - Saving audio to sessions\session_20250802_160208.wav
2025-08-02 16:02:13,564 - __main__ - INFO - Audio saved successfully to sessions\session_20250802_160208.wav
2025-08-02 16:02:13,564 - __main__ - INFO - Stopping camera recording
2025-08-02 16:02:15,607 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 16:02:15,607 - __main__ - INFO - Starting utterance processing
2025-08-02 16:02:15,970 - __main__ - INFO - ASR transcription completed in 0.362s: 'i also did something that i really did for myself'
2025-08-02 16:02:15,973 - LiteLLM - INFO - 
LiteLLM completion() model= qwen3:0.6b; provider = ollama
2025-08-02 16:02:18,046 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:02:21,806 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:02:21,809 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:02:21,857 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:02:21,858 - __main__ - INFO - LiteLLM chat completed in 5.887s
2025-08-02 16:02:23,646 - __main__ - INFO - TTS synthesis completed in 1.785s
2025-08-02 16:02:23,885 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:02:23,936 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:02:23,984 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:03:31,303 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-02 16:12:38,959 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-02 16:12:54,257 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-tiny
2025-08-02 16:12:58,585 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-02 16:13:04,762 - __main__ - INFO - Initializing TTS Processor
2025-08-02 16:13:07,886 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-02 16:13:07,886 - __main__ - INFO - Initializing VAD Processor
2025-08-02 16:13:08,377 - __main__ - INFO - VAD model loaded successfully
2025-08-02 16:13:08,377 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-02 16:13:08,422 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-02 16:13:08,422 - __main__ - INFO - Initializing CameraRecorder
2025-08-02 16:13:09,267 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:13:09,271 - __main__ - INFO - Successfully read resume from r.pdf
2025-08-02 16:13:40,354 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-02 16:13:42,474 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:13:48,166 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:13:48,170 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:13:50,264 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:13:50,265 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:13:50,265 - __main__ - INFO - LiteLLM chat completed in 9.937s
2025-08-02 16:13:50,323 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:13:50,377 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:13:52,827 - __main__ - INFO - TTS synthesis completed in 2.557s
2025-08-02 16:14:12,092 - __main__ - INFO - Starting continuous recording
2025-08-02 16:14:12,298 - __main__ - INFO - Recording started successfully
2025-08-02 16:14:12,298 - __main__ - INFO - Starting camera recording
2025-08-02 16:14:13,794 - __main__ - INFO - Starting continuous audio processing thread
2025-08-02 16:14:13,794 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:14:14,368 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-02 16:14:20,028 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 16:14:20,028 - __main__ - INFO - Starting utterance processing
2025-08-02 16:14:20,403 - __main__ - INFO - ASR transcription completed in 0.374s: 'okay so i think they have helped me in getting better'
2025-08-02 16:14:20,405 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-02 16:14:22,498 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:14:26,034 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:14:26,035 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:14:26,089 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:14:26,091 - __main__ - INFO - LiteLLM chat completed in 5.687s
2025-08-02 16:14:27,313 - __main__ - INFO - TTS synthesis completed in 1.221s
2025-08-02 16:14:28,143 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:14:28,204 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:14:28,268 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:14:57,862 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-02 16:14:59,959 - __main__ - INFO - Saving audio to sessions\session_20250802_161412.wav
2025-08-02 16:14:59,960 - __main__ - INFO - Audio saved successfully to sessions\session_20250802_161412.wav
2025-08-02 16:14:59,962 - __main__ - INFO - Stopping camera recording
2025-08-02 16:18:25,592 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-02 16:18:36,809 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-tiny
2025-08-02 16:18:40,902 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-02 16:18:47,513 - __main__ - INFO - Initializing TTS Processor
2025-08-02 16:18:50,543 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-02 16:18:50,543 - __main__ - INFO - Initializing VAD Processor
2025-08-02 16:18:51,037 - __main__ - INFO - VAD model loaded successfully
2025-08-02 16:18:51,037 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-02 16:18:51,090 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-02 16:18:51,090 - __main__ - INFO - Initializing CameraRecorder
2025-08-02 16:18:51,897 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:18:51,900 - __main__ - INFO - Successfully read resume from r.pdf
2025-08-02 16:18:57,384 - __main__ - INFO - Starting continuous recording
2025-08-02 16:18:57,581 - __main__ - INFO - Recording started successfully
2025-08-02 16:18:57,584 - __main__ - INFO - Starting camera recording
2025-08-02 16:18:59,222 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-02 16:18:59,734 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-02 16:19:01,316 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:19:04,859 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:19:04,860 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:19:04,917 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:19:04,974 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:19:05,032 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:19:05,094 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:19:06,932 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:19:06,932 - __main__ - INFO - LiteLLM chat completed in 7.741s
2025-08-02 16:19:19,037 - __main__ - INFO - TTS synthesis completed in 12.105s
2025-08-02 16:19:40,870 - __main__ - INFO - Starting continuous audio processing thread
2025-08-02 16:19:40,872 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:19:45,576 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 16:19:45,576 - __main__ - INFO - Starting utterance processing
2025-08-02 16:19:46,103 - __main__ - INFO - ASR transcription completed in 0.527s: 'i have no idea'
2025-08-02 16:19:46,105 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-02 16:19:48,178 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:19:51,674 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:19:51,674 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:19:51,736 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:19:51,737 - __main__ - INFO - LiteLLM chat completed in 5.634s
2025-08-02 16:19:52,454 - __main__ - INFO - TTS synthesis completed in 0.716s
2025-08-02 16:19:53,766 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:19:53,825 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:19:53,890 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:20:14,639 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-02 16:20:19,263 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 16:20:19,263 - __main__ - INFO - Starting utterance processing
2025-08-02 16:20:19,666 - __main__ - INFO - ASR transcription completed in 0.403s: 'i said i why you do'
2025-08-02 16:20:19,669 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-02 16:20:21,766 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:20:26,468 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:20:26,469 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:20:26,530 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:20:26,533 - __main__ - INFO - LiteLLM chat completed in 6.866s
2025-08-02 16:20:28,139 - __main__ - INFO - TTS synthesis completed in 1.606s
2025-08-02 16:20:28,582 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:20:28,645 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:20:28,707 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:21:19,797 - __main__ - INFO - Stopping recording
2025-08-02 16:21:19,851 - __main__ - INFO - Audio stream closed successfully
2025-08-02 16:21:19,851 - __main__ - INFO - Sentinel value added to queue
2025-08-02 16:21:19,851 - __main__ - INFO - Recording stopped. Total chunks recorded: 4438
2025-08-02 16:21:19,851 - __main__ - INFO - Stopping camera recording
2025-08-02 16:21:20,347 - __main__ - INFO - Saving audio to sessions\session_20250802_161857.wav
2025-08-02 16:21:20,350 - __main__ - INFO - Audio saved successfully to sessions\session_20250802_161857.wav
2025-08-02 16:21:20,381 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-02 16:25:14,324 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-02 16:25:21,037 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-tiny
2025-08-02 16:25:25,112 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-02 16:25:26,702 - __main__ - INFO - Initializing TTS Processor
2025-08-02 16:25:29,375 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-02 16:25:29,375 - __main__ - INFO - Initializing VAD Processor
2025-08-02 16:25:29,884 - __main__ - INFO - VAD model loaded successfully
2025-08-02 16:25:29,884 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-02 16:25:29,931 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-02 16:25:29,931 - __main__ - INFO - Initializing CameraRecorder
2025-08-02 16:25:30,514 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:25:30,519 - __main__ - INFO - Successfully read resume from r.pdf
2025-08-02 16:25:37,057 - __main__ - INFO - Starting continuous recording
2025-08-02 16:25:37,235 - __main__ - INFO - Recording started successfully
2025-08-02 16:25:37,235 - __main__ - INFO - Starting camera recording
2025-08-02 16:25:38,557 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-02 16:25:39,064 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-02 16:25:40,664 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:25:47,386 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:25:47,393 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:25:49,475 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:25:49,482 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:25:49,483 - __main__ - INFO - LiteLLM chat completed in 10.950s
2025-08-02 16:25:49,534 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:25:49,594 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:25:51,724 - __main__ - INFO - TTS synthesis completed in 2.241s
2025-08-02 16:26:14,346 - __main__ - INFO - Starting continuous audio processing thread
2025-08-02 16:26:14,348 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:26:18,771 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 16:26:18,771 - __main__ - INFO - Starting utterance processing
2025-08-02 16:26:19,082 - __main__ - INFO - ASR transcription completed in 0.311s: 'it was very easy'
2025-08-02 16:26:19,084 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-02 16:26:21,182 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:26:23,685 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:26:23,685 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:26:23,741 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:26:23,742 - __main__ - INFO - LiteLLM chat completed in 4.658s
2025-08-02 16:26:24,113 - __main__ - INFO - TTS synthesis completed in 0.368s
2025-08-02 16:26:25,789 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:26:25,854 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:26:25,910 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:26:42,865 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 16:26:42,865 - __main__ - INFO - Starting utterance processing
2025-08-02 16:26:43,047 - __main__ - INFO - ASR transcription completed in 0.181s: 'i have faced many challenges in my standing'
2025-08-02 16:26:43,048 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2:1b; provider = ollama
2025-08-02 16:26:45,130 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:26:47,430 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:26:47,431 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:26:47,487 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:26:47,487 - __main__ - INFO - LiteLLM chat completed in 4.439s
2025-08-02 16:26:47,714 - __main__ - INFO - TTS synthesis completed in 0.228s
2025-08-02 16:26:49,500 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:26:49,561 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:26:49,617 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:27:14,355 - __main__ - INFO - Stopping recording
2025-08-02 16:27:14,410 - __main__ - INFO - Audio stream closed successfully
2025-08-02 16:27:14,410 - __main__ - INFO - Sentinel value added to queue
2025-08-02 16:27:14,410 - __main__ - INFO - Recording stopped. Total chunks recorded: 3031
2025-08-02 16:27:14,410 - __main__ - INFO - Stopping camera recording
2025-08-02 16:27:14,868 - __main__ - INFO - Saving audio to sessions\session_20250802_162537.wav
2025-08-02 16:27:14,871 - __main__ - INFO - Audio saved successfully to sessions\session_20250802_162537.wav
2025-08-02 16:27:14,908 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-02 16:28:10,181 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-02 16:28:16,028 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-tiny
2025-08-02 16:28:20,159 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-02 16:28:24,334 - __main__ - INFO - Initializing TTS Processor
2025-08-02 16:28:27,054 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-02 16:28:27,054 - __main__ - INFO - Initializing VAD Processor
2025-08-02 16:28:27,581 - __main__ - INFO - VAD model loaded successfully
2025-08-02 16:28:27,581 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-02 16:28:27,634 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-02 16:28:27,634 - __main__ - INFO - Initializing CameraRecorder
2025-08-02 16:28:28,338 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:28:28,346 - __main__ - INFO - Successfully read resume from r.pdf
2025-08-02 16:28:34,281 - __main__ - INFO - Starting continuous recording
2025-08-02 16:28:34,496 - __main__ - INFO - Recording started successfully
2025-08-02 16:28:34,496 - __main__ - INFO - Starting camera recording
2025-08-02 16:28:35,954 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:1.5b; provider = ollama
2025-08-02 16:28:36,462 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-02 16:28:38,056 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:28:46,744 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:28:46,750 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:28:48,835 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:28:48,838 - __main__ - INFO - LiteLLM chat completed in 12.914s
2025-08-02 16:28:48,846 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:28:48,910 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:28:48,956 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:28:52,220 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-02 16:28:52,658 - __main__ - INFO - TTS synthesis completed in 3.819s
2025-08-02 16:29:57,634 - __main__ - INFO - Starting continuous audio processing thread
2025-08-02 16:29:57,641 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:30:03,330 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 16:30:03,330 - __main__ - INFO - Starting utterance processing
2025-08-02 16:30:03,609 - __main__ - INFO - ASR transcription completed in 0.280s: 'i am sorry'
2025-08-02 16:30:03,612 - LiteLLM - INFO - 
LiteLLM completion() model= deepseek-r1:1.5b; provider = ollama
2025-08-02 16:30:05,678 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:30:09,842 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-08-02 16:30:09,844 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:30:09,891 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:30:09,892 - __main__ - INFO - LiteLLM chat completed in 6.283s
2025-08-02 16:30:11,366 - phonemizer - WARNING - words count mismatch on 100.0% of the lines (1/1)
2025-08-02 16:30:11,759 - __main__ - INFO - TTS synthesis completed in 1.864s
2025-08-02 16:30:11,906 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:30:11,959 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:30:12,008 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show "HTTP/1.1 200 OK"
2025-08-02 16:30:51,564 - __main__ - INFO - Stopping recording
2025-08-02 16:30:51,597 - __main__ - INFO - Audio stream closed successfully
2025-08-02 16:30:52,600 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-02 16:30:52,600 - __main__ - INFO - Recording stopped. Total chunks recorded: 4275
2025-08-02 16:30:59,928 - __main__ - INFO - Stopping camera recording
2025-08-02 16:31:00,399 - __main__ - INFO - Saving audio to sessions\session_20250802_162834.wav
2025-08-02 16:31:00,403 - __main__ - INFO - Audio saved successfully to sessions\session_20250802_162834.wav
2025-08-02 16:31:00,433 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-02 16:32:22,146 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-02 16:32:28,857 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-base
2025-08-02 16:32:40,481 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-08-02 16:33:15,694 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-02 16:33:19,258 - __main__ - INFO - Initializing TTS Processor
2025-08-02 16:33:22,242 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-02 16:33:22,243 - __main__ - INFO - Initializing VAD Processor
2025-08-02 16:33:22,464 - __main__ - INFO - VAD model loaded successfully
2025-08-02 16:33:22,464 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-02 16:33:22,515 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-02 16:33:22,515 - __main__ - INFO - Initializing CameraRecorder
2025-08-02 16:33:23,196 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:33:23,200 - __main__ - INFO - Successfully read resume from r.pdf
2025-08-02 16:33:29,235 - __main__ - INFO - Starting continuous recording
2025-08-02 16:33:29,431 - __main__ - INFO - Recording started successfully
2025-08-02 16:33:29,433 - __main__ - INFO - Starting camera recording
2025-08-02 16:33:30,835 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 16:33:31,317 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-02 16:33:33,300 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 16:33:33,307 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:33:33,317 - __main__ - INFO - LiteLLM chat completed in 2.509s
2025-08-02 16:33:35,328 - __main__ - INFO - TTS synthesis completed in 2.010s
2025-08-02 16:33:51,447 - __main__ - INFO - Starting continuous audio processing thread
2025-08-02 16:33:51,450 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 16:33:55,878 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 16:33:55,878 - __main__ - INFO - Starting utterance processing
2025-08-02 16:33:56,195 - __main__ - INFO - ASR transcription completed in 0.317s: 'adam'
2025-08-02 16:33:56,196 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 16:33:57,719 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 16:33:57,719 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:33:57,721 - __main__ - INFO - LiteLLM chat completed in 1.522s
2025-08-02 16:33:57,990 - __main__ - INFO - TTS synthesis completed in 0.267s
2025-08-02 16:34:04,720 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-02 16:34:11,210 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 16:34:11,210 - __main__ - INFO - Starting utterance processing
2025-08-02 16:34:11,436 - __main__ - INFO - ASR transcription completed in 0.226s: 'i expect people to love my .'
2025-08-02 16:34:11,438 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 16:34:12,432 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 16:34:12,434 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 16:34:12,434 - __main__ - INFO - LiteLLM chat completed in 0.997s
2025-08-02 16:34:12,771 - __main__ - INFO - TTS synthesis completed in 0.337s
2025-08-02 16:34:23,750 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-02 16:34:26,600 - __main__ - INFO - Stopping recording
2025-08-02 16:34:26,673 - __main__ - INFO - Audio stream closed successfully
2025-08-02 16:34:26,674 - __main__ - INFO - Sentinel value added to queue
2025-08-02 16:34:26,674 - __main__ - INFO - Recording stopped. Total chunks recorded: 1783
2025-08-02 16:34:26,674 - __main__ - INFO - Stopping camera recording
2025-08-02 16:34:27,167 - __main__ - INFO - Saving audio to sessions\session_20250802_163329.wav
2025-08-02 16:34:27,170 - __main__ - INFO - Audio saved successfully to sessions\session_20250802_163329.wav
2025-08-02 16:34:27,204 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-02 22:57:14,119 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-02 22:57:22,335 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-02 22:57:35,082 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-02 22:57:38,049 - __main__ - INFO - Initializing TTS Processor
2025-08-02 22:57:41,636 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-02 22:57:41,636 - __main__ - INFO - Initializing VAD Processor
2025-08-02 22:57:41,873 - __main__ - INFO - VAD model loaded successfully
2025-08-02 22:57:41,873 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-02 22:57:41,953 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-02 22:57:41,953 - __main__ - INFO - Initializing CameraRecorder
2025-08-02 22:57:42,779 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 22:57:42,785 - __main__ - INFO - Successfully read resume from r.pdf
2025-08-02 22:57:49,807 - __main__ - INFO - Starting continuous recording
2025-08-02 22:57:50,110 - __main__ - INFO - Recording started successfully
2025-08-02 22:57:50,111 - __main__ - INFO - Starting camera recording
2025-08-02 22:57:51,678 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 22:57:52,292 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-02 22:57:55,269 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 22:57:55,272 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 22:57:55,280 - __main__ - INFO - LiteLLM chat completed in 3.657s
2025-08-02 22:57:57,342 - __main__ - INFO - TTS synthesis completed in 2.061s
2025-08-02 22:57:57,342 - __main__ - ERROR - An unexpected error occurred in the main loop: name 'play_audio' is not defined
2025-08-02 22:57:57,344 - __main__ - INFO - Stopping recording
2025-08-02 22:57:57,397 - __main__ - INFO - Audio stream closed successfully
2025-08-02 22:57:58,404 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-02 22:57:58,404 - __main__ - INFO - Recording stopped. Total chunks recorded: 225
2025-08-02 22:57:58,404 - __main__ - INFO - Stopping camera recording
2025-08-02 22:57:58,887 - __main__ - INFO - Saving audio to sessions\session_20250802_225749.wav
2025-08-02 22:57:58,889 - __main__ - INFO - Audio saved successfully to sessions\session_20250802_225749.wav
2025-08-02 22:57:58,893 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-02 22:59:17,500 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-02 22:59:32,400 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-08-02 22:59:50,480 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-02 23:00:11,036 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-02 23:00:25,377 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-02 23:00:28,302 - __main__ - INFO - Initializing TTS Processor
2025-08-02 23:00:31,928 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-02 23:00:31,928 - __main__ - INFO - Initializing VAD Processor
2025-08-02 23:00:32,553 - __main__ - INFO - VAD model loaded successfully
2025-08-02 23:00:32,553 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-02 23:00:32,627 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-02 23:00:32,627 - __main__ - INFO - Initializing CameraRecorder
2025-08-02 23:00:33,674 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 23:00:33,727 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-02 23:00:40,209 - __main__ - INFO - Starting continuous recording
2025-08-02 23:00:40,402 - __main__ - INFO - Recording started successfully
2025-08-02 23:00:40,402 - __main__ - INFO - Starting camera recording
2025-08-02 23:00:41,946 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 23:00:42,491 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-02 23:00:45,962 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 23:00:45,964 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 23:00:45,984 - __main__ - INFO - LiteLLM chat completed in 4.079s
2025-08-02 23:00:48,863 - __main__ - INFO - TTS synthesis completed in 2.874s
2025-08-02 23:01:12,907 - __main__ - INFO - Starting continuous audio processing thread
2025-08-02 23:01:12,907 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-02 23:01:22,106 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 23:01:22,106 - __main__ - INFO - Starting utterance processing
2025-08-02 23:01:23,323 - __main__ - INFO - ASR transcription completed in 1.216s: 'so i face challenges of requiring many gpu for this task'
2025-08-02 23:01:23,323 - __main__ - INFO - Generating next question
2025-08-02 23:01:23,331 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 23:01:24,480 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 23:01:24,481 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 23:01:24,481 - __main__ - INFO - LiteLLM qgen completed in 1.159s
2025-08-02 23:01:24,481 - __main__ - INFO - Getting conversational response
2025-08-02 23:01:24,481 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 23:01:25,803 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 23:01:25,805 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 23:01:25,805 - __main__ - INFO - LiteLLM chat completed in 1.323s
2025-08-02 23:01:26,209 - __main__ - INFO - TTS synthesis completed in 0.401s
2025-08-02 23:01:36,744 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-02 23:01:43,954 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 23:01:43,954 - __main__ - INFO - Starting utterance processing
2025-08-02 23:01:45,145 - __main__ - INFO - ASR transcription completed in 1.191s: 'i used cloud based solutions to help multi teams'
2025-08-02 23:01:45,145 - __main__ - INFO - Generating next question
2025-08-02 23:01:45,145 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 23:01:47,023 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 23:01:47,033 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 23:01:47,034 - __main__ - INFO - LiteLLM qgen completed in 1.889s
2025-08-02 23:01:47,034 - __main__ - INFO - Getting conversational response
2025-08-02 23:01:47,037 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 23:01:48,203 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 23:01:48,203 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 23:01:48,203 - __main__ - INFO - LiteLLM chat completed in 1.169s
2025-08-02 23:01:48,650 - __main__ - INFO - TTS synthesis completed in 0.443s
2025-08-02 23:02:01,455 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-02 23:02:10,217 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-02 23:02:10,217 - __main__ - INFO - Starting utterance processing
2025-08-02 23:02:11,228 - __main__ - INFO - ASR transcription completed in 1.011s: 'i used aws and specifically the s 3 bucket'
2025-08-02 23:02:11,229 - __main__ - INFO - Generating next question
2025-08-02 23:02:11,230 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 23:02:14,044 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 23:02:14,046 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 23:02:14,046 - __main__ - INFO - LiteLLM qgen completed in 2.817s
2025-08-02 23:02:14,046 - __main__ - INFO - Getting conversational response
2025-08-02 23:02:14,046 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-02 23:02:15,341 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-02 23:02:15,347 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-02 23:02:15,347 - __main__ - INFO - LiteLLM chat completed in 1.301s
2025-08-02 23:02:15,753 - __main__ - INFO - TTS synthesis completed in 0.406s
2025-08-02 23:02:26,988 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-02 23:02:28,722 - __main__ - INFO - Stopping recording
2025-08-02 23:02:28,783 - __main__ - INFO - Audio stream closed successfully
2025-08-02 23:02:28,783 - __main__ - INFO - Sentinel value added to queue
2025-08-02 23:02:28,783 - __main__ - INFO - Recording stopped. Total chunks recorded: 3370
2025-08-02 23:02:28,783 - __main__ - INFO - Stopping camera recording
2025-08-02 23:02:29,321 - __main__ - INFO - Saving audio to sessions\session_20250802_230040.wav
2025-08-02 23:02:29,325 - __main__ - INFO - Audio saved successfully to sessions\session_20250802_230040.wav
2025-08-02 23:02:29,330 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-06 19:33:21,113 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-06 19:34:18,327 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-06 19:34:30,589 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-06 19:34:45,480 - __main__ - INFO - Initializing TTS Processor
2025-08-06 19:34:49,102 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-06 19:34:49,102 - __main__ - INFO - Initializing VAD Processor
2025-08-06 19:34:49,837 - __main__ - INFO - VAD model loaded successfully
2025-08-06 19:34:49,837 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-06 19:34:49,883 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-06 19:34:49,883 - __main__ - INFO - Initializing CameraRecorder
2025-08-06 19:34:50,853 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-06 19:34:50,896 - __main__ - INFO - Successfully read resume from Resume v3.01.pdf
2025-08-06 19:35:05,958 - __main__ - INFO - Starting continuous recording
2025-08-06 19:35:05,986 - __main__ - INFO - Recording started successfully
2025-08-06 19:35:05,986 - __main__ - INFO - Starting camera recording
2025-08-06 19:35:07,460 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 19:35:07,913 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-06 19:35:09,712 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=GEMINI "HTTP/1.1 400 Bad Request"
2025-08-06 19:35:09,731 - __main__ - ERROR - LiteLLM error: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-08-06 19:35:11,536 - __main__ - INFO - TTS synthesis completed in 1.805s
2025-08-06 19:35:15,879 - __main__ - INFO - Starting continuous audio processing thread
2025-08-06 19:35:15,879 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-06 19:35:21,026 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-06 19:35:29,526 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-06 19:35:29,526 - __main__ - INFO - Starting utterance processing
2025-08-06 19:35:30,946 - __main__ - INFO - ASR transcription completed in 1.420s: 'okay i hope you enjoyed it i hope you enjoyed it'
2025-08-06 19:35:30,948 - __main__ - INFO - Generating next question
2025-08-06 19:35:30,949 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 19:35:34,116 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=GEMINI "HTTP/1.1 400 Bad Request"
2025-08-06 19:35:34,126 - __main__ - ERROR - LiteLLM error: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-08-06 19:35:34,126 - __main__ - INFO - Getting conversational response
2025-08-06 19:35:34,128 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 19:35:34,723 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=GEMINI "HTTP/1.1 400 Bad Request"
2025-08-06 19:35:34,732 - __main__ - ERROR - LiteLLM error: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-08-06 19:35:35,002 - __main__ - INFO - TTS synthesis completed in 0.269s
2025-08-06 19:35:39,906 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-06 19:35:43,813 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-06 19:35:43,813 - __main__ - INFO - Starting utterance processing
2025-08-06 19:35:44,405 - __main__ - INFO - ASR transcription completed in 0.593s: 'bye'
2025-08-06 19:35:44,407 - __main__ - INFO - Generating next question
2025-08-06 19:35:44,408 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 19:35:46,537 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=GEMINI "HTTP/1.1 400 Bad Request"
2025-08-06 19:35:46,546 - __main__ - ERROR - LiteLLM error: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-08-06 19:35:46,547 - __main__ - INFO - Getting conversational response
2025-08-06 19:35:46,547 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 19:35:46,921 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=GEMINI "HTTP/1.1 400 Bad Request"
2025-08-06 19:35:46,930 - __main__ - ERROR - LiteLLM error: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-08-06 19:35:47,107 - __main__ - INFO - TTS synthesis completed in 0.177s
2025-08-06 19:35:56,647 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-06 19:35:56,648 - __main__ - INFO - Starting utterance processing
2025-08-06 19:35:57,584 - __main__ - INFO - ASR transcription completed in 0.936s: 'i did not think i would be here'
2025-08-06 19:35:57,586 - __main__ - INFO - Generating next question
2025-08-06 19:35:57,587 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 19:35:59,220 - __main__ - INFO - Stopping recording
2025-08-06 19:35:59,244 - __main__ - INFO - Audio stream closed successfully
2025-08-06 19:35:59,391 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=GEMINI "HTTP/1.1 400 Bad Request"
2025-08-06 19:35:59,401 - __main__ - ERROR - LiteLLM error: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-08-06 19:35:59,401 - __main__ - INFO - Getting conversational response
2025-08-06 19:35:59,402 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 19:35:59,765 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=GEMINI "HTTP/1.1 400 Bad Request"
2025-08-06 19:35:59,774 - __main__ - ERROR - LiteLLM error: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-08-06 19:35:59,954 - __main__ - INFO - TTS synthesis completed in 0.179s
2025-08-06 19:36:00,250 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-06 19:36:00,250 - __main__ - INFO - Recording stopped. Total chunks recorded: 1650
2025-08-06 19:36:04,193 - __main__ - INFO - Stopping camera recording
2025-08-06 19:36:04,619 - __main__ - INFO - Saving audio to sessions\session_20250806_193505.wav
2025-08-06 19:36:04,621 - __main__ - INFO - Audio saved successfully to sessions\session_20250806_193505.wav
2025-08-06 19:36:04,626 - __main__ - INFO - Session data saved to sessions\session.json
2025-08-06 19:39:19,505 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-06 19:39:49,425 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-08-06 19:41:14,943 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-06 19:41:19,309 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-small
2025-08-06 19:59:01,224 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-08-06 19:59:11,127 - __main__ - INFO - Initializing ASR Processor with model: openai/whisper-medium
2025-08-06 19:59:22,491 - __main__ - INFO - ASR model loaded successfully on cuda
2025-08-06 19:59:27,659 - __main__ - INFO - Initializing TTS Processor
2025-08-06 19:59:31,432 - __main__ - INFO - TTS pipeline initialized successfully
2025-08-06 19:59:31,432 - __main__ - INFO - Initializing VAD Processor
2025-08-06 19:59:32,217 - __main__ - INFO - VAD model loaded successfully
2025-08-06 19:59:32,217 - __main__ - INFO - Initializing AudioRecorder: sample_rate=16000, channels=1, chunk_size=512
2025-08-06 19:59:32,273 - __main__ - INFO - AudioRecorder initialized successfully
2025-08-06 19:59:32,273 - __main__ - INFO - Initializing CameraRecorder
2025-08-06 19:59:33,209 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-06 19:59:33,240 - __main__ - INFO - Successfully read resume from Resume_Meet Bhatt_June25.pdf
2025-08-06 19:59:47,363 - __main__ - INFO - Starting continuous recording
2025-08-06 19:59:47,396 - __main__ - INFO - Recording started successfully
2025-08-06 19:59:47,396 - __main__ - INFO - Starting camera recording
2025-08-06 19:59:49,071 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 19:59:49,633 - rfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().
2025-08-06 19:59:55,095 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 19:59:55,100 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 19:59:55,106 - __main__ - INFO - LiteLLM chat completed in 6.065s
2025-08-06 19:59:57,761 - __main__ - INFO - TTS synthesis completed in 2.654s
2025-08-06 20:00:16,669 - __main__ - INFO - Starting continuous audio processing thread
2025-08-06 20:00:16,670 - __main__ - INFO - Initializing LiteLLM client with base URL: http://localhost:11434
2025-08-06 20:00:44,745 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-06 20:00:44,746 - __main__ - INFO - Starting utterance processing
2025-08-06 20:00:54,286 - __main__ - INFO - ASR transcription completed in 9.540s: 'so at blink analytics while i implemented rlhf one of the major challenges i faced was evaluating the model performances and specifically the components that were subjective in justification as it is difficult to come to a concrete and singular conclusion when there is ambiguity regarding the test ratings'
2025-08-06 20:00:54,287 - __main__ - INFO - Generating next question
2025-08-06 20:00:54,288 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 20:00:55,890 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 20:00:55,899 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 20:00:55,900 - __main__ - INFO - LiteLLM qgen completed in 1.613s
2025-08-06 20:00:55,902 - __main__ - INFO - Getting conversational response
2025-08-06 20:00:55,903 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 20:00:57,363 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 20:00:57,364 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 20:00:57,364 - __main__ - INFO - LiteLLM chat completed in 1.463s
2025-08-06 20:00:58,024 - __main__ - INFO - TTS synthesis completed in 0.658s
2025-08-06 20:01:11,937 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-06 20:01:45,654 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-06 20:01:45,654 - __main__ - INFO - Starting utterance processing
2025-08-06 20:01:56,822 - __main__ - INFO - ASR transcription completed in 11.168s: 'so we evaluated the subjectivity by basically comparing the 2 model responses and providing a concrete explanation of each and every judgment that we did for the model is response we tried to be as subjective as possible and provide concrete justifications that any person would understand and then rated if the 2 model responses were better than one another or how they performed with relation to one another'
2025-08-06 20:01:56,823 - __main__ - INFO - Generating next question
2025-08-06 20:01:56,824 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 20:02:00,567 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 20:02:00,567 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 20:02:00,569 - __main__ - INFO - LiteLLM qgen completed in 3.746s
2025-08-06 20:02:00,570 - __main__ - INFO - Getting conversational response
2025-08-06 20:02:00,570 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 20:02:02,287 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 20:02:02,288 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 20:02:02,290 - __main__ - INFO - LiteLLM chat completed in 1.720s
2025-08-06 20:02:03,123 - __main__ - INFO - TTS synthesis completed in 0.831s
2025-08-06 20:02:20,012 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-06 20:02:56,260 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-06 20:02:56,260 - __main__ - INFO - Starting utterance processing
2025-08-06 20:03:07,164 - __main__ - INFO - ASR transcription completed in 10.904s: 'so the concrete justification included multiple structured criteria like instruction following verbosity if the model has elucidated or not and is it actually doing what the user intended it to do and we provided simple language and thorough explanations that any other person reading the response should be able to understand and allow other people to come to the same conclusion universally'
2025-08-06 20:03:07,165 - __main__ - INFO - Generating next question
2025-08-06 20:03:07,166 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 20:03:11,046 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 20:03:11,048 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 20:03:11,048 - __main__ - INFO - LiteLLM qgen completed in 3.882s
2025-08-06 20:03:11,049 - __main__ - INFO - Getting conversational response
2025-08-06 20:03:11,049 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 20:03:13,767 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 20:03:13,768 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 20:03:13,768 - __main__ - INFO - LiteLLM chat completed in 2.719s
2025-08-06 20:03:14,517 - __main__ - INFO - TTS synthesis completed in 0.744s
2025-08-06 20:03:30,046 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-06 20:03:59,387 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-06 20:03:59,387 - __main__ - INFO - Starting utterance processing
2025-08-06 20:04:12,483 - __main__ - INFO - ASR transcription completed in 13.096s: 'we rated the 2 model responses on all of these factors and after rating the responses we compared the performances on the likert scale this basically compared the model responses to evaluate if one model performed better than the other or was it much better or both the responses were not used and then this whole was used the entire response was used to train the ai model further but that was not our task our task was just the rating part'
2025-08-06 20:04:12,483 - __main__ - INFO - Generating next question
2025-08-06 20:04:12,485 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 20:04:18,318 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 20:04:18,319 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 20:04:18,320 - __main__ - INFO - LiteLLM qgen completed in 5.836s
2025-08-06 20:04:18,321 - __main__ - INFO - Getting conversational response
2025-08-06 20:04:18,322 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 20:04:19,858 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 20:04:19,861 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 20:04:19,862 - __main__ - INFO - LiteLLM chat completed in 1.539s
2025-08-06 20:04:20,508 - __main__ - INFO - TTS synthesis completed in 0.644s
2025-08-06 20:04:34,362 - __main__ - INFO - VAD rejected utterance as non-speech
2025-08-06 20:05:12,490 - __main__ - INFO - VAD confirmed speech, processing utterance
2025-08-06 20:05:12,490 - __main__ - INFO - Starting utterance processing
2025-08-06 20:05:24,906 - __main__ - INFO - ASR transcription completed in 12.416s: 'so every task had different amount of human feedback and every task had different metrics that were used to assess whether the performance of the models was up to the quality required or not the workflow was managed basically by comparing the 2 model responses side by side and then determining if which response or how these responses scale to the respective metrics and then comparing the responses on like that scale and then providing an objective justification regarding why we choose them'
2025-08-06 20:05:24,908 - __main__ - INFO - Generating next question
2025-08-06 20:05:24,909 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 20:05:26,769 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 20:05:26,770 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 20:05:26,771 - __main__ - INFO - LiteLLM qgen completed in 1.863s
2025-08-06 20:05:26,772 - __main__ - INFO - Getting conversational response
2025-08-06 20:05:26,772 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.5-flash; provider = gemini
2025-08-06 20:05:28,069 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyA_lZ78Rf_J9lCBqpu4hFaHSzYopB4CY0Y "HTTP/1.1 200 OK"
2025-08-06 20:05:28,071 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-06 20:05:28,072 - __main__ - INFO - LiteLLM chat completed in 1.300s
2025-08-06 20:05:28,914 - __main__ - INFO - TTS synthesis completed in 0.841s
2025-08-06 20:05:38,720 - __main__ - INFO - Stopping recording
2025-08-06 20:05:38,766 - __main__ - INFO - Audio stream closed successfully
2025-08-06 20:05:39,773 - __main__ - WARNING - Queue was full when adding sentinel value
2025-08-06 20:05:39,773 - __main__ - INFO - Recording stopped. Total chunks recorded: 10695
2025-08-06 20:05:47,598 - __main__ - INFO - Stopping camera recording
2025-08-06 20:05:48,035 - __main__ - INFO - Saving audio to sessions\session_20250806_195947.wav
2025-08-06 20:05:48,049 - __main__ - INFO - Audio saved successfully to sessions\session_20250806_195947.wav
2025-08-06 20:05:48,057 - __main__ - INFO - Session data saved to sessions\session.json
